From: Hannes Reinecke <hare@suse.de>
Date: Fri, 30 Sep 2016 00:35:34 +0800
Subject: [PATCH] md: lockless I/O submission for RAID1
Patch-mainline: Not yet, Hannes will send to upstream soon
References: bsc#982783,bsc#998106,bsc#1020048

We should avoid taking a lock in the hotpath during I/O submission;
this hurts performance very bad when running on fast storage.

(Minor fix in need_to_wait_for_sync() by Coly Li)

Signed-off-by: Hannes Reinecke <hare@suse.com>
Signed-off-by: Coly LI <colyli@suse.de>
---
 drivers/md/raid1.c |   96 ++++++++++++++++++++++++++++-------------------------
 drivers/md/raid1.h |   13 +++----
 2 files changed, 58 insertions(+), 51 deletions(-)

--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@ -828,11 +828,11 @@ static void raise_barrier(struct r1conf
 	spin_lock_irq(&conf->resync_lock);
 
 	/* Wait until no block IO is waiting */
-	wait_event_lock_irq(conf->wait_barrier, !conf->nr_waiting,
+	wait_event_lock_irq(conf->wait_barrier, !atomic_read(&conf->nr_waiting),
 			    conf->resync_lock);
 
 	/* block any new IO from starting */
-	conf->barrier++;
+	atomic_inc(&conf->barrier);
 	conf->next_resync = sector_nr;
 
 	/* For these conditions we must wait:
@@ -845,25 +845,22 @@ static void raise_barrier(struct r1conf
 	 * D: while there are any active requests in the current window.
 	 */
 	wait_event_lock_irq(conf->wait_barrier,
-			    !conf->array_frozen &&
-			    conf->barrier < RESYNC_DEPTH &&
+			    !test_bit(R1FLAG_ARRAY_FROZEN, &conf->flags) &&
+			    atomic_read(&conf->barrier) < RESYNC_DEPTH &&
 			    conf->current_window_requests == 0 &&
 			    (conf->start_next_window >=
 			     conf->next_resync + RESYNC_SECTORS),
 			    conf->resync_lock);
 
-	conf->nr_pending++;
+	atomic_inc(&conf->nr_pending);
 	spin_unlock_irq(&conf->resync_lock);
 }
 
 static void lower_barrier(struct r1conf *conf)
 {
-	unsigned long flags;
-	BUG_ON(conf->barrier <= 0);
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->barrier--;
-	conf->nr_pending--;
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
+	BUG_ON(atomic_read(&conf->barrier) <= 0);
+	atomic_dec(&conf->barrier);
+	atomic_dec(&conf->nr_pending);
 	wake_up(&conf->wait_barrier);
 }
 
@@ -871,16 +868,22 @@ static bool need_to_wait_for_sync(struct
 {
 	bool wait = false;
 
-	if (conf->array_frozen || !bio)
+	if (test_bit(R1FLAG_ARRAY_FROZEN, &conf->flags) || !bio) {
+		atomic_inc(&conf->nr_waiting);
 		wait = true;
-	else if (conf->barrier && bio_data_dir(bio) == WRITE) {
+	}
+	else if (atomic_read(&conf->barrier) && bio_data_dir(bio) == WRITE) {
+		spin_lock_irq(&conf->resync_lock);
 		if ((conf->mddev->curr_resync_completed
 		     >= bio_end_sector(bio)) ||
 		    (conf->next_resync + NEXT_NORMALIO_DISTANCE
 		     <= bio->bi_iter.bi_sector))
 			wait = false;
-		else
+		else {
+			atomic_inc(&conf->nr_waiting);
 			wait = true;
+		}
+		spin_unlock_irq(&conf->resync_lock);
 	}
 
 	return wait;
@@ -890,9 +893,8 @@ static sector_t wait_barrier(struct r1co
 {
 	sector_t sector = 0;
 
-	spin_lock_irq(&conf->resync_lock);
 	if (need_to_wait_for_sync(conf, bio)) {
-		conf->nr_waiting++;
+		spin_lock_irq(&conf->resync_lock);
 		/* Wait for the barrier to drop.
 		 * However if there are already pending
 		 * requests (preventing the barrier from
@@ -903,17 +905,20 @@ static sector_t wait_barrier(struct r1co
 		 * to increase.
 		 */
 		wait_event_lock_irq(conf->wait_barrier,
-				    !conf->array_frozen &&
-				    (!conf->barrier ||
+				    !test_bit(R1FLAG_ARRAY_FROZEN, &conf->flags) &&
+				    (!atomic_read(&conf->barrier) ||
 				     ((conf->start_next_window <
 				       conf->next_resync + RESYNC_SECTORS) &&
 				      current->bio_list &&
 				      !bio_list_empty(current->bio_list))),
 				    conf->resync_lock);
-		conf->nr_waiting--;
+		/* nr_waiting got incremented in need_to_wait_for_sync() */
+		atomic_dec(&conf->nr_waiting);
+		spin_unlock_irq(&conf->resync_lock);
 	}
 
 	if (bio && bio_data_dir(bio) == WRITE) {
+		spin_lock_irq(&conf->resync_lock);
 		if (bio->bi_iter.bi_sector >= conf->next_resync) {
 			if (conf->start_next_window == MaxSector)
 				conf->start_next_window =
@@ -927,10 +932,9 @@ static sector_t wait_barrier(struct r1co
 				conf->current_window_requests++;
 			sector = conf->start_next_window;
 		}
+		spin_unlock_irq(&conf->resync_lock);
 	}
-
-	conf->nr_pending++;
-	spin_unlock_irq(&conf->resync_lock);
+	atomic_inc(&conf->nr_pending);
 	return sector;
 }
 
@@ -939,9 +943,9 @@ static void allow_barrier(struct r1conf
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->nr_pending--;
+	atomic_dec(&conf->nr_pending);
 	if (start_next_window) {
+		spin_lock_irqsave(&conf->resync_lock, flags);
 		if (start_next_window == conf->start_next_window) {
 			if (conf->start_next_window + NEXT_NORMALIO_DISTANCE
 			    <= bi_sector)
@@ -961,8 +965,8 @@ static void allow_barrier(struct r1conf
 			} else
 				conf->start_next_window = MaxSector;
 		}
+		spin_unlock_irqrestore(&conf->resync_lock, flags);
 	}
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
 	wake_up(&conf->wait_barrier);
 }
 
@@ -979,21 +983,22 @@ static void freeze_array(struct r1conf *
 	 * must match the number of pending IOs (nr_pending) before
 	 * we continue.
 	 */
-	spin_lock_irq(&conf->resync_lock);
-	conf->array_frozen = 1;
-	wait_event_lock_irq_cmd(conf->wait_barrier,
-				conf->nr_pending == conf->nr_queued+extra,
-				conf->resync_lock,
-				flush_pending_writes(conf));
-	spin_unlock_irq(&conf->resync_lock);
+	if (!test_and_set_bit(R1FLAG_ARRAY_FROZEN, &conf->flags)) {
+		spin_lock_irq(&conf->resync_lock);
+		wait_event_lock_irq_cmd(conf->wait_barrier,
+					atomic_read(&conf->nr_pending) ==
+					conf->nr_queued + extra,
+					conf->resync_lock,
+					flush_pending_writes(conf));
+		spin_unlock_irq(&conf->resync_lock);
+	}
 }
+
 static void unfreeze_array(struct r1conf *conf)
 {
 	/* reverse the effect of the freeze */
-	spin_lock_irq(&conf->resync_lock);
-	conf->array_frozen = 0;
-	wake_up(&conf->wait_barrier);
-	spin_unlock_irq(&conf->resync_lock);
+	if (test_and_clear_bit(R1FLAG_ARRAY_FROZEN, &conf->flags))
+		wake_up(&conf->wait_barrier);
 }
 
 /* duplicate the data pages for behind I/O
@@ -1658,7 +1663,7 @@ static int raid1_add_disk(struct mddev *
 			 * if this was recently any drive of the array
 			 */
 			if (rdev->saved_raid_disk < 0)
-				conf->fullsync = 1;
+				set_bit(R1FLAG_FULLSYNC, &conf->flags);
 			rcu_assign_pointer(p->rdev, rdev);
 			break;
 		}
@@ -1669,7 +1674,7 @@ static int raid1_add_disk(struct mddev *
 			set_bit(Replacement, &rdev->flags);
 			rdev->raid_disk = mirror;
 			err = 0;
-			conf->fullsync = 1;
+			set_bit(R1FLAG_FULLSYNC, &conf->flags);
 			rcu_assign_pointer(p[conf->raid_disks].rdev, rdev);
 			break;
 		}
@@ -2566,7 +2571,7 @@ static sector_t raid1_sync_request(struc
 			bitmap_end_sync(mddev->bitmap, mddev->curr_resync,
 						&sync_blocks, 1);
 		else /* completed sync */
-			conf->fullsync = 0;
+			clear_bit(R1FLAG_FULLSYNC, &conf->flags);
 
 		bitmap_close_sync(mddev->bitmap);
 		close_sync(conf);
@@ -2581,7 +2586,7 @@ static sector_t raid1_sync_request(struc
 	if (mddev->bitmap == NULL &&
 	    mddev->recovery_cp == MaxSector &&
 	    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&
-	    conf->fullsync == 0) {
+	    !test_bit(R1FLAG_FULLSYNC, &conf->flags)) {
 		*skipped = 1;
 		return max_sector - sector_nr;
 	}
@@ -2589,7 +2594,8 @@ static sector_t raid1_sync_request(struc
 	 * This call the bitmap_start_sync doesn't actually record anything
 	 */
 	if (!bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, 1) &&
-	    !conf->fullsync && !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {
+	    !test_bit(R1FLAG_FULLSYNC, &conf->flags) &&
+	    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {
 		/* We can skip this block, and probably several more */
 		*skipped = 1;
 		return sync_blocks;
@@ -2599,7 +2605,7 @@ static sector_t raid1_sync_request(struc
 	 * If there is non-resync activity waiting for a turn, then let it
 	 * though before starting on this new sync request.
 	 */
-	if (conf->nr_waiting)
+	if (atomic_read(&conf->nr_waiting))
 		schedule_timeout_uninterruptible(1);
 
 	/* we are incrementing sector_nr below. To be safe, we check against
@@ -2764,7 +2770,7 @@ static sector_t raid1_sync_request(struc
 		if (sync_blocks == 0) {
 			if (!bitmap_start_sync(mddev->bitmap, sector_nr,
 					       &sync_blocks, still_degraded) &&
-			    !conf->fullsync &&
+			    !test_bit(R1FLAG_FULLSYNC, &conf->flags) &&
 			    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))
 				break;
 			if ((len >> 9) > sync_blocks)
@@ -2939,7 +2945,7 @@ static struct r1conf *setup_conf(struct
 			disk->head_position = 0;
 			if (disk->rdev &&
 			    (disk->rdev->saved_raid_disk < 0))
-				conf->fullsync = 1;
+				set_bit(R1FLAG_FULLSYNC, &conf->flags);
 		}
 	}
 
@@ -3235,7 +3241,7 @@ static void *raid1_takeover(struct mddev
 		conf = setup_conf(mddev);
 		if (!IS_ERR(conf))
 			/* Array must appear to be quiesced */
-			conf->array_frozen = 1;
+			set_bit(R1FLAG_ARRAY_FROZEN, &conf->flags);
 		return conf;
 	}
 	return ERR_PTR(-EINVAL);
--- a/drivers/md/raid1.h
+++ b/drivers/md/raid1.h
@@ -79,16 +79,17 @@ struct r1conf {
 	 */
 	wait_queue_head_t	wait_barrier;
 	spinlock_t		resync_lock;
-	int			nr_pending;
-	int			nr_waiting;
+	atomic_t		nr_pending;
+	atomic_t		nr_waiting;
 	int			nr_queued;
-	int			barrier;
-	int			array_frozen;
+	atomic_t		barrier;
+	unsigned long		flags;
+#define R1FLAG_ARRAY_FROZEN 0
 
-	/* Set to 1 if a full sync is needed, (fresh device added).
+	/* Set if a full sync is needed, (fresh device added).
 	 * Cleared when a sync completes.
 	 */
-	int			fullsync;
+#define R1FLAG_FULLSYNC 1
 
 	/* When the same as mddev->recovery_disabled we don't allow
 	 * recovery to be attempted as we expect a read error.
