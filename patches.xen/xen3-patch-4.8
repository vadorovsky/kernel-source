From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: xen: Linux: 4.8
Patch-mainline: Never, SUSE-Xen specific
References: none

 This patch contains the differences between 4.7 and 4.8.

Automatically created from "patch-4.8" by xen-port-patches.py
Acked-by: jbeulich@suse.com

--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -162,7 +162,7 @@ config X86
 	select IRQ_FORCED_THREADING
 	select PERF_EVENTS
 	select RTC_LIB				if !XEN_UNPRIVILEGED_GUEST
-	select RTC_MC146818_LIB
+	select RTC_MC146818_LIB			if !XEN_UNPRIVILEGED_GUEST
 	select SPARSE_IRQ
 	select SRCU
 	select SYSCTL_EXCEPTION_TRACE
--- a/arch/x86/entry/entry_32-xen.S
+++ b/arch/x86/entry/entry_32-xen.S
@@ -1266,3 +1266,14 @@ ENTRY(async_page_fault)
 	jmp	error_code
 END(async_page_fault)
 #endif
+
+ENTRY(rewind_stack_do_exit)
+	/* Prevent any naive code from trying to unwind to our caller. */
+	xorl	%ebp, %ebp
+
+	movl	PER_CPU_VAR(cpu_current_top_of_stack), %esi
+	leal	-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%esi), %esp
+
+	call	do_exit
+1:	jmp 1b
+END(rewind_stack_do_exit)
--- a/arch/x86/entry/entry_64-xen.S
+++ b/arch/x86/entry/entry_64-xen.S
@@ -684,7 +684,6 @@ ENTRY(error_entry)
 	testb	$3, CS+8(%rsp)
 	jz	.Lerror_kernelspace
 
-.Lerror_entry_from_usermode_swapgs:
 	/*
 	 * We entered from user mode or we're pretending to have entered
 	 * from user mode due to an IRET fault.
@@ -729,7 +728,8 @@ ENTRY(error_entry)
 	 * gsbase and proceed.  We'll fix up the exception and land in
 	 * .Lgs_change's error handler with kernel gsbase.
 	 */
-	jmp	.Lerror_entry_from_usermode_swapgs
+	SWAPGS
+	jmp .Lerror_entry_done
 
 .Lbstep_iret:
 	/* Fix truncated RIP */
@@ -797,3 +797,14 @@ ENTRY(ignore_sysret)
 	HYPERVISOR_IRET VGCF_i387_valid
 END(ignore_sysret)
 #endif
+
+ENTRY(rewind_stack_do_exit)
+	/* Prevent any naive code from trying to unwind to our caller. */
+	xorl	%ebp, %ebp
+
+	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rax
+	leaq	-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%rax), %rsp
+
+	call	do_exit
+1:	jmp 1b
+END(rewind_stack_do_exit)
--- a/arch/x86/include/asm/kexec.h
+++ b/arch/x86/include/asm/kexec.h
@@ -234,11 +234,10 @@ extern void kdump_nmi_shootdown_cpus(voi
  */
 
 #ifdef CONFIG_XEN
-#define KEXEC_ARCH_HAS_PAGE_MACROS
-#define kexec_page_to_pfn(page)  pfn_to_mfn(page_to_pfn(page))
-#define kexec_pfn_to_page(pfn)   pfn_to_page(mfn_to_pfn(pfn))
-#define kexec_virt_to_phys(addr) virt_to_machine(addr)
-#define kexec_phys_to_virt(addr) phys_to_virt(machine_to_phys(addr))
+#define page_to_boot_pfn(page)	pfn_to_mfn(page_to_pfn(page))
+#define boot_pfn_to_page(mfn)	pfn_to_page(mfn_to_pfn(mfn))
+#define phys_to_boot_phys	phys_to_machine
+#define boot_phys_to_phys	machine_to_phys
 #endif
 
 #endif /* __ASSEMBLY__ */
--- a/arch/x86/include/mach-xen/asm/mmu_context.h
+++ b/arch/x86/include/mach-xen/asm/mmu_context.h
@@ -207,7 +207,7 @@ static inline void arch_exit_mmap(struct
 #ifdef CONFIG_X86_64
 static inline bool is_64bit_mm(struct mm_struct *mm)
 {
-	return	!config_enabled(CONFIG_IA32_EMULATION) ||
+	return	!IS_ENABLED(CONFIG_IA32_EMULATION) ||
 		!(mm->context.ia32_compat == TIF_IA32);
 }
 #else
--- a/arch/x86/include/mach-xen/asm/pgtable.h
+++ b/arch/x86/include/mach-xen/asm/pgtable.h
@@ -482,7 +482,7 @@ pte_t *populate_extra_pte(unsigned long
 
 static inline int pte_none(pte_t pte)
 {
-	return !pte.pte;
+	return !(pte.pte & ~(_PAGE_KNL_ERRATUM_MASK));
 }
 
 #define __HAVE_ARCH_PTE_SAME
@@ -560,7 +560,8 @@ static inline int pmd_none(pmd_t pmd)
 {
 	/* Only check low word on 32-bit platforms, since it might be
 	   out of sync with upper half. */
-	return (unsigned long)__pmd_val(pmd) == 0;
+	unsigned long val = __pmd_val(pmd);
+	return (val & ~_PAGE_KNL_ERRATUM_MASK) == 0;
 }
 
 static inline unsigned long pmd_page_vaddr(pmd_t pmd)
@@ -632,7 +633,7 @@ static inline unsigned long pages_to_mb(
 #if CONFIG_PGTABLE_LEVELS > 2
 static inline int pud_none(pud_t pud)
 {
-	return __pud_val(pud) == 0;
+	return (__pud_val(pud) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;
 }
 
 static inline int pud_present(pud_t pud)
@@ -710,6 +711,12 @@ static inline int pgd_bad(pgd_t pgd)
 
 static inline int pgd_none(pgd_t pgd)
 {
+	/*
+	 * There is no need to do a workaround for the KNL stray
+	 * A/D bit erratum here.  PGDs only point to page tables
+	 * except on 32-bit non-PAE which is not supported on
+	 * KNL.
+	 */
 	return !__pgd_val(pgd);
 }
 #endif	/* CONFIG_PGTABLE_LEVELS > 3 */
@@ -746,6 +753,23 @@ static inline int pgd_none(pgd_t pgd)
 void init_mem_mapping(void);
 void early_alloc_pgt_buf(void);
 
+#if defined(CONFIG_X86_64) && !defined(CONFIG_XEN)
+/* Realmode trampoline initialization. */
+extern pgd_t trampoline_pgd_entry;
+static inline void __meminit init_trampoline_default(void)
+{
+	/* Default trampoline pgd value */
+	trampoline_pgd_entry = init_level4_pgt[pgd_index(__PAGE_OFFSET)];
+}
+# ifdef CONFIG_RANDOMIZE_MEMORY
+void __meminit init_trampoline(void);
+# else
+#  define init_trampoline init_trampoline_default
+# endif
+#else
+static inline void init_trampoline(void) { }
+#endif
+
 /* local pte updates need not use xchg for locking */
 static inline pte_t xen_local_ptep_get_and_clear(pte_t *ptep, pte_t res)
 {
--- a/arch/x86/include/mach-xen/asm/pgtable_64.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_64.h
@@ -140,18 +140,32 @@ static inline int pgd_large(pgd_t pgd) {
 #define pte_offset_map(dir, address) pte_offset_kernel((dir), (address))
 #define pte_unmap(pte) ((void)(pte))/* NOP */
 
-/* Encode and de-code a swap entry */
+/*
+ * Encode and de-code a swap entry
+ *
+ * |     ...            | 11| 10|  9|8|7|6|5| 4| 3|2|1|0| <- bit number
+ * |     ...            |SW3|SW2|SW1|G|L|D|A|CD|WT|U|W|P| <- bit names
+ * | OFFSET (14->63) | TYPE (9-13)  |0|X|X|X| X| X|X|X|0| <- swp entry
+ *
+ * G (8) is aliased and used as a PROT_NONE indicator for
+ * !present ptes.  We need to start storing swap entries above
+ * there.  We also need to avoid using A and D because of an
+ * erratum where they can be incorrectly set by hardware on
+ * non-present PTEs.
+ */
+#define SWP_TYPE_FIRST_BIT (_PAGE_BIT_PROTNONE + 1)
 #define SWP_TYPE_BITS 5
-#define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 1)
+/* Place the offset above the type: */
+#define SWP_OFFSET_FIRST_BIT (SWP_TYPE_FIRST_BIT + SWP_TYPE_BITS)
 
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > SWP_TYPE_BITS)
 
-#define __swp_type(x)			(((x).val >> (_PAGE_BIT_PRESENT + 1)) \
+#define __swp_type(x)			(((x).val >> (SWP_TYPE_FIRST_BIT)) \
 					 & ((1U << SWP_TYPE_BITS) - 1))
-#define __swp_offset(x)			((x).val >> SWP_OFFSET_SHIFT)
+#define __swp_offset(x)			((x).val >> SWP_OFFSET_FIRST_BIT)
 #define __swp_entry(type, offset)	((swp_entry_t) { \
-					 ((type) << (_PAGE_BIT_PRESENT + 1)) \
-					 | ((offset) << SWP_OFFSET_SHIFT) })
+					 ((type) << (SWP_TYPE_FIRST_BIT)) \
+					 | ((offset) << SWP_OFFSET_FIRST_BIT) })
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { __pte_val(pte) })
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
 
--- a/arch/x86/include/mach-xen/asm/pgtable_64_types.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_64_types.h
@@ -5,6 +5,7 @@
 
 #ifndef __ASSEMBLY__
 #include <linux/types.h>
+#include <asm/kaslr.h>
 
 /*
  * These are used to make use of C type-checking..
@@ -54,10 +55,16 @@ typedef union { pteval_t pte; unsigned i
 
 /* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
 #define MAX_PHYSMEM_BITS 43
-#define MAXMEM		 _AC(__AC(1, UL) << MAX_PHYSMEM_BITS, UL)
-#define VMALLOC_START    _AC(0xffffc90000000000, UL)
-#define VMALLOC_END      _AC(0xffffe8ffffffffff, UL)
-#define VMEMMAP_START	 _AC(0xffffea0000000000, UL)
+#define MAXMEM		_AC(__AC(1, UL) << MAX_PHYSMEM_BITS, UL)
+#define VMALLOC_SIZE_TB	_AC(32, UL)
+#define __VMALLOC_BASE	_AC(0xffffc90000000000, UL)
+#define VMEMMAP_START	_AC(0xffffea0000000000, UL)
+#ifdef CONFIG_RANDOMIZE_MEMORY
+#define VMALLOC_START	vmalloc_base
+#else
+#define VMALLOC_START	__VMALLOC_BASE
+#endif /* CONFIG_RANDOMIZE_MEMORY */
+#define VMALLOC_END	(VMALLOC_START + _AC((VMALLOC_SIZE_TB << 40) - 1, UL))
 #define MODULES_VADDR    (__START_KERNEL_map + KERNEL_IMAGE_SIZE)
 #define MODULES_END      _AC(0xffffffffff000000, UL)
 #define MODULES_LEN   (MODULES_END - MODULES_VADDR)
--- a/arch/x86/include/mach-xen/asm/pgtable_types.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_types.h
@@ -71,6 +71,12 @@
 			 _PAGE_PKEY_BIT2 | \
 			 _PAGE_PKEY_BIT3)
 
+#if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)
+#define _PAGE_KNL_ERRATUM_MASK (_PAGE_DIRTY | _PAGE_ACCESSED)
+#else
+#define _PAGE_KNL_ERRATUM_MASK 0
+#endif
+
 #ifdef CONFIG_KMEMCHECK
 #define _PAGE_HIDDEN	(_AT(pteval_t, 1) << _PAGE_BIT_HIDDEN)
 #else
@@ -525,8 +531,6 @@ extern pmd_t *lookup_pmd_address(unsigne
 extern phys_addr_t slow_virt_to_phys(void *__address);
 extern int kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,
 				   unsigned numpages, unsigned long page_flags);
-void kernel_unmap_pages_in_pgd(pgd_t *root, unsigned long address,
-			       unsigned numpages);
 #endif	/* !__ASSEMBLY__ */
 
 #endif /* _ASM_X86_PGTABLE_DEFS_H */
--- a/arch/x86/include/mach-xen/asm/processor.h
+++ b/arch/x86/include/mach-xen/asm/processor.h
@@ -387,10 +387,15 @@ DECLARE_PER_CPU(struct irq_stack *, hard
 DECLARE_PER_CPU(struct irq_stack *, softirq_stack);
 #endif	/* X86_64 */
 
-extern unsigned int xstate_size;
+extern unsigned int fpu_kernel_xstate_size;
+extern unsigned int fpu_user_xstate_size;
 
 struct perf_event;
 
+typedef struct {
+	unsigned long		seg;
+} mm_segment_t;
+
 struct thread_struct {
 	/* Cached TLS descriptors: */
 	struct desc_struct	tls_array[GDT_ENTRY_TLS_ENTRIES];
@@ -439,6 +444,11 @@ struct thread_struct {
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
 
+	mm_segment_t		addr_limit;
+
+	unsigned int		sig_on_uaccess_err:1;
+	unsigned int		uaccess_err:1;	/* uaccess failed */
+
 	/* Floating point and extended processor state */
 	struct fpu		fpu;
 	/*
@@ -495,11 +505,6 @@ static inline unsigned long current_top_
 
 #define set_iopl_mask xen_set_iopl_mask
 
-typedef struct {
-	unsigned long		seg;
-} mm_segment_t;
-
-
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);
 
@@ -721,6 +726,7 @@ static inline void spin_lock_prefetch(co
 	.sp0			= TOP_OF_INIT_STACK,			  \
 	.sysenter_cs		= __KERNEL_CS,				  \
 	.io_bitmap_ptr		= NULL,					  \
+	.addr_limit		= KERNEL_DS,				  \
 }
 
 extern unsigned long thread_saved_pc(struct task_struct *tsk);
@@ -768,8 +774,9 @@ extern unsigned long thread_saved_pc(str
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
-#define INIT_THREAD  { \
-	.sp0 = TOP_OF_INIT_STACK \
+#define INIT_THREAD  {						\
+	.sp0			= TOP_OF_INIT_STACK,		\
+	.addr_limit		= KERNEL_DS,			\
 }
 
 /*
--- a/arch/x86/include/mach-xen/asm/smp-processor-id.h
+++ b/arch/x86/include/mach-xen/asm/smp-processor-id.h
@@ -15,14 +15,6 @@ DECLARE_PER_CPU(int, cpu_number);
 #define raw_smp_processor_id() this_cpu_read_4(cpu_number)
 #define safe_smp_processor_id() smp_processor_id()
 
-#ifdef CONFIG_X86_64_SMP
-#define stack_smp_processor_id()					\
-({									\
-	struct thread_info *ti;						\
-	__asm__("andq %%rsp,%0; ":"=r" (ti) : "0" (CURRENT_MASK));	\
-	ti->cpu;							\
-})
-#endif
 
 #ifdef CONFIG_DEBUG_PREEMPT
 extern unsigned int debug_smp_processor_id(void);
--- a/arch/x86/include/mach-xen/asm/smp.h
+++ b/arch/x86/include/mach-xen/asm/smp.h
@@ -33,6 +33,7 @@ static inline struct cpumask *cpu_llc_sh
 }
 
 DECLARE_EARLY_PER_CPU_READ_MOSTLY(u16, x86_cpu_to_apicid);
+DECLARE_EARLY_PER_CPU_READ_MOSTLY(u32, x86_cpu_to_acpiid);
 DECLARE_EARLY_PER_CPU_READ_MOSTLY(u16, x86_bios_cpu_apicid);
 #if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_X86_32)
 DECLARE_EARLY_PER_CPU_READ_MOSTLY(int, x86_cpu_to_logical_apicid);
@@ -139,6 +140,7 @@ int native_cpu_up(unsigned int cpunum, s
 int native_cpu_disable(void);
 int common_cpu_die(unsigned int cpu);
 void native_cpu_die(unsigned int cpu);
+void hlt_play_dead(void);
 void native_play_dead(void);
 void play_dead_common(void);
 void wbinvd_on_cpu(int cpu);
@@ -148,6 +150,7 @@ void x86_idle_thread_init(unsigned int c
 void smp_store_boot_cpu_info(void);
 void smp_store_cpu_info(int id);
 #define cpu_physical_id(cpu)	per_cpu(x86_cpu_to_apicid, cpu)
+#define cpu_acpi_id(cpu)	per_cpu(x86_cpu_to_acpiid, cpu)
 
 #else /* CONFIG_XEN */
 
--- a/arch/x86/include/mach-xen/asm/special_insns.h
+++ b/arch/x86/include/mach-xen/asm/special_insns.h
@@ -300,52 +300,6 @@ static inline void clwb(volatile void *_
 		: [pax] "a" (p));
 }
 
-/**
- * pcommit_sfence() - persistent commit and fence
- *
- * The PCOMMIT instruction ensures that data that has been flushed from the
- * processor's cache hierarchy with CLWB, CLFLUSHOPT or CLFLUSH is accepted to
- * memory and is durable on the DIMM.  The primary use case for this is
- * persistent memory.
- *
- * This function shows how to properly use CLWB/CLFLUSHOPT/CLFLUSH and PCOMMIT
- * with appropriate fencing.
- *
- * Example:
- * void flush_and_commit_buffer(void *vaddr, unsigned int size)
- * {
- *         unsigned long clflush_mask = boot_cpu_data.x86_clflush_size - 1;
- *         void *vend = vaddr + size;
- *         void *p;
- *
- *         for (p = (void *)((unsigned long)vaddr & ~clflush_mask);
- *              p < vend; p += boot_cpu_data.x86_clflush_size)
- *                 clwb(p);
- *
- *         // SFENCE to order CLWB/CLFLUSHOPT/CLFLUSH cache flushes
- *         // MFENCE via mb() also works
- *         wmb();
- *
- *         // PCOMMIT and the required SFENCE for ordering
- *         pcommit_sfence();
- * }
- *
- * After this function completes the data pointed to by 'vaddr' has been
- * accepted to memory and will be durable if the 'vaddr' points to persistent
- * memory.
- *
- * PCOMMIT must always be ordered by an MFENCE or SFENCE, so to help simplify
- * things we include both the PCOMMIT and the required SFENCE in the
- * alternatives generated by pcommit_sfence().
- */
-static inline void pcommit_sfence(void)
-{
-	alternative(ASM_NOP7,
-		    ".byte 0x66, 0x0f, 0xae, 0xf8\n\t" /* pcommit */
-		    "sfence",
-		    X86_FEATURE_PCOMMIT);
-}
-
 #define nop() asm volatile ("nop")
 
 
--- a/arch/x86/include/mach-xen/asm/tlbflush.h
+++ b/arch/x86/include/mach-xen/asm/tlbflush.h
@@ -29,7 +29,7 @@ DECLARE_PER_CPU_SHARED_ALIGNED(struct tl
 /* Initialize cr4 shadow for this CPU. */
 static inline void cr4_init_shadow(void)
 {
-	this_cpu_write(cpu_tlbstate.cr4, __read_cr4());
+	this_cpu_write(cpu_tlbstate.cr4, __read_cr4_safe());
 }
 
 /* Set in this cpu's CR4. */
--- a/arch/x86/kernel/apic/io_apic-xen.c
+++ b/arch/x86/kernel/apic/io_apic-xen.c
@@ -39,7 +39,7 @@
 #include <linux/mc146818rtc.h>
 #include <linux/compiler.h>
 #include <linux/acpi.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/syscore_ops.h>
 #include <linux/freezer.h>
 #include <linux/kthread.h>
@@ -1053,7 +1053,7 @@ static int alloc_irq_from_domain(struct
 
 	return __irq_domain_alloc_irqs(domain, irq, 1,
 				       ioapic_alloc_attr_node(info),
-				       info, legacy);
+				       info, legacy, NULL);
 }
 
 /*
@@ -1086,7 +1086,8 @@ static int alloc_isa_irq_from_domain(str
 					  info->ioapic_pin))
 			return -ENOMEM;
 	} else {
-		irq = __irq_domain_alloc_irqs(domain, irq, 1, node, info, true);
+		irq = __irq_domain_alloc_irqs(domain, irq, 1, node, info, true,
+					      NULL);
 		if (irq >= 0) {
 			irq_data = irq_domain_get_irq_data(domain, irq);
 			data = irq_data->chip_data;
@@ -2660,29 +2661,25 @@ static struct resource * __init ioapic_s
 	unsigned long n;
 	struct resource *res;
 	char *mem;
-	int i, num = 0;
+	int i;
 
-	for_each_ioapic(i)
-		num++;
-	if (num == 0)
+	if (nr_ioapics == 0)
 		return NULL;
 
 	n = IOAPIC_RESOURCE_NAME_SIZE + sizeof(struct resource);
-	n *= num;
+	n *= nr_ioapics;
 
 	mem = alloc_bootmem(n);
 	res = (void *)mem;
 
-	mem += sizeof(struct resource) * num;
+	mem += sizeof(struct resource) * nr_ioapics;
 
-	num = 0;
 	for_each_ioapic(i) {
-		res[num].name = mem;
-		res[num].flags = IORESOURCE_MEM | IORESOURCE_BUSY;
+		res[i].name = mem;
+		res[i].flags = IORESOURCE_MEM | IORESOURCE_BUSY;
 		snprintf(mem, IOAPIC_RESOURCE_NAME_SIZE, "IOAPIC %u", i);
 		mem += IOAPIC_RESOURCE_NAME_SIZE;
-		ioapics[i].iomem_res = &res[num];
-		num++;
+		ioapics[i].iomem_res = &res[i];
 	}
 
 	ioapic_resources = res;
--- a/arch/x86/kernel/apic/probe_32-xen.c
+++ b/arch/x86/kernel/apic/probe_32-xen.c
@@ -8,7 +8,7 @@
  */
 #include <linux/threads.h>
 #include <linux/cpumask.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/string.h>
 #include <linux/kernel.h>
 #include <linux/ctype.h>
--- a/arch/x86/kernel/apic/vector-xen.c
+++ b/arch/x86/kernel/apic/vector-xen.c
@@ -532,7 +532,7 @@ static int apic_set_affinity(struct irq_
 	struct apic_chip_data *data = irq_data->chip_data;
 	int err, irq = irq_data->irq;
 
-	if (!config_enabled(CONFIG_SMP))
+	if (!IS_ENABLED(CONFIG_SMP))
 		return -EPERM;
 
 	if (!cpumask_intersects(dest, cpu_online_mask))
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -707,11 +707,13 @@ static void init_amd_gh(struct cpuinfo_x
 
 static void init_amd_ln(struct cpuinfo_x86 *c)
 {
+#ifndef CONFIG_XEN
 	/*
 	 * Apply erratum 665 fix unconditionally so machines without a BIOS
 	 * fix work.
 	 */
 	msr_set_bit(MSR_AMD64_DE_CFG, 31);
+#endif
 }
 
 static void init_amd_bd(struct cpuinfo_x86 *c)
--- a/arch/x86/kernel/cpu/common-xen.c
+++ b/arch/x86/kernel/cpu/common-xen.c
@@ -2,7 +2,7 @@
 #include <linux/linkage.h>
 #include <linux/bitops.h>
 #include <linux/kernel.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/percpu.h>
 #include <linux/string.h>
 #include <linux/ctype.h>
@@ -853,21 +853,20 @@ static void __init early_identify_cpu(st
 		identify_cpu_without_cpuid(c);
 
 	/* cyrix could have cpuid enabled via c_identify()*/
-	if (!have_cpuid_p())
-		return;
+	if (have_cpuid_p()) {
+		cpu_detect(c);
+		get_cpu_vendor(c);
+		get_cpu_cap(c);
 
-	cpu_detect(c);
-	get_cpu_vendor(c);
-	get_cpu_cap(c);
-
-	if (this_cpu->c_early_init)
-		this_cpu->c_early_init(c);
+		if (this_cpu->c_early_init)
+			this_cpu->c_early_init(c);
 
-	c->cpu_index = 0;
-	filter_cpuid_features(c, false);
+		c->cpu_index = 0;
+		filter_cpuid_features(c, false);
 
-	if (this_cpu->c_bsp_init)
-		this_cpu->c_bsp_init(c);
+		if (this_cpu->c_bsp_init)
+			this_cpu->c_bsp_init(c);
+	}
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 	fpu__init_system(c);
@@ -1578,7 +1577,7 @@ void cpu_init(void)
 	int i;
 #endif
 	struct task_struct *me;
-	int cpu = stack_smp_processor_id();
+	int cpu = raw_smp_processor_id();
 
 	wait_for_master_cpu(cpu);
 
--- a/arch/x86/kernel/cpu/mtrr/main-xen.c
+++ b/arch/x86/kernel/cpu/mtrr/main-xen.c
@@ -1,7 +1,7 @@
 #define DEBUG
 
 #include <linux/uaccess.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/mutex.h>
 #include <linux/init.h>
 
--- a/arch/x86/kernel/head32-xen.c
+++ b/arch/x86/kernel/head32-xen.c
@@ -23,8 +23,6 @@ static void __init i386_default_early_se
 	x86_init.resources.reserve_resources = i386_reserve_resources;
 #ifndef CONFIG_XEN
 	x86_init.mpparse.setup_ioapic_ids = setup_ioapic_ids_from_mpc;
-
-	reserve_ebda_region();
 #endif
 }
 
--- a/arch/x86/kernel/mpparse-xen.c
+++ b/arch/x86/kernel/mpparse-xen.c
@@ -16,7 +16,6 @@
 #include <linux/mc146818rtc.h>
 #include <linux/bitops.h>
 #include <linux/acpi.h>
-#include <linux/module.h>
 #include <linux/smp.h>
 #include <linux/pci.h>
 
--- a/arch/x86/kernel/pci-dma-xen.c
+++ b/arch/x86/kernel/pci-dma-xen.c
@@ -109,7 +109,7 @@ void __init pci_iommu_alloc(void)
 }
 void *dma_generic_alloc_coherent(struct device *dev, size_t size,
 				 dma_addr_t *dma_addr, gfp_t flag,
-				 struct dma_attrs *attrs)
+				 unsigned long attrs)
 {
 	unsigned long dma_mask;
 	struct page *page;
@@ -173,7 +173,7 @@ again:
 }
 
 void dma_generic_free_coherent(struct device *dev, size_t size, void *vaddr,
-			       dma_addr_t dma_addr, struct dma_attrs *attrs)
+			       dma_addr_t dma_addr, unsigned long attrs)
 {
 	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
 	struct page *page = virt_to_page(vaddr);
--- a/arch/x86/kernel/pci-nommu-xen.c
+++ b/arch/x86/kernel/pci-nommu-xen.c
@@ -23,7 +23,7 @@ do {							\
 static dma_addr_t
 gnttab_map_page(struct device *dev, struct page *page, unsigned long offset,
 		size_t size, enum dma_data_direction dir,
-		struct dma_attrs *attrs)
+		unsigned long attrs)
 {
 	dma_addr_t dma;
 
@@ -39,14 +39,14 @@ gnttab_map_page(struct device *dev, stru
 
 static void
 gnttab_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
-		  enum dma_data_direction dir, struct dma_attrs *attrs)
+		  enum dma_data_direction dir, unsigned long attrs)
 {
 	gnttab_dma_unmap_page(dma_addr);
 }
 
 static int
 gnttab_map_sg(struct device *hwdev, struct scatterlist *sgl, int nents,
-	      enum dma_data_direction dir, struct dma_attrs *attrs)
+	      enum dma_data_direction dir, unsigned long attrs)
 {
 	unsigned int i;
 	struct scatterlist *sg;
@@ -66,7 +66,7 @@ gnttab_map_sg(struct device *hwdev, stru
 
 static void
 gnttab_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nents,
-		enum dma_data_direction dir, struct dma_attrs *attrs)
+		enum dma_data_direction dir, unsigned long attrs)
 {
 	unsigned int i;
 	struct scatterlist *sg;
--- a/arch/x86/kernel/process-xen.c
+++ b/arch/x86/kernel/process-xen.c
@@ -7,7 +7,8 @@
 #include <linux/prctl.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
-#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/export.h>
 #include <linux/pm.h>
 #include <linux/tick.h>
 #include <linux/random.h>
@@ -399,7 +400,7 @@ static int prefer_mwait_c1_over_halt(con
 	if (c->x86_vendor != X86_VENDOR_INTEL)
 		return 0;
 
-	if (!cpu_has(c, X86_FEATURE_MWAIT))
+	if (!cpu_has(c, X86_FEATURE_MWAIT) || static_cpu_has_bug(X86_BUG_MONITOR))
 		return 0;
 
 	return 1;
--- a/arch/x86/kernel/process_32-xen.c
+++ b/arch/x86/kernel/process_32-xen.c
@@ -25,7 +25,7 @@
 #include <linux/delay.h>
 #include <linux/reboot.h>
 #include <linux/mc146818rtc.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/kallsyms.h>
 #include <linux/ptrace.h>
 #include <linux/personality.h>
--- a/arch/x86/kernel/process_64-xen.c
+++ b/arch/x86/kernel/process_64-xen.c
@@ -29,7 +29,7 @@
 #include <linux/user.h>
 #include <linux/interrupt.h>
 #include <linux/delay.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/ptrace.h>
 #include <linux/notifier.h>
 #include <linux/kprobes.h>
--- a/arch/x86/kernel/setup-xen.c
+++ b/arch/x86/kernel/setup-xen.c
@@ -36,7 +36,7 @@
 #include <linux/console.h>
 #include <linux/root_dev.h>
 #include <linux/highmem.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/efi.h>
 #include <linux/init.h>
 #include <linux/edd.h>
@@ -110,6 +110,7 @@
 #include <asm/mce.h>
 #include <asm/alternative.h>
 #include <asm/prom.h>
+#include <asm/kaslr.h>
 
 #ifdef CONFIG_XEN
 #include <asm/hypervisor.h>
@@ -461,10 +462,6 @@ static void __init reserve_initrd(void)
 	memblock_free(ramdisk_image, ramdisk_end - ramdisk_image);
 }
 
-static void __init early_initrd_acpi_init(void)
-{
-	early_acpi_table_init((void *)initrd_start, initrd_end - initrd_start);
-}
 #else
 static void __init early_reserve_initrd(void)
 {
@@ -472,9 +469,6 @@ static void __init early_reserve_initrd(
 static void __init reserve_initrd(void)
 {
 }
-static void __init early_initrd_acpi_init(void)
-{
-}
 #endif /* CONFIG_BLK_DEV_INITRD */
 
 static void __init parse_setup_data(void)
@@ -1198,6 +1192,12 @@ void __init setup_arch(char **cmdline_p)
 
 	max_possible_pfn = max_pfn;
 
+	/*
+	 * Define random base addresses for memory sections after max_pfn is
+	 * defined and before each memory section base is used.
+	 */
+	kernel_randomize_memory();
+
 #ifdef CONFIG_X86_32
 	/* max_low_pfn get updated here */
 	find_low_pfn_range();
@@ -1244,6 +1244,8 @@ void __init setup_arch(char **cmdline_p)
 		efi_find_mirror();
 	}
 
+	reserve_bios_regions();
+
 	/*
 	 * The EFI specification says that boot service code won't be called
 	 * after ExitBootServices(). This is, in fact, a lie.
@@ -1274,9 +1276,13 @@ void __init setup_arch(char **cmdline_p)
 
 	early_trap_pf_init();
 
-#ifndef CONFIG_XEN
-	setup_real_mode();
-#endif
+	/*
+	 * Update mmu_cr4_features (and, indirectly, trampoline_cr4_features)
+	 * with the current CR4 value.  This may not be necessary, but
+	 * auditing all the early-boot CR4 manipulation would be needed to
+	 * rule it out.
+	 */
+	mmu_cr4_features = __read_cr4_safe();
 
 	memblock_set_current_limit(get_max_mapped());
 
@@ -1293,7 +1299,7 @@ void __init setup_arch(char **cmdline_p)
 
 	reserve_initrd();
 
-	early_initrd_acpi_init();
+	acpi_table_upgrade();
 
 	vsmp_init();
 
@@ -1332,13 +1338,6 @@ void __init setup_arch(char **cmdline_p)
 
 	kasan_init();
 
-	if (boot_cpu_data.cpuid_level >= 0) {
-		/* A CPU has %cr4 if and only if it has CPUID */
-		mmu_cr4_features = __read_cr4();
-		if (trampoline_cr4_features)
-			*trampoline_cr4_features = mmu_cr4_features;
-	}
-
 #if defined(CONFIG_X86_32) && !defined(CONFIG_XEN)
 	/* sync back kernel address range */
 	clone_pgd_range(initial_page_table + KERNEL_PGD_BOUNDARY,
--- a/arch/x86/kernel/traps-xen.c
+++ b/arch/x86/kernel/traps-xen.c
@@ -21,7 +21,7 @@
 #include <linux/kdebug.h>
 #include <linux/kgdb.h>
 #include <linux/kernel.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/ptrace.h>
 #include <linux/uprobes.h>
 #include <linux/string.h>
--- a/arch/x86/kernel/x86_init-xen.c
+++ b/arch/x86/kernel/x86_init-xen.c
@@ -7,7 +7,7 @@
 #include <linux/init.h>
 #include <linux/ioport.h>
 #include <linux/list.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/pci.h>
 #include <linux/spinlock_types.h>
 #include <linux/threads.h>
--- a/arch/x86/lib/cache-smp-xen.c
+++ b/arch/x86/lib/cache-smp-xen.c
@@ -1,5 +1,5 @@
 #include <linux/smp.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <asm/hypervisor.h>
 
 static void __wbinvd(void *dummy)
--- a/arch/x86/mm/dump_pagetables-xen.c
+++ b/arch/x86/mm/dump_pagetables-xen.c
@@ -13,9 +13,8 @@
  */
 
 #include <linux/debugfs.h>
-#include <linux/init.h>
 #include <linux/mm.h>
-#include <linux/module.h>
+#include <linux/init.h>
 #include <linux/seq_file.h>
 
 #include <xen/interface/xen.h>
@@ -76,9 +75,9 @@ static struct addr_marker address_marker
 	{ 0, "User Space" },
 #ifdef CONFIG_X86_64
 	{ HYPERVISOR_VIRT_START,      "Hypervisor Space" },
-	{ PAGE_OFFSET,                "Low Kernel Mapping" },
-	{ VMALLOC_START,              "vmalloc() Area" },
-	{ VMEMMAP_START,              "Vmemmap" },
+	{ 0/* PAGE_OFFSET */,         "Low Kernel Mapping" },
+	{ 0/* VMALLOC_START */,       "vmalloc() Area" },
+	{ 0/* VMEMMAP_START */,       "Vmemmap" },
 # ifdef CONFIG_X86_ESPFIX64
 	{ ESPFIX_BASE_ADDR,           "ESPfix Area", 16 },
 # endif
@@ -450,8 +449,16 @@ void ptdump_walk_pgd_level_checkwx(void)
 
 static int __init pt_dump_init(void)
 {
+	/*
+	 * Various markers are not compile-time constants, so assign them
+	 * here.
+	 */
+#ifdef CONFIG_X86_64
+	address_markers[LOW_KERNEL_NR].start_address = PAGE_OFFSET;
+	address_markers[VMALLOC_START_NR].start_address = VMALLOC_START;
+	address_markers[VMEMMAP_START_NR].start_address = VMEMMAP_START;
+#endif
 #ifdef CONFIG_X86_32
-	/* Not a compile-time constant on x86-32 */
 	address_markers[VMALLOC_START_NR].start_address = VMALLOC_START;
 	address_markers[VMALLOC_END_NR].start_address = VMALLOC_END;
 # ifdef CONFIG_HIGHMEM
@@ -463,8 +470,4 @@ static int __init pt_dump_init(void)
 
 	return 0;
 }
-
 __initcall(pt_dump_init);
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("Arjan van de Ven <arjan@linux.intel.com>");
-MODULE_DESCRIPTION("Kernel debugging helper that dumps pagetables");
--- a/arch/x86/mm/fault-xen.c
+++ b/arch/x86/mm/fault-xen.c
@@ -447,7 +447,7 @@ static noinline int vmalloc_fault(unsign
 	 * happen within a race in page table update. In the later
 	 * case just flush:
 	 */
-	pgd = pgd_offset(current->active_mm, address);
+	pgd = (pgd_t *)__va(read_cr3()) + pgd_index(address);
 	pgd_ref = pgd_offset_k(address);
 	if (pgd_none(*pgd_ref))
 		return -1;
@@ -746,7 +746,7 @@ no_context(struct pt_regs *regs, unsigne
 		 * In this case we need to make sure we're not recursively
 		 * faulting through the emulate_vsyscall() logic.
 		 */
-		if (current_thread_info()->sig_on_uaccess_error && signal) {
+		if (current->thread.sig_on_uaccess_err && signal) {
 			tsk->thread.trap_nr = X86_TRAP_PF;
 			tsk->thread.error_code = error_code | PF_USER;
 			tsk->thread.cr2 = address;
@@ -1382,7 +1382,7 @@ good_area:
 	 * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if
 	 * we get VM_FAULT_RETRY back, the mmap_sem has been unlocked.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 	major |= fault & VM_FAULT_MAJOR;
 
 	/*
--- a/arch/x86/mm/highmem_32-xen.c
+++ b/arch/x86/mm/highmem_32-xen.c
@@ -1,5 +1,5 @@
 #include <linux/highmem.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/swap.h> /* for totalram_pages */
 #include <linux/bootmem.h>
 
--- a/arch/x86/mm/ident_map-xen.c
+++ b/arch/x86/mm/ident_map-xen.c
@@ -3,15 +3,17 @@
  * included by both the compressed kernel and the regular kernel.
  */
 
-static void ident_pmd_init(unsigned long pmd_flag, pmd_t *pmd_page,
+static void ident_pmd_init(struct x86_mapping_info *info, pmd_t *pmd_page,
 			   unsigned long addr, unsigned long end)
 {
 	addr &= PMD_MASK;
 	for (; addr < end; addr += PMD_SIZE) {
 		pmd_t *pmd = pmd_page + pmd_index(addr);
 
-		if (!pmd_present(*pmd))
-			*pmd = __pmd_ma(addr | pmd_flag);
+		if (pmd_present(*pmd))
+			continue;
+
+		*pmd = __pmd_ma((addr - info->offset) | info->pmd_flag);
 	}
 }
 
@@ -30,13 +32,13 @@ static int ident_pud_init(struct x86_map
 
 		if (pud_present(*pud)) {
 			pmd = pmd_offset(pud, 0);
-			ident_pmd_init(info->pmd_flag, pmd, addr, next);
+			ident_pmd_init(info, pmd, addr, next);
 			continue;
 		}
 		pmd = (pmd_t *)info->alloc_pgt_page(info->context);
 		if (!pmd)
 			return -ENOMEM;
-		ident_pmd_init(info->pmd_flag, pmd, addr, next);
+		ident_pmd_init(info, pmd, addr, next);
 		*pud = __pud(__pa(pmd) | _KERNPG_TABLE);
 	}
 
@@ -44,14 +46,15 @@ static int ident_pud_init(struct x86_map
 }
 
 int kernel_ident_mapping_init(struct x86_mapping_info *info, pgd_t *pgd_page,
-			      unsigned long addr, unsigned long end)
+			      unsigned long pstart, unsigned long pend)
 {
+	unsigned long addr = pstart + info->offset;
+	unsigned long end = pend + info->offset;
 	unsigned long next;
 	int result;
-	int off = info->kernel_mapping ? pgd_index(__PAGE_OFFSET) : 0;
 
 	for (; addr < end; addr = next) {
-		pgd_t *pgd = pgd_page + pgd_index(addr) + off;
+		pgd_t *pgd = pgd_page + pgd_index(addr);
 		pud_t *pud;
 
 		next = (addr & PGDIR_MASK) + PGDIR_SIZE;
--- a/arch/x86/mm/init-xen.c
+++ b/arch/x86/mm/init-xen.c
@@ -17,6 +17,7 @@
 #include <asm/proto.h>
 #include <asm/dma.h>		/* for MAX_DMA_PFN */
 #include <asm/microcode.h>
+#include <asm/kaslr.h>
 
 /*
  * We need to define the tracepoints somewhere, and tlb.c
@@ -130,8 +131,18 @@ __ref void *alloc_low_pages(unsigned int
 	return __va(pfn << PAGE_SHIFT);
 }
 
-/* need 3 4k for initial PMD_SIZE,  3 4k for 0-ISA_END_ADDRESS */
-#define INIT_PGT_BUF_SIZE	(6 * PAGE_SIZE)
+/*
+ * By default need 3 4k for initial PMD_SIZE,  3 4k for 0-ISA_END_ADDRESS.
+ * With KASLR memory randomization, depending on the machine e820 memory
+ * and the PUD alignment. We may need twice more pages when KASLR memory
+ * randomization is enabled.
+ */
+#ifndef CONFIG_RANDOMIZE_MEMORY
+#define INIT_PGD_PAGE_COUNT      6
+#else
+#define INIT_PGD_PAGE_COUNT      12
+#endif
+#define INIT_PGT_BUF_SIZE	(INIT_PGD_PAGE_COUNT * PAGE_SIZE)
 RESERVE_BRK(early_pgt_alloc, INIT_PGT_BUF_SIZE);
 void  __init early_alloc_pgt_buf(void)
 {
@@ -228,7 +239,7 @@ static int __meminit save_mr(struct map_
  * adjust the page_size_mask for small range to go with
  *	big page size instead small one if nearby are ram too.
  */
-static void __init_refok adjust_range_page_size_mask(struct map_range *mr,
+static void __ref adjust_range_page_size_mask(struct map_range *mr,
 							 int nr_range)
 {
 	int i;
@@ -416,7 +427,7 @@ bool pfn_range_is_mapped(unsigned long s
  * This runs before bootmem is initialized and gets pages directly from
  * the physical memory. To access them they are temporarily mapped.
  */
-unsigned long __init_refok init_memory_mapping(unsigned long start,
+unsigned long __ref init_memory_mapping(unsigned long start,
 					       unsigned long end)
 {
 	struct map_range mr[NR_RANGE_MR];
@@ -608,6 +619,9 @@ void __init init_mem_mapping(void)
 	end = max_low_pfn << PAGE_SHIFT;
 #endif
 
+	/* Init the trampoline, possibly with KASLR memory offset */
+	init_trampoline();
+
 	/*
 	 * If the allocation is in bottom-up direction, we setup direct mapping
 	 * in bottom-up, otherwise we setup direct mapping in top-down.
@@ -727,13 +741,6 @@ void free_initmem(void)
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
 	/*
-	 * Remember, initrd memory may contain microcode or other useful things.
-	 * Before we lose initrd mem, we need to find a place to hold them
-	 * now that normal virtual memory is enabled.
-	 */
-	save_microcode_in_initrd();
-
-	/*
 	 * end could be not aligned, and We can not align that,
 	 * decompresser could be confused by aligned initrd_end
 	 * We already reserve the end partial page before in
--- a/arch/x86/mm/init_32-xen.c
+++ b/arch/x86/mm/init_32-xen.c
@@ -5,7 +5,6 @@
  *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
  */
 
-#include <linux/module.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
--- a/arch/x86/mm/init_64-xen.c
+++ b/arch/x86/mm/init_64-xen.c
@@ -30,7 +30,6 @@
 #include <linux/pfn.h>
 #include <linux/poison.h>
 #include <linux/dma-mapping.h>
-#include <linux/module.h>
 #include <linux/memory.h>
 #include <linux/memory_hotplug.h>
 #include <linux/memremap.h>
@@ -427,23 +426,29 @@ static inline int __meminit make_readonl
 	return readonly;
 }
 
+/*
+ * Create PTE level page table mapping for physical addresses.
+ * It returns the last physical address mapped.
+ */
 static unsigned long __meminit
-phys_pte_init(pte_t *pte_page, unsigned long addr, unsigned long end,
+phys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,
 	      pgprot_t prot)
 {
-	unsigned long pages = 0, next;
-	unsigned long last_map_addr = end;
+	unsigned long pages = 0, paddr_next;
+	unsigned long paddr_last = paddr_end;
+	pte_t *pte;
 	int i;
 
-	pte_t *pte = pte_page + pte_index(addr);
+	pte = pte_page + pte_index(paddr);
+	i = pte_index(paddr);
 
-	for (i = pte_index(addr); i < PTRS_PER_PTE; i++, addr = next, pte++) {
-		unsigned long pteval = addr | pgprot_val(prot);
+	for (; i < PTRS_PER_PTE; i++, paddr = paddr_next, pte++) {
+		unsigned long pteval = paddr | pgprot_val(prot);
 
-		next = (addr & PAGE_MASK) + PAGE_SIZE;
-		if (addr >= end ||
+		paddr_next = (paddr & PAGE_MASK) + PAGE_SIZE;
+		if (paddr >= paddr_end ||
 		    (!after_bootmem &&
-		     (addr >> PAGE_SHIFT) >= xen_start_info->nr_pages))
+		     (paddr >> PAGE_SHIFT) >= xen_start_info->nr_pages))
 			break;
 
 		/*
@@ -452,54 +457,59 @@ phys_pte_init(pte_t *pte_page, unsigned
 		 * pagetable pages as RO. So assume someone who pre-setup
 		 * these mappings are more intelligent.
 		 */
-		if (__pte_val(*pte)) {
+		if (!pte_none(*pte)) {
 			if (!after_bootmem)
 				pages++;
 			continue;
 		}
 
-		if (make_readonly(addr))
+		if (make_readonly(paddr))
 			pteval &= ~_PAGE_RW;
 		if (0)
-			printk("   pte=%p addr=%lx pte=%016lx\n",
-			       pte, addr, pteval);
+			pr_info("   pte=%p addr=%lx pte=%016lx\n", pte, paddr,
+				pteval);
 		pages++;
 		if (!after_bootmem)
 			*pte = __pte(pteval & __supported_pte_mask);
 		else
 			set_pte(pte, __pte(pteval & __supported_pte_mask));
-		last_map_addr = (addr & PAGE_MASK) + PAGE_SIZE;
+		paddr_last = (paddr & PAGE_MASK) + PAGE_SIZE;
 	}
 
 	update_page_count(PG_LEVEL_4K, pages);
 
-	return last_map_addr;
+	return paddr_last;
 }
 
+/*
+ * Create PMD level page table mapping for physical addresses. The virtual
+ * and physical address have to be aligned at this level.
+ * It returns the last physical address mapped.
+ */
 static unsigned long __meminit
-phys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,
+phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,
 	      unsigned long page_size_mask, pgprot_t prot)
 {
-	unsigned long pages = 0, next;
-	unsigned long last_map_addr = end;
+	unsigned long pages = 0, paddr_next;
+	unsigned long paddr_last = paddr_end;
 
-	int i = pmd_index(address);
+	int i = pmd_index(paddr);
 
-	for (; i < PTRS_PER_PMD; i++, address = next) {
-		pmd_t *pmd = pmd_page + pmd_index(address);
+	for (; i < PTRS_PER_PMD; i++, paddr = paddr_next) {
+		pmd_t *pmd = pmd_page + pmd_index(paddr);
 		pte_t *pte;
 		pgprot_t new_prot = prot;
 
-		next = (address & PMD_MASK) + PMD_SIZE;
-		if (address >= end)
+		paddr_next = (paddr & PMD_MASK) + PMD_SIZE;
+		if (paddr >= paddr_end)
 			break;
 
-		if (__pmd_val(*pmd)) {
+		if (!pmd_none(*pmd)) {
 			if (!pmd_large(*pmd)) {
 				spin_lock(&init_mm.page_table_lock);
 				pte = (pte_t *)pmd_page_vaddr(*pmd);
-				last_map_addr = phys_pte_init(pte, address,
-								end, prot);
+				paddr_last = phys_pte_init(pte, paddr,
+							   paddr_end, prot);
 				spin_unlock(&init_mm.page_table_lock);
 				continue;
 			}
@@ -518,7 +528,7 @@ phys_pmd_init(pmd_t *pmd_page, unsigned
 			if (page_size_mask & (1 << PG_LEVEL_2M)) {
 				if (!after_bootmem)
 					pages++;
-				last_map_addr = next;
+				paddr_last = paddr_next;
 				continue;
 			}
 			new_prot = pte_pgprot(pte_clrhuge(*(pte_t *)pmd));
@@ -528,15 +538,15 @@ phys_pmd_init(pmd_t *pmd_page, unsigned
 			pages++;
 			spin_lock(&init_mm.page_table_lock);
 			set_pte((pte_t *)pmd,
-				pfn_pte((address & PMD_MASK) >> PAGE_SHIFT,
+				pfn_pte((paddr & PMD_MASK) >> PAGE_SHIFT,
 					__pgprot(pgprot_val(prot) | _PAGE_PSE)));
 			spin_unlock(&init_mm.page_table_lock);
-			last_map_addr = next;
+			paddr_last = paddr_next;
 			continue;
 		}
 
 		pte = alloc_low_page();
-		last_map_addr = phys_pte_init(pte, address, end, new_prot);
+		paddr_last = phys_pte_init(pte, paddr, paddr_end, new_prot);
 
 		make_page_readonly(pte, XENFEAT_writable_page_tables);
 		if (!after_bootmem) {
@@ -557,32 +567,43 @@ phys_pmd_init(pmd_t *pmd_page, unsigned
 		}
 	}
 	update_page_count(PG_LEVEL_2M, pages);
-	return last_map_addr;
+	return paddr_last;
 }
 
+/*
+ * Create PUD level page table mapping for physical addresses. The virtual
+ * and physical address do not have to be aligned at this level. KASLR can
+ * randomize virtual addresses up to this level.
+ * It returns the last physical address mapped.
+ */
 static unsigned long __meminit
-phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,
-			 unsigned long page_size_mask)
+phys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,
+	      unsigned long page_size_mask)
 {
-	unsigned long pages = 0, next;
-	unsigned long last_map_addr = end;
-	int i = pud_index(addr);
+	unsigned long pages = 0, paddr_next;
+	unsigned long paddr_last = paddr_end;
+	unsigned long vaddr = (unsigned long)__va(paddr);
+	int i = pud_index(vaddr);
 
-	for (; i < PTRS_PER_PUD; i++, addr = next) {
-		pud_t *pud = pud_page + pud_index(addr);
+	for (; i < PTRS_PER_PUD; i++, paddr = paddr_next) {
+		pud_t *pud;
 		pmd_t *pmd;
 		pgprot_t prot = PAGE_KERNEL;
 
-		next = (addr & PUD_MASK) + PUD_SIZE;
-		if (addr >= end)
+		vaddr = (unsigned long)__va(paddr);
+		pud = pud_page + pud_index(vaddr);
+		paddr_next = (paddr & PUD_MASK) + PUD_SIZE;
+
+		if (paddr >= paddr_end)
 			break;
 
-		if (__pud_val(*pud)) {
+		if (!pud_none(*pud)) {
 			if (!pud_large(*pud)) {
 				pmd = pmd_offset(pud, 0);
-				last_map_addr = phys_pmd_init(pmd, addr, end,
-					page_size_mask | (1 << PG_LEVEL_NUM),
-					prot);
+				paddr_last = phys_pmd_init(pmd, paddr,
+							   paddr_end,
+							   page_size_mask | (1 << PG_LEVEL_NUM),
+							   prot);
 				__flush_tlb_all();
 				continue;
 			}
@@ -601,7 +622,7 @@ phys_pud_init(pud_t *pud_page, unsigned
 			if (page_size_mask & (1 << PG_LEVEL_1G)) {
 				if (!after_bootmem)
 					pages++;
-				last_map_addr = next;
+				paddr_last = paddr_next;
 				continue;
 			}
 			prot = pte_pgprot(pte_clrhuge(*(pte_t *)pud));
@@ -611,17 +632,17 @@ phys_pud_init(pud_t *pud_page, unsigned
 			pages++;
 			spin_lock(&init_mm.page_table_lock);
 			set_pte((pte_t *)pud,
-				pfn_pte((addr & PUD_MASK) >> PAGE_SHIFT,
+				pfn_pte((paddr & PUD_MASK) >> PAGE_SHIFT,
 					PAGE_KERNEL_LARGE));
 			spin_unlock(&init_mm.page_table_lock);
-			last_map_addr = next;
+			paddr_last = paddr_next;
 			continue;
 		}
 
 		pmd = alloc_low_page();
-		last_map_addr = phys_pmd_init(pmd, addr, end,
-					      page_size_mask & ~(1 << PG_LEVEL_NUM),
-					      prot);
+		paddr_last = phys_pmd_init(pmd, paddr, paddr_end,
+					   page_size_mask & ~(1 << PG_LEVEL_NUM),
+					   prot);
 
 		make_page_readonly(pmd, XENFEAT_writable_page_tables);
 		if (!after_bootmem) {
@@ -645,7 +666,7 @@ phys_pud_init(pud_t *pud_page, unsigned
 
 	update_page_count(PG_LEVEL_1G, pages);
 
-	return last_map_addr;
+	return paddr_last;
 }
 
 RESERVE_BRK(kernel_pgt_alloc,
@@ -837,35 +858,41 @@ void __init xen_finish_init_mapping(void
 	}
 }
 
+/*
+ * Create page table mapping for the physical memory for specific physical
+ * addresses. The virtual and physical addresses have to be aligned on PMD level
+ * down. It returns the last physical address mapped.
+ */
 unsigned long __meminit
-kernel_physical_mapping_init(unsigned long start,
-			     unsigned long end,
+kernel_physical_mapping_init(unsigned long paddr_start,
+			     unsigned long paddr_end,
 			     unsigned long page_size_mask)
 {
 	bool pgd_changed = false;
-	unsigned long next, last_map_addr = end;
-	unsigned long addr;
+	unsigned long vaddr, vaddr_start, vaddr_end, vaddr_next, paddr_last;
 
-	start = (unsigned long)__va(start);
-	end = (unsigned long)__va(end);
-	addr = start;
+	paddr_last = paddr_end;
+	vaddr = (unsigned long)__va(paddr_start);
+	vaddr_end = (unsigned long)__va(paddr_end);
+	vaddr_start = vaddr;
 
-	for (; start < end; start = next) {
-		pgd_t *pgd = pgd_offset_k(start);
+	for (; vaddr < vaddr_end; vaddr = vaddr_next) {
+		pgd_t *pgd = pgd_offset_k(vaddr);
 		pud_t *pud;
 
-		next = (start & PGDIR_MASK) + PGDIR_SIZE;
+		vaddr_next = (vaddr & PGDIR_MASK) + PGDIR_SIZE;
 
 		if (__pgd_val(*pgd)) {
 			pud = (pud_t *)pgd_page_vaddr(*pgd);
-			last_map_addr = phys_pud_init(pud, __pa(start),
-				__pa(end), page_size_mask | (1 << PG_LEVEL_NUM));
+			paddr_last = phys_pud_init(pud, __pa(vaddr),
+						   __pa(vaddr_end),
+						   page_size_mask | (1 << PG_LEVEL_NUM));
 			continue;
 		}
 
 		pud = alloc_low_page();
-		last_map_addr = phys_pud_init(pud, __pa(start), __pa(end),
-						 page_size_mask);
+		paddr_last = phys_pud_init(pud, __pa(vaddr), __pa(vaddr_end),
+					   page_size_mask);
 
 		make_page_readonly(pud, XENFEAT_writable_page_tables);
 		if (!after_bootmem)
@@ -879,9 +906,9 @@ kernel_physical_mapping_init(unsigned lo
 	}
 
 	if (pgd_changed)
-		sync_global_pgds(addr, end - 1, 0);
+		sync_global_pgds(vaddr_start, vaddr_end - 1, 0);
 
-	return last_map_addr;
+	return paddr_last;
 }
 
 #ifndef CONFIG_NUMA
@@ -990,7 +1017,7 @@ static void __meminit free_pte_table(pte
 
 	for (i = 0; i < PTRS_PER_PTE; i++) {
 		pte = pte_start + i;
-		if (pte_val(*pte))
+		if (!pte_none(*pte))
 			return;
 	}
 
@@ -1008,7 +1035,7 @@ static void __meminit free_pmd_table(pmd
 
 	for (i = 0; i < PTRS_PER_PMD; i++) {
 		pmd = pmd_start + i;
-		if (pmd_val(*pmd))
+		if (!pmd_none(*pmd))
 			return;
 	}
 
@@ -1019,27 +1046,6 @@ static void __meminit free_pmd_table(pmd
 	spin_unlock(&init_mm.page_table_lock);
 }
 
-/* Return true if pgd is changed, otherwise return false. */
-static bool __meminit free_pud_table(pud_t *pud_start, pgd_t *pgd)
-{
-	pud_t *pud;
-	int i;
-
-	for (i = 0; i < PTRS_PER_PUD; i++) {
-		pud = pud_start + i;
-		if (pud_val(*pud))
-			return false;
-	}
-
-	/* free a pud table */
-	free_pagetable(pgd_page(*pgd), 0);
-	spin_lock(&init_mm.page_table_lock);
-	pgd_clear(pgd);
-	spin_unlock(&init_mm.page_table_lock);
-
-	return true;
-}
-
 static void __meminit
 remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,
 		 bool direct)
@@ -1230,7 +1236,6 @@ remove_pagetable(unsigned long start, un
 	unsigned long addr;
 	pgd_t *pgd;
 	pud_t *pud;
-	bool pgd_changed = false;
 
 	for (addr = start; addr < end; addr = next) {
 		next = pgd_addr_end(addr, end);
@@ -1241,13 +1246,8 @@ remove_pagetable(unsigned long start, un
 
 		pud = (pud_t *)pgd_page_vaddr(*pgd);
 		remove_pud_table(pud, addr, next, direct);
-		if (free_pud_table(pud, pgd))
-			pgd_changed = true;
 	}
 
-	if (pgd_changed)
-		sync_global_pgds(start, end - 1, 1);
-
 	flush_tlb_all();
 }
 
--- a/arch/x86/mm/iomap_32-xen.c
+++ b/arch/x86/mm/iomap_32-xen.c
@@ -19,7 +19,7 @@
 #include <asm/iomap.h>
 #include <asm/pat.h>
 #include <linux/bitops.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/highmem.h>
 
 static int is_io_mapping_possible(resource_size_t base, unsigned long size)
--- a/arch/x86/mm/ioremap-xen.c
+++ b/arch/x86/mm/ioremap-xen.c
@@ -9,7 +9,6 @@
 #include <linux/bootmem.h>
 #include <linux/init.h>
 #include <linux/io.h>
-#include <linux/module.h>
 #include <linux/pfn.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
--- a/arch/x86/mm/pageattr-xen.c
+++ b/arch/x86/mm/pageattr-xen.c
@@ -101,7 +101,8 @@ static inline unsigned long highmap_star
 
 static inline unsigned long highmap_end_pfn(void)
 {
-	return PFN_UP(__pa_symbol(_brk_end));
+	/* Do not reference physical address outside the kernel. */
+	return PFN_UP(__pa_symbol(_brk_end - 1));
 }
 
 #endif
@@ -112,6 +113,12 @@ within(unsigned long addr, unsigned long
 	return addr >= start && addr < end;
 }
 
+static inline int
+within_inclusive(unsigned long addr, unsigned long start, unsigned long end)
+{
+	return addr >= start && addr <= end;
+}
+
 /*
  * Flushing functions
  */
@@ -764,18 +771,6 @@ static bool try_to_free_pmd_page(pmd_t *
 	return true;
 }
 
-static bool try_to_free_pud_page(pud_t *pud)
-{
-	int i;
-
-	for (i = 0; i < PTRS_PER_PUD; i++)
-		if (!pud_none(pud[i]))
-			return false;
-
-	free_page((unsigned long)pud);
-	return true;
-}
-
 static bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)
 {
 	pte_t *pte = pte_offset_kernel(pmd, start);
@@ -889,16 +884,6 @@ static void unmap_pud_range(pgd_t *pgd,
 	 */
 }
 
-static void unmap_pgd_range(pgd_t *root, unsigned long addr, unsigned long end)
-{
-	pgd_t *pgd_entry = root + pgd_index(addr);
-
-	unmap_pud_range(pgd_entry, addr, end);
-
-	if (try_to_free_pud_page((pud_t *)pgd_page_vaddr(*pgd_entry)))
-		pgd_clear(pgd_entry);
-}
-
 static int alloc_pte_page(pmd_t *pmd)
 {
 	pte_t *pte = (pte_t *)get_zeroed_page(GFP_KERNEL | __GFP_NOTRACK);
@@ -950,11 +935,11 @@ static void populate_pte(struct cpa_data
 	}
 }
 
-static int populate_pmd(struct cpa_data *cpa,
-			unsigned long start, unsigned long end,
-			unsigned num_pages, pud_t *pud, pgprot_t pgprot)
+static long populate_pmd(struct cpa_data *cpa,
+			 unsigned long start, unsigned long end,
+			 unsigned num_pages, pud_t *pud, pgprot_t pgprot)
 {
-	unsigned int cur_pages = 0;
+	long cur_pages = 0;
 	pmd_t *pmd;
 	pgprot_t pmd_pgprot;
 
@@ -1024,12 +1009,12 @@ static int populate_pmd(struct cpa_data
 	return num_pages;
 }
 
-static int populate_pud(struct cpa_data *cpa, unsigned long start, pgd_t *pgd,
-			pgprot_t pgprot)
+static long populate_pud(struct cpa_data *cpa, unsigned long start, pgd_t *pgd,
+			 pgprot_t pgprot)
 {
 	pud_t *pud;
 	unsigned long end;
-	int cur_pages = 0;
+	long cur_pages = 0;
 	pgprot_t pud_pgprot;
 
 	end = start + (cpa->numpages << PAGE_SHIFT);
@@ -1085,7 +1070,7 @@ static int populate_pud(struct cpa_data
 
 	/* Map trailing leftover */
 	if (start < end) {
-		int tmp;
+		long tmp;
 
 		pud = pud_offset(pgd, start);
 		if (pud_none(*pud))
@@ -1111,7 +1096,7 @@ static int populate_pgd(struct cpa_data
 	pgprot_t pgprot = __pgprot(_KERNPG_TABLE);
 	pud_t *pud = NULL;	/* shut up gcc */
 	pgd_t *pgd_entry;
-	int ret;
+	long ret;
 
 	pgd_entry = cpa->pgd + pgd_index(addr);
 
@@ -1131,7 +1116,12 @@ static int populate_pgd(struct cpa_data
 
 	ret = populate_pud(cpa, addr, pgd_entry, pgprot);
 	if (ret < 0) {
-		unmap_pgd_range(cpa->pgd, addr,
+		/*
+		 * Leave the PUD page in place in case some other CPU or thread
+		 * already found it, but remove any useless entries we just
+		 * added to it.
+		 */
+		unmap_pud_range(pgd_entry, addr,
 				addr + (cpa->numpages << PAGE_SHIFT));
 		return ret;
 	}
@@ -1208,7 +1198,7 @@ repeat:
 		return __cpa_process_fault(cpa, address, primary);
 
 	old_pte = *kpte;
-	if (!__pte_val(old_pte))
+	if (pte_none(old_pte))
 		return __cpa_process_fault(cpa, address, primary);
 
 	if (level == PG_LEVEL_4K) {
@@ -1362,7 +1352,8 @@ static int cpa_process_alias(struct cpa_
 	 * to touch the high mapped kernel as well:
 	 */
 	if (!within(vaddr, (unsigned long)_text, _brk_end) &&
-	    within(cpa->pfn, highmap_start_pfn(), highmap_end_pfn())) {
+	    within_inclusive(cpa->pfn, highmap_start_pfn(),
+			     highmap_end_pfn())) {
 		unsigned long temp_cpa_vaddr = (cpa->pfn << PAGE_SHIFT) +
 					       __START_KERNEL_map;
 		alias_cpa = *cpa;
@@ -1382,7 +1373,8 @@ static int cpa_process_alias(struct cpa_
 
 static int __change_page_attr_set_clr(struct cpa_data *cpa, int checkalias)
 {
-	int ret, numpages = cpa->numpages;
+	unsigned long numpages = cpa->numpages;
+	int ret;
 
 	while (numpages) {
 		/*
@@ -2082,12 +2074,6 @@ int kernel_map_pages_in_pgd(pgd_t *pgd,
 out:
 	return retval;
 }
-
-void kernel_unmap_pages_in_pgd(pgd_t *root, unsigned long address,
-			       unsigned numpages)
-{
-	unmap_pgd_range(root, address, address + (numpages << PAGE_SHIFT));
-}
 #endif
 
 static inline int in_secondary_range(unsigned long va)
--- a/arch/x86/mm/pat-xen.c
+++ b/arch/x86/mm/pat-xen.c
@@ -11,7 +11,6 @@
 #include <linux/bootmem.h>
 #include <linux/debugfs.h>
 #include <linux/kernel.h>
-#include <linux/module.h>
 #include <linux/pfn_t.h>
 #include <linux/slab.h>
 #include <linux/mm.h>
@@ -771,11 +770,8 @@ static inline int range_is_allowed(unsig
 		return 1;
 
 	while (cursor < to) {
-		if (!devmem_is_allowed(mfn)) {
-			pr_info("x86/PAT: Program %s tried to access /dev/mem between [mem %#010Lx-%#010Lx], PAT prevents it\n",
-				current->comm, from, to - 1);
+		if (!devmem_is_allowed(mfn))
 			return 0;
-		}
 		cursor += PAGE_SIZE;
 		mfn++;
 	}
@@ -925,9 +921,10 @@ int track_pfn_copy(struct vm_area_struct
 }
 
 /*
- * prot is passed in as a parameter for the new mapping. If the vma has a
- * linear pfn mapping for the entire range reserve the entire vma range with
- * single reserve_pfn_range call.
+ * prot is passed in as a parameter for the new mapping. If the vma has
+ * a linear pfn mapping for the entire range, or no vma is provided,
+ * reserve the entire pfn + size range with single reserve_pfn_range
+ * call.
  */
 int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,
 		    unsigned long pfn, unsigned long addr, unsigned long size)
@@ -936,11 +933,12 @@ int track_pfn_remap(struct vm_area_struc
 	enum page_cache_mode pcm;
 
 	/* reserve the whole chunk starting from paddr */
-	if (addr == vma->vm_start && size == (vma->vm_end - vma->vm_start)) {
+	if (!vma || (addr == vma->vm_start
+				&& size == (vma->vm_end - vma->vm_start))) {
 		int ret;
 
 		ret = reserve_pfn_range(paddr, size, prot, 0);
-		if (!ret)
+		if (ret == 0 && vma)
 			vma->vm_flags |= VM_PAT;
 		return ret;
 	}
@@ -995,7 +993,7 @@ void untrack_pfn(struct vm_area_struct *
 	resource_size_t paddr;
 	unsigned long prot;
 
-	if (!(vma->vm_flags & VM_PAT))
+	if (vma && !(vma->vm_flags & VM_PAT))
 		return;
 
 	/* free the chunk starting from pfn or the whole chunk */
@@ -1009,7 +1007,8 @@ void untrack_pfn(struct vm_area_struct *
 		size = vma->vm_end - vma->vm_start;
 	}
 	free_pfn_range(paddr, size);
-	vma->vm_flags &= ~VM_PAT;
+	if (vma)
+		vma->vm_flags &= ~VM_PAT;
 }
 
 /*
--- a/arch/x86/mm/pgtable-xen.c
+++ b/arch/x86/mm/pgtable-xen.c
@@ -1,6 +1,6 @@
 #include <linux/mm.h>
 #include <linux/gfp.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <xen/features.h>
 #include <asm/pgalloc.h>
 #include <asm/pgtable.h>
@@ -9,7 +9,7 @@
 #include <asm/hypervisor.h>
 #include <asm/mmu_context.h>
 
-#define PGALLOC_GFP GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO
+#define PGALLOC_GFP (GFP_KERNEL_ACCOUNT | __GFP_NOTRACK | __GFP_ZERO)
 
 #ifdef CONFIG_HIGHPTE
 #define PGALLOC_USER_GFP __GFP_HIGHMEM
@@ -21,7 +21,7 @@ gfp_t __userpte_alloc_gfp = PGALLOC_GFP
 
 pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
 {
-	pte_t *pte = (pte_t *)__get_free_page(PGALLOC_GFP);
+	pte_t *pte = (pte_t *)__get_free_page(PGALLOC_GFP & ~__GFP_ACCOUNT);
 	if (pte)
 		make_lowmem_page_readonly(pte, XENFEAT_writable_page_tables);
 	return pte;
@@ -109,8 +109,12 @@ static void _pmd_free(struct page *page,
 pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long address)
 {
 	struct page *pmd;
+	gfp_t gfp = PGALLOC_GFP;
 
-	pmd = alloc_pages(PGALLOC_GFP, 0);
+	if (mm == &init_mm)
+		gfp &= ~__GFP_ACCOUNT;
+
+	pmd = alloc_pages(gfp, 0);
 	if (!pmd)
 		return NULL;
 	if (!pgtable_pmd_page_ctor(pmd)) {
--- a/arch/x86/mm/pgtable_32-xen.c
+++ b/arch/x86/mm/pgtable_32-xen.c
@@ -8,7 +8,6 @@
 #include <linux/highmem.h>
 #include <linux/pagemap.h>
 #include <linux/spinlock.h>
-#include <linux/module.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -52,7 +51,7 @@ void set_pte_vaddr(unsigned long vaddr,
 		return;
 	}
 	pte = pte_offset_kernel(pmd, vaddr);
-	if (pte_val(pteval))
+	if (!pte_none(pteval))
 		set_pte_at(&init_mm, vaddr, pte, pteval);
 	else
 		pte_clear(&init_mm, vaddr, pte);
--- a/drivers/acpi/processor_idle.c
+++ b/drivers/acpi/processor_idle.c
@@ -60,10 +60,12 @@ module_param(latency_factor, uint, 0644)
 
 static DEFINE_PER_CPU(struct cpuidle_device *, acpi_cpuidle_device);
 
+#ifndef CONFIG_PROCESSOR_EXTERNAL_CONTROL
 struct cpuidle_driver acpi_idle_driver = {
 	.name =		"acpi_idle",
 	.owner =	THIS_MODULE,
 };
+#endif
 
 #ifdef CONFIG_ACPI_PROCESSOR_CSTATE
 static int disabled_by_idle_boot_param(void)
@@ -901,8 +903,11 @@ static int acpi_processor_setup_cstates(
 	return 0;
 }
 #else
-static void acpi_processor_setup_cpuidle_cx(struct acpi_processor *pr,
-					    struct cpuidle_device *dev) {}
+static int acpi_processor_setup_cpuidle_cx(struct acpi_processor *pr,
+					   struct cpuidle_device *dev)
+{
+	return 0;
+}
 #endif /* CONFIG_PROCESSOR_EXTERNAL_CONTROL */
 
 static inline void acpi_processor_cstate_first_run_checks(void)
@@ -1232,6 +1237,7 @@ int __weak acpi_processor_ffh_lpi_probe(
 	return -ENODEV;
 }
 
+#ifndef CONFIG_PROCESSOR_EXTERNAL_CONTROL
 int __weak acpi_processor_ffh_lpi_enter(struct acpi_lpi_state *lpi)
 {
 	return -ENODEV;
@@ -1317,6 +1323,7 @@ static int acpi_processor_setup_cpuidle_
 
 	return acpi_processor_setup_cstates(pr);
 }
+#endif /* CONFIG_PROCESSOR_EXTERNAL_CONTROL */
 
 /**
  * acpi_processor_setup_cpuidle_dev - prepares and configures CPUIDLE
--- a/drivers/char/tpm/tpm.h
+++ b/drivers/char/tpm/tpm.h
@@ -493,12 +493,12 @@ enum tpm_transmit_flags {
 
 static inline void *chip_get_private(const struct tpm_chip *chip)
 {
-	return chip->vendor.priv;
+	return dev_get_drvdata(&chip->dev);
 }
 
 static inline void chip_set_private(struct tpm_chip *chip, void *priv)
 {
-	chip->vendor.priv = priv;
+	dev_set_drvdata(&chip->dev, priv);
 }
 
 ssize_t tpm_transmit(struct tpm_chip *chip, const u8 *buf, size_t bufsiz,
--- a/drivers/char/tpm/tpm_vtpm.c
+++ b/drivers/char/tpm/tpm_vtpm.c
@@ -484,8 +484,8 @@ struct tpm_chip *init_vtpm(struct device
 	}
 
 	chip_set_private(chip, vtpms);
-	for (rc = 0; rc < ARRAY_SIZE(chip->vendor.duration); ++rc)
-		chip->vendor.duration[rc] = TPM_LONG_TIMEOUT;
+	for (rc = 0; rc < ARRAY_SIZE(chip->duration); ++rc)
+		chip->duration[rc] = TPM_LONG_TIMEOUT;
 
 	return chip;
 
--- a/drivers/gpu/drm/i915/intel_fbc.c
+++ b/drivers/gpu/drm/i915/intel_fbc.c
@@ -1313,7 +1313,7 @@ static int intel_sanitize_fbc_option(str
 
 static bool need_fbc_vtd_wa(struct drm_i915_private *dev_priv)
 {
-#ifdef CONFIG_INTEL_IOMMU
+#if defined(CONFIG_INTEL_IOMMU) || defined(CONFIG_XEN)
 	/* WaFbcTurnOffFbcWhenHyperVisorIsUsed:skl,bxt */
 	if (intel_iommu_gfx_mapped &&
 	    (IS_SKYLAKE(dev_priv) || IS_BROXTON(dev_priv))) {
--- a/drivers/pci/Kconfig
+++ b/drivers/pci/Kconfig
@@ -26,7 +26,7 @@ config PCI_MSI
 
 config PCI_MSI_IRQ_DOMAIN
 	def_bool ARC || ARM || ARM64 || X86
-	depends on PCI_MSI
+	depends on PCI_MSI && !XEN
 	select GENERIC_MSI_IRQ_DOMAIN
 
 config PCI_DEBUG
--- a/drivers/pci/msi-xen.c
+++ b/drivers/pci/msi-xen.c
@@ -4,6 +4,7 @@
  *
  * Copyright (C) 2003-2004 Intel
  * Copyright (C) Tom Long Nguyen (tom.l.nguyen@intel.com)
+ * Copyright (C) 2016 Christoph Hellwig.
  */
 
 #include <linux/err.h>
@@ -591,34 +592,41 @@ static int msix_capability_init(struct p
 	for (i = 0; i < nvec; i++) {
 		mapped = 0;
 		list_for_each_entry(pirq_entry, &dev->msi_list, list) {
-			if (pirq_entry->entry_nr == entries[i].entry) {
+			if (pirq_entry->entry_nr == (entries ? entries[i].entry
+							     : i)) {
 				dev_warn(&dev->dev,
 					 "msix entry %d was not freed\n",
-					 entries[i].entry);
-				(entries + i)->vector = pirq_entry->pirq;
+					 pirq_entry->entry_nr);
+				if (entries)
+					entries[i].vector = pirq_entry->pirq;
 				mapped = 1;
 				break;
 			}
 		}
 		if (mapped)
 			continue;
-		pirq = msi_map_vector(dev, entries[i].entry, table_base,
-				      msi_dev_entry->owner);
+		pirq = msi_map_vector(dev, entries ? entries[i].entry : i,
+				      table_base, msi_dev_entry->owner);
 		if (pirq < 0)
 			break;
-		attach_pirq_entry(pirq, entries[i].entry, msi_dev_entry);
-		(entries + i)->vector = pirq;
+		attach_pirq_entry(pirq, entries ? entries[i].entry : i,
+				  msi_dev_entry);
+		if (entries)
+			entries[i].vector = pirq;
 	}
 
 	if (i != nvec) {
 		for (j = i - 1; j >= 0; j--) {
 			list_for_each_entry(pirq_entry, &dev->msi_list, list)
-				if (pirq_entry->entry_nr == entries[j].entry)
+				if (pirq_entry->entry_nr ==
+				    (entries ? entries[j].entry : j))
 					break;
-			msi_unmap_pirq(dev, entries[j].vector, 1,
+			msi_unmap_pirq(dev, pirq_entry->pirq, 1,
 				       msi_dev_entry->owner);
-			detach_pirq_entry(entries[j].entry, msi_dev_entry);
-			entries[j].vector = 0;
+			detach_pirq_entry(entries ? entries[j].entry : j,
+					  msi_dev_entry);
+			if (entries)
+				entries[j].vector = 0;
 		}
 		/*
 		 * If we had some success, report the number of irqs
@@ -769,7 +777,7 @@ EXPORT_SYMBOL(pci_msix_vec_count);
 /**
  * pci_enable_msix - configure device's MSI-X capability structure
  * @dev: pointer to the pci_dev data structure of MSI-X device function
- * @entries: pointer to an array of MSI-X entries
+ * @entries: pointer to an array of MSI-X entries (optional)
  * @nvec: number of MSI-X irqs requested for allocation by device driver
  *
  * Setup the MSI-X capability structure of device function with the number
@@ -790,17 +798,25 @@ int pci_enable_msix(struct pci_dev *dev,
 	if (!pci_msi_supported(dev, nvec))
 		return -EINVAL;
 
-	if (!entries)
-		return -EINVAL;
-
 	if (!is_initial_xendomain()) {
 #ifdef CONFIG_XEN_PCIDEV_FRONTEND
 		struct msi_pirq_entry *pirq_entry;
 		int ret, irq;
 
+		status = !entries;
+		if (status) {
+			entries = kcalloc(sizeof(*entries), nvec, GFP_KERNEL);
+			if (!entries)
+				return -ENOMEM;
+			for (i = 0; i < nvec; ++i)
+				entries[i].entry = i;
+		}
+
 		temp = dev->irq;
 		ret = pci_frontend_enable_msix(dev, entries, nvec);
 		if (ret) {
+			if (status)
+				kfree(entries);
 			dev_warn(&dev->dev,
 				 "got %x from frontend_enable_msix\n", ret);
 			return ret;
@@ -826,6 +842,8 @@ int pci_enable_msix(struct pci_dev *dev,
 			attach_pirq_entry(irq, entries[i].entry, msi_dev_entry);
 			entries[i].vector = irq;
 		}
+		if (status)
+			kfree(entries);
 		populate_msi_sysfs(dev);
 		return 0;
 #else
@@ -839,13 +857,15 @@ int pci_enable_msix(struct pci_dev *dev,
 	if (nvec > nr_entries)
 		return nr_entries;
 
-	/* Check for any invalid entries */
-	for (i = 0; i < nvec; i++) {
-		if (entries[i].entry >= nr_entries)
-			return -EINVAL;		/* invalid entry */
-		for (j = i + 1; j < nvec; j++) {
-			if (entries[i].entry == entries[j].entry)
-				return -EINVAL;	/* duplicate entry */
+	if (entries) {
+		/* Check for any invalid entries */
+		for (i = 0; i < nvec; i++) {
+			if (entries[i].entry >= nr_entries)
+				return -EINVAL;		/* invalid entry */
+			for (j = i + 1; j < nvec; j++) {
+				if (entries[i].entry == entries[j].entry)
+					return -EINVAL;	/* duplicate entry */
+			}
 		}
 	}
 
@@ -936,19 +956,8 @@ int pci_msi_enabled(void)
 }
 EXPORT_SYMBOL(pci_msi_enabled);
 
-/**
- * pci_enable_msi_range - configure device's MSI capability structure
- * @dev: device to configure
- * @minvec: minimal number of interrupts to configure
- * @maxvec: maximum number of interrupts to configure
- *
- * This function tries to allocate a maximum possible number of interrupts in a
- * range between @minvec and @maxvec. It returns a negative errno if an error
- * occurs. If it succeeds, it returns the actual number of interrupts allocated
- * and updates the @dev's irq member to the lowest new interrupt number;
- * the other interrupt numbers allocated to this device are consecutive.
- **/
-int pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec)
+static int __pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec,
+		unsigned int flags)
 {
 	int nvec, temp;
 	int rc;
@@ -972,30 +981,42 @@ int pci_enable_msi_range(struct pci_dev
 	nvec = pci_msi_vec_count(dev);
 	if (nvec < 0)
 		return nvec;
-	else if (nvec < minvec)
+	if (nvec < minvec)
 		return -EINVAL;
-	else if (nvec > maxvec)
+
+	if (nvec > maxvec)
 		nvec = maxvec;
 
 	temp = dev->irq;
 
-	do {
+	for (;;) {
+		if (flags & PCI_IRQ_AFFINITY) {
+			dev->irq_affinity = irq_create_affinity_mask(&nvec);
+			if (nvec < minvec)
+				return -ENOSPC;
+		}
+
 		if (is_initial_xendomain())
 			rc = msi_capability_init(dev, nvec);
 		else
 #ifdef CONFIG_XEN_PCIDEV_FRONTEND
 			rc = pci_frontend_enable_msi(dev, nvec);
 #else
-			return -EOPNOTSUPP;
+			rc = -EOPNOTSUPP;
 #endif
-		if (rc < 0) {
+		if (rc == 0)
+			break;
+
+		kfree(dev->irq_affinity);
+		dev->irq_affinity = NULL;
+
+		if (rc < 0)
 			return rc;
-		} else if (rc > 0) {
-			if (rc < minvec)
-				return -ENOSPC;
-			nvec = rc;
-		}
-	} while (rc);
+		if (rc < minvec)
+			return -ENOSPC;
+
+		nvec = rc;
+	}
 
 	msi_dev_entry->default_irq = temp;
 
@@ -1010,8 +1031,58 @@ int pci_enable_msi_range(struct pci_dev
 
 	return nvec;
 }
+
+/**
+ * pci_enable_msi_range - configure device's MSI capability structure
+ * @dev: device to configure
+ * @minvec: minimal number of interrupts to configure
+ * @maxvec: maximum number of interrupts to configure
+ *
+ * This function tries to allocate a maximum possible number of interrupts in a
+ * range between @minvec and @maxvec. It returns a negative errno if an error
+ * occurs. If it succeeds, it returns the actual number of interrupts allocated
+ * and updates the @dev's irq member to the lowest new interrupt number;
+ * the other interrupt numbers allocated to this device are consecutive.
+ **/
+int pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec)
+{
+	return __pci_enable_msi_range(dev, minvec, maxvec, 0);
+}
 EXPORT_SYMBOL(pci_enable_msi_range);
 
+static int __pci_enable_msix_range(struct pci_dev *dev,
+		struct msix_entry *entries, int minvec, int maxvec,
+		unsigned int flags)
+{
+	int nvec = maxvec;
+	int rc;
+
+	if (maxvec < minvec)
+		return -ERANGE;
+
+	for (;;) {
+		if (flags & PCI_IRQ_AFFINITY) {
+			dev->irq_affinity = irq_create_affinity_mask(&nvec);
+			if (nvec < minvec)
+				return -ENOSPC;
+		}
+
+		rc = pci_enable_msix(dev, entries, nvec);
+		if (rc == 0)
+			return nvec;
+
+		kfree(dev->irq_affinity);
+		dev->irq_affinity = NULL;
+
+		if (rc < 0)
+			return rc;
+		if (rc < minvec)
+			return -ENOSPC;
+
+		nvec = rc;
+	}
+}
+
 /**
  * pci_enable_msix_range - configure device's MSI-X capability structure
  * @dev: pointer to the pci_dev data structure of MSI-X device function
@@ -1028,25 +1099,98 @@ EXPORT_SYMBOL(pci_enable_msi_range);
  * with new allocated MSI-X interrupts.
  **/
 int pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries,
-			       int minvec, int maxvec)
+		int minvec, int maxvec)
 {
-	int nvec = maxvec;
-	int rc;
+	return __pci_enable_msix_range(dev, entries, minvec, maxvec, 0);
+}
+EXPORT_SYMBOL(pci_enable_msix_range);
 
-	if (maxvec < minvec)
-		return -ERANGE;
+/**
+ * pci_alloc_irq_vectors - allocate multiple IRQs for a device
+ * @dev:		PCI device to operate on
+ * @min_vecs:		minimum number of vectors required (must be >= 1)
+ * @max_vecs:		maximum (desired) number of vectors
+ * @flags:		flags or quirks for the allocation
+ *
+ * Allocate up to @max_vecs interrupt vectors for @dev, using MSI-X or MSI
+ * vectors if available, and fall back to a single legacy vector
+ * if neither is available.  Return the number of vectors allocated,
+ * (which might be smaller than @max_vecs) if successful, or a negative
+ * error code on error. If less than @min_vecs interrupt vectors are
+ * available for @dev the function will fail with -ENOSPC.
+ *
+ * To get the Linux IRQ number used for a vector that can be passed to
+ * request_irq() use the pci_irq_vector() helper.
+ */
+int pci_alloc_irq_vectors(struct pci_dev *dev, unsigned int min_vecs,
+		unsigned int max_vecs, unsigned int flags)
+{
+	int vecs = -ENOSPC;
 
-	do {
-		rc = pci_enable_msix(dev, entries, nvec);
-		if (rc < 0) {
-			return rc;
-		} else if (rc > 0) {
-			if (rc < minvec)
-				return -ENOSPC;
-			nvec = rc;
-		}
-	} while (rc);
+	if (flags & PCI_IRQ_MSIX) {
+		vecs = __pci_enable_msix_range(dev, NULL, min_vecs, max_vecs,
+				flags);
+		if (vecs > 0)
+			return vecs;
+	}
 
-	return nvec;
+	if (flags & PCI_IRQ_MSI) {
+		vecs = __pci_enable_msi_range(dev, min_vecs, max_vecs, flags);
+		if (vecs > 0)
+			return vecs;
+	}
+
+	/* use legacy irq if allowed */
+	if ((flags & PCI_IRQ_LEGACY) && min_vecs == 1) {
+		pci_intx(dev, 1);
+		return 1;
+	}
+
+	return vecs;
 }
-EXPORT_SYMBOL(pci_enable_msix_range);
+EXPORT_SYMBOL(pci_alloc_irq_vectors);
+
+/**
+ * pci_free_irq_vectors - free previously allocated IRQs for a device
+ * @dev:		PCI device to operate on
+ *
+ * Undoes the allocations and enabling in pci_alloc_irq_vectors().
+ */
+void pci_free_irq_vectors(struct pci_dev *dev)
+{
+	pci_disable_msix(dev);
+	pci_disable_msi(dev);
+}
+EXPORT_SYMBOL(pci_free_irq_vectors);
+
+/**
+ * pci_irq_vector - return Linux IRQ number of a device vector
+ * @dev: PCI device to operate on
+ * @nr: device-relative interrupt vector index (0-based).
+ */
+int pci_irq_vector(struct pci_dev *dev, unsigned int nr)
+{
+	if (dev->msix_enabled) {
+		const struct msi_pirq_entry *entry;
+
+		list_for_each_entry(entry, &dev->msi_list, list)
+			if (entry->entry_nr == nr)
+				return entry->pirq;
+		WARN_ON_ONCE(1);
+		return -EINVAL;
+	}
+
+	if (dev->msi_enabled) {
+		const struct msi_dev_list *entry = get_msi_dev_pirq_list(dev);
+
+		if (WARN_ON_ONCE(entry->e.entry_nr >= 0 ||
+				 nr >= -entry->e.entry_nr))
+			return -EINVAL;
+	} else {
+		if (WARN_ON_ONCE(nr > 0))
+			return -EINVAL;
+	}
+
+	return dev->irq + nr;
+}
+EXPORT_SYMBOL(pci_irq_vector);
--- a/drivers/xen/Makefile
+++ b/drivers/xen/Makefile
@@ -31,6 +31,7 @@ priv-$(CONFIG_PCI)			+= pci.o
 
 obj-$(CONFIG_XEN)			+= features.o $(xen-backend-y) $(xen-backend-m)
 obj-$(CONFIG_XEN_PRIVILEGED_GUEST)	+= $(priv-y)
+dom0-$(CONFIG_ARM64) += arm-device.o
 dom0-$(CONFIG_PCI) += pci.o
 dom0-$(CONFIG_USB_SUPPORT) += dbgp.o
 dom0-$(CONFIG_XEN_ACPI) += acpi.o $(xen-pad-y)
--- a/drivers/xen/blkback/blkback.c
+++ b/drivers/xen/blkback/blkback.c
@@ -341,7 +341,7 @@ static void dispatch_discard(blkif_t *bl
 	preq.sector_number = req->sector_number;
 	preq.nr_sects      = req->nr_sectors;
 
-	if (vbd_translate(&preq, blkif, REQ_DISCARD) != 0) {
+	if (vbd_translate(&preq, blkif, REQ_OP_DISCARD) != 0) {
 		DPRINTK("access denied: discard of [%Lu,%Lu) on dev=%04x\n",
 			preq.sector_number,
 			preq.sector_number + preq.nr_sects,
@@ -483,25 +483,27 @@ static void dispatch_rw_block_io(blkif_t
 	unsigned int nseg, i, nbio = 0;
 	struct bio *bio = NULL;
 	uint32_t flags;
-	int ret, operation;
+	int ret, operation, operation_flags = 0;
 	struct blk_plug plug;
 
 	switch (req->operation) {
 	case BLKIF_OP_READ:
 		blkif->st_rd_req++;
-		operation = READ;
+		operation = REQ_OP_READ;
 		break;
 	case BLKIF_OP_WRITE:
 		blkif->st_wr_req++;
-		operation = WRITE;
+		operation = REQ_OP_WRITE;
 		break;
 	case BLKIF_OP_WRITE_BARRIER:
 		blkif->st_br_req++;
-		operation = WRITE_FLUSH_FUA;
+		operation = REQ_OP_WRITE;
+		operation_flags = WRITE_FLUSH_FUA;
 		break;
 	case BLKIF_OP_FLUSH_DISKCACHE:
 		blkif->st_fl_req++;
-		operation = WRITE_FLUSH;
+		operation = REQ_OP_WRITE;
+		operation_flags = WRITE_FLUSH;
 		break;
 	default:
 		operation = 0; /* make gcc happy */
@@ -510,7 +512,7 @@ static void dispatch_rw_block_io(blkif_t
 
 	/* Check that number of segments is sane. */
 	nseg = req->nr_segments;
-	if (unlikely(nseg == 0 && !(operation & REQ_FLUSH)) ||
+	if (unlikely(nseg == 0 && !(operation_flags & REQ_PREFLUSH)) ||
 	    unlikely(nseg > BLKIF_MAX_SEGMENTS_PER_REQUEST)) {
 		DPRINTK("Bad number of segments in request (%d)\n", nseg);
 		goto fail_response;
@@ -525,7 +527,7 @@ static void dispatch_rw_block_io(blkif_t
 	pending_req->nr_pages  = nseg;
 
 	flags = GNTMAP_host_map;
-	if (operation != READ)
+	if (operation != REQ_OP_READ)
 		flags |= GNTMAP_readonly;
 
 	for (i = 0; i < nseg; i++) {
@@ -573,7 +575,7 @@ static void dispatch_rw_block_io(blkif_t
 
 	if (vbd_translate(&preq, blkif, operation) != 0) {
 		DPRINTK("access denied: %s of [%llu,%llu] on dev=%04x\n", 
-			operation == READ ? "read" : "write",
+			operation == REQ_OP_READ ? "read" : "write",
 			preq.sector_number,
 			preq.sector_number + preq.nr_sects,
 			blkif->vbd.pdevice);
@@ -608,6 +610,7 @@ static void dispatch_rw_block_io(blkif_t
 			bio->bi_bdev    = preq.bdev;
 			bio->bi_private = pending_req;
 			bio->bi_end_io  = end_block_io_op;
+			bio_set_op_attrs(bio, operation, operation_flags);
 			bio->bi_iter.bi_sector = preq.sector_number;
 		}
 
@@ -619,7 +622,7 @@ static void dispatch_rw_block_io(blkif_t
 	}
 
 	if (!bio) {
-		BUG_ON(!(operation & (REQ_FLUSH|REQ_FUA)));
+		BUG_ON(!(operation_flags & (REQ_PREFLUSH|REQ_FUA)));
 		bio = bio_alloc(GFP_KERNEL, 0);
 		if (unlikely(bio == NULL))
 			goto fail_put_bio;
@@ -629,6 +632,7 @@ static void dispatch_rw_block_io(blkif_t
 		bio->bi_bdev    = preq.bdev;
 		bio->bi_private = pending_req;
 		bio->bi_end_io  = end_block_io_op;
+		bio_set_op_attrs(bio, operation, operation_flags);
 		bio->bi_iter.bi_sector = -1;
 	}
 
@@ -636,11 +640,11 @@ static void dispatch_rw_block_io(blkif_t
 	blk_start_plug(&plug);
 
 	for (i = 0; i < nbio; ++i)
-		submit_bio(operation, seg[i].bio);
+		submit_bio(seg[i].bio);
 
 	blk_finish_plug(&plug);
 
-	if (operation == READ)
+	if (operation == REQ_OP_READ)
 		blkif->st_rd_sect += preq.nr_sects;
 	else
 		blkif->st_wr_sect += preq.nr_sects;
--- a/drivers/xen/blkback/vbd.c
+++ b/drivers/xen/blkback/vbd.c
@@ -92,7 +92,7 @@ int vbd_create(blkif_t *blkif, blkif_vde
 	if (q && test_bit(QUEUE_FLAG_WC, &q->queue_flags))
 		vbd->flush_support = true;
 
-	if (q && blk_queue_secdiscard(q))
+	if (q && blk_queue_secure_erase(q))
 		vbd->discard_secure = true;
 
 	DPRINTK("Successful creation of handle=%04x (dom=%u)\n",
@@ -112,7 +112,7 @@ int vbd_translate(struct phys_req *req,
 	struct vbd *vbd = &blkif->vbd;
 	int rc = -EACCES;
 
-	if ((operation != READ) && !(vbd->mode & FMODE_WRITE))
+	if ((operation != REQ_OP_READ) && !(vbd->mode & FMODE_WRITE))
 		goto out;
 
 	if (likely(req->nr_sects)) {
--- a/drivers/xen/blkfront/blkfront.c
+++ b/drivers/xen/blkfront/blkfront.c
@@ -413,7 +413,7 @@ static void connect(struct blkfront_info
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,37)
 	info->feature_flush = 0;
 	if (err > 0 && barrier)
-		info->feature_flush = REQ_FLUSH | REQ_FUA;
+		info->feature_flush = REQ_PREFLUSH | REQ_FUA;
 	/*
 	 * And if there is "feature-flush-cache" use that above
 	 * barriers.
@@ -421,7 +421,7 @@ static void connect(struct blkfront_info
 	err = xenbus_scanf(XBT_NIL, info->xbdev->otherend,
 			   "feature-flush-cache", "%d", &flush);
 	if (err > 0 && flush)
-		info->feature_flush = REQ_FLUSH;
+		info->feature_flush = REQ_PREFLUSH;
 #else
 	if (err <= 0)
 		info->feature_flush = QUEUE_ORDERED_DRAIN;
@@ -460,7 +460,7 @@ static void connect(struct blkfront_info
 	kick_pending_request_queues(info);
 	spin_unlock_irq(&info->io_lock);
 
-	add_disk(info->gd);
+	device_add_disk(&info->xbdev->dev, info->gd);
 
 	info->is_ready = 1;
 }
@@ -846,12 +846,16 @@ static int blkif_queue_request(struct re
 	ring_req->operation = rq_data_dir(req) ?
 		BLKIF_OP_WRITE : BLKIF_OP_READ;
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,37)
+# if LINUX_VERSION_CODE >= KERNEL_VERSION(4,8,0)
+	if (req_op(req) == REQ_OP_FLUSH || (req->cmd_flags & REQ_FUA))
+# else
 	if (req->cmd_flags & (REQ_FLUSH | REQ_FUA))
-		switch (info->feature_flush & (REQ_FLUSH | REQ_FUA)) {
-		case REQ_FLUSH | REQ_FUA:
+# endif
+		switch (info->feature_flush & (REQ_PREFLUSH | REQ_FUA)) {
+		case REQ_PREFLUSH | REQ_FUA:
 			ring_req->operation = BLKIF_OP_WRITE_BARRIER;
 			break;
-		case REQ_FLUSH:
+		case REQ_PREFLUSH:
 			ring_req->operation = BLKIF_OP_FLUSH_DISKCACHE;
 			break;
 		}
@@ -860,14 +864,16 @@ static int blkif_queue_request(struct re
 		ring_req->operation = BLKIF_OP_WRITE_BARRIER;
 #endif
 
-	if (unlikely(req->cmd_flags & (REQ_DISCARD | REQ_SECURE))) {
+	if (unlikely(req_op(req) == REQ_OP_DISCARD) ||
+	    unlikely(req_op(req) == REQ_OP_SECURE_ERASE)) {
 		struct blkif_request_discard *discard = (void *)ring_req;
 
 		/* id, sector_number and handle are set above. */
 		discard->operation = BLKIF_OP_DISCARD;
 		discard->flag = 0;
 		discard->nr_sectors = blk_rq_sectors(req);
-		if ((req->cmd_flags & REQ_SECURE) && info->feature_secdiscard)
+		if (req_op(req) == REQ_OP_SECURE_ERASE &&
+		    info->feature_secdiscard)
 			discard->flag = BLKIF_DISCARD_SECURE;
 	} else {
 		ring_req->nr_segments = blk_rq_map_sg(req->q, req, info->sg);
@@ -928,8 +934,13 @@ void do_blkif_request(struct request_que
 		blk_start_request(req);
 
 		if ((req->cmd_type != REQ_TYPE_FS) ||
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,8,0)
+		    (((req_op(req) == REQ_OP_FLUSH ? REQ_PREFLUSH : 0) |
+		      (req->cmd_flags & REQ_FUA)) >
+#else
 		    ((req->cmd_flags & (REQ_FLUSH | REQ_FUA)) >
-		     (info->feature_flush & (REQ_FLUSH | REQ_FUA)))) {
+#endif
+		     (info->feature_flush & (REQ_PREFLUSH | REQ_FUA)))) {
 			req->errors = (DID_ERROR << 16) |
 				      (DRIVER_INVALID << 24);
 			__blk_end_request_all(req, -EOPNOTSUPP);
@@ -1054,7 +1065,7 @@ static irqreturn_t blkif_int(int irq, vo
 				info->feature_discard = 0;
 				info->feature_secdiscard = 0;
 				queue_flag_clear(QUEUE_FLAG_DISCARD, rq);
-				queue_flag_clear(QUEUE_FLAG_SECDISCARD, rq);
+				queue_flag_clear(QUEUE_FLAG_SECERASE, rq);
 			}
 			__blk_end_request_all(req, ret);
 			break;
--- a/drivers/xen/blkfront/block.h
+++ b/drivers/xen/blkfront/block.h
@@ -57,6 +57,10 @@
 #include <asm/io.h>
 #include <asm/uaccess.h>
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,8,0)
+# define REQ_PREFLUSH REQ_FLUSH
+#endif
+
 #define DPRINTK(_f, _a...) pr_debug(_f, ## _a)
 
 #if 0
--- a/drivers/xen/blkfront/vbd.c
+++ b/drivers/xen/blkfront/vbd.c
@@ -372,7 +372,7 @@ xlvbd_init_blk_queue(struct gendisk *gd,
 		rq->limits.discard_granularity = info->discard_granularity;
 		rq->limits.discard_alignment = info->discard_alignment;
 		if (info->feature_secdiscard)
-			queue_flag_set_unlocked(QUEUE_FLAG_SECDISCARD, rq);
+			queue_flag_set_unlocked(QUEUE_FLAG_SECERASE, rq);
 	}
 
 	/* Hard sector size and max sectors impersonate the equiv. hardware. */
@@ -474,7 +474,6 @@ xlvbd_add(blkif_sector_t capacity, int v
 	gd->first_minor = minor;
 	gd->fops = &xlvbd_block_fops;
 	gd->private_data = info;
-	gd->driverfs_dev = &(info->xbdev->dev);
 	set_capacity(gd, capacity);
 
 	if (xlvbd_init_blk_queue(gd, sector_size, physical_sector_size, info)) {
@@ -533,10 +532,10 @@ xlvbd_del(struct blkfront_info *info)
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,37)
 static const char *flush_info(unsigned int feature_flush)
 {
-	switch (feature_flush & (REQ_FLUSH | REQ_FUA)) {
-	case REQ_FLUSH | REQ_FUA:
+	switch (feature_flush & (REQ_PREFLUSH | REQ_FUA)) {
+	case REQ_PREFLUSH | REQ_FUA:
 		return "barrier";
-	case REQ_FLUSH:
+	case REQ_PREFLUSH:
 		return "flush diskcache";
 	default:
 		return "barrier or flush";
@@ -549,7 +548,7 @@ xlvbd_flush(struct blkfront_info *info)
 {
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,37)
 # if LINUX_VERSION_CODE >= KERNEL_VERSION(4,7,0)
-	blk_queue_write_cache(info->rq, info->feature_flush & REQ_FLUSH,
+	blk_queue_write_cache(info->rq, info->feature_flush & REQ_PREFLUSH,
 				info->feature_flush & REQ_FUA);
 # else
 	blk_queue_flush(info->rq, info->feature_flush);
@@ -601,7 +600,7 @@ int xlvbd_sysfs_addif(struct blkfront_in
 	int error = 0;
 
 	for (i = 0; i < ARRAY_SIZE(xlvbd_attrs); i++) {
-		error = device_create_file(info->gd->driverfs_dev,
+		error = device_create_file(&info->xbdev->dev,
 				&xlvbd_attrs[i]);
 		if (error)
 			goto fail;
@@ -610,7 +609,7 @@ int xlvbd_sysfs_addif(struct blkfront_in
 
 fail:
 	while (--i >= 0)
-		device_remove_file(info->gd->driverfs_dev, &xlvbd_attrs[i]);
+		device_remove_file(&info->xbdev->dev, &xlvbd_attrs[i]);
 	return error;
 }
 
@@ -619,7 +618,7 @@ void xlvbd_sysfs_delif(struct blkfront_i
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(xlvbd_attrs); i++)
-		device_remove_file(info->gd->driverfs_dev, &xlvbd_attrs[i]);
+		device_remove_file(&info->xbdev->dev, &xlvbd_attrs[i]);
 }
 
 #endif /* CONFIG_SYSFS */
--- a/drivers/xen/blktap2/device.c
+++ b/drivers/xen/blktap2/device.c
@@ -718,7 +718,7 @@ blktap_device_forward_request(struct blk
 
 	rq_for_each_bio_safe(bio, tmp, req) {
 		bio->bi_bdev = dev->bdev;
-		submit_bio(bio->bi_rw, bio);
+		submit_bio(bio);
 	}
 }
 
@@ -822,7 +822,8 @@ blktap_device_run_queue(struct blktap *t
 			continue;
 		}
 
-		if (req->cmd_flags & (REQ_FLUSH|REQ_FUA)) {
+		if (req_op(req) == REQ_OP_FLUSH ||
+		    (req->cmd_flags & (REQ_PREFLUSH|REQ_FUA))) {
 			blk_start_request(req);
 			__blk_end_request_all(req, -EOPNOTSUPP);
 			continue;
--- a/drivers/xen/char/mem.c
+++ b/drivers/xen/char/mem.c
@@ -43,12 +43,8 @@ static inline int range_is_allowed(unsig
 	u64 cursor = from;
 
 	while (cursor < to) {
-		if (!devmem_is_allowed(pfn)) {
-			printk(KERN_INFO
-		"Program %s tried to access /dev/mem between %Lx->%Lx.\n",
-				current->comm, from, to);
+		if (!devmem_is_allowed(pfn))
 			return 0;
-		}
 		cursor += PAGE_SIZE;
 		pfn++;
 	}
--- a/drivers/xen/core/evtchn.c
+++ b/drivers/xen/core/evtchn.c
@@ -1348,7 +1348,8 @@ struct irq_cfg *alloc_irq_and_cfg_at(uns
 {
 #ifdef CONFIG_IRQ_DOMAIN_HIERARCHY
 	struct irq_alloc_info info = {};
-	int res = __irq_domain_alloc_irqs(NULL, at, 1, node, &info, false);
+	int res = __irq_domain_alloc_irqs(NULL, at, 1, node, &info, false,
+					  NULL);
 
 	return res >= 0 || res == -EEXIST ? irq_cfg(at) : NULL;
 #else
--- a/drivers/xen/core/machine_kexec.c
+++ b/drivers/xen/core/machine_kexec.c
@@ -234,7 +234,7 @@ void __noreturn machine_kexec(struct kim
 }
 
 #ifdef CONFIG_X86
-unsigned long paddr_vmcoreinfo_note(void)
+phys_addr_t paddr_vmcoreinfo_note(void)
 {
 	return virt_to_machine(&vmcoreinfo_note);
 }
--- a/drivers/xen/evtchn.c
+++ b/drivers/xen/evtchn.c
@@ -61,6 +61,7 @@
 #else
 #include <xen/evtchn.h>
 #include <xen/public/evtchn.h>
+#define xen_vcpu_nr(cpu) (cpu)
 #define bind_evtchn_to_irqhandler bind_caller_port_to_irqhandler
 #endif
 
--- a/drivers/xen/pcifront/xenbus.c
+++ b/drivers/xen/pcifront/xenbus.c
@@ -388,8 +388,8 @@ static int pcifront_detach_devices(struc
 	return err;
 }
 
-static void __init_refok pcifront_backend_changed(struct xenbus_device *xdev,
-						  enum xenbus_state be_state)
+static void pcifront_backend_changed(struct xenbus_device *xdev,
+				     enum xenbus_state be_state)
 {
 	struct pcifront_device *pdev = dev_get_drvdata(&xdev->dev);
 
--- a/drivers/xen/scsiback/scsiback.c
+++ b/drivers/xen/scsiback/scsiback.c
@@ -373,7 +373,7 @@ static struct bio *request_map_sg(pendin
 			if (bio->bi_vcnt >= nr_vecs) {
 				bio->bi_flags &= ~(1 << BIO_SEG_VALID);
 				if (pending_req->sc_data_direction == WRITE)
-					bio->bi_rw |= REQ_WRITE;
+					bio_set_op_attrs(bio, REQ_OP_WRITE, 0);
 				bio = NULL;
 			}
 
@@ -402,7 +402,7 @@ int scsiback_cmd_exec(pending_req_t *pen
 	unsigned int timeout;
 	struct bio *bio;
 	struct request *rq;
-	int write;
+	int err;
 
 	DPRINTK("%s\n", __func__);
 
@@ -412,7 +412,6 @@ int scsiback_cmd_exec(pending_req_t *pen
 	else
 		timeout = VSCSIIF_TIMEOUT;
 
-	write = (data_dir == DMA_TO_DEVICE);
 	if (pending_req->nr_segments) {
 		bio = request_map_sg(pending_req);
 		if (IS_ERR(bio)) {
@@ -423,27 +422,26 @@ int scsiback_cmd_exec(pending_req_t *pen
 	} else
 		bio = NULL;
 
-	if (bio) {
-		rq = blk_make_request(pending_req->sdev->request_queue, bio,
-				      GFP_KERNEL);
-		if (IS_ERR(rq)) {
-			do {
-				struct bio *b = bio->bi_next;
-
-				bio_put(bio);
-				bio = b;
-			} while (bio);
-			pr_err("scsiback: Make Request Error %ld\n",
-			       PTR_ERR(rq));
-			return PTR_ERR(rq);
-		}
-	} else {
-		rq = blk_get_request(pending_req->sdev->request_queue, write,
-				     GFP_KERNEL);
-		if (unlikely(!rq)) {
-			pr_err("scsiback: Get Request Error\n");
-			return -ENOMEM;
-		}
+	rq = blk_get_request(pending_req->sdev->request_queue,
+			     data_dir == DMA_TO_DEVICE ? WRITE : READ,
+			     GFP_KERNEL);
+
+	err = PTR_ERR_OR_ZERO(rq);
+	while (bio) {
+		struct bio *b = bio->bi_next;
+
+		if (!err)
+			err = blk_rq_append_bio(rq, bio);
+		else
+			bio_put(bio);
+		bio = b;
+	}
+
+	if (err) {
+		if (!IS_ERR(rq))
+			blk_put_request(rq);
+		pr_err("scsiback: Get Request Error\n");
+		return err;
 	}
 
 	rq->cmd_type = REQ_TYPE_BLOCK_PC;
--- a/include/uapi/xen/public/evtchn.h
+++ b/include/uapi/xen/public/evtchn.h
@@ -85,4 +85,19 @@ struct ioctl_evtchn_notify {
 #define IOCTL_EVTCHN_RESET				\
 	_IOC(_IOC_NONE, 'E', 5, 0)
 
+/*
+ * Restrict this file descriptor so that it can only be used to bind
+ * new interdomain events from one domain.
+ *
+ * Once a file descriptor has been restricted it cannot be
+ * de-restricted, and must be closed and re-opened.  Event channels
+ * which were bound before restricting remain bound afterwards, and
+ * can be notified as usual.
+ */
+#define IOCTL_EVTCHN_RESTRICT_DOMID			\
+	_IOC(_IOC_NONE, 'E', 6, sizeof(struct ioctl_evtchn_restrict_domid))
+struct ioctl_evtchn_restrict_domid {
+	domid_t domid;
+};
+
 #endif /* __LINUX_PUBLIC_EVTCHN_H__ */
--- a/lib/swiotlb-xen.c
+++ b/lib/swiotlb-xen.c
@@ -607,7 +607,7 @@ swiotlb_full(struct device *dev, size_t
 dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,
 			    unsigned long offset, size_t size,
 			    enum dma_data_direction dir,
-			    struct dma_attrs *attrs)
+			    unsigned long attrs)
 {
 	phys_addr_t map, phys = page_to_pseudophys(page) + offset;
 	dma_addr_t dev_addr = gnttab_dma_map_page(page, offset);
@@ -670,7 +670,7 @@ static void unmap_single(struct device *
 
 void swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 			size_t size, enum dma_data_direction dir,
-			struct dma_attrs *attrs)
+			unsigned long attrs)
 {
 	unmap_single(hwdev, dev_addr, size, dir);
 }
@@ -733,7 +733,7 @@ EXPORT_SYMBOL(swiotlb_sync_single_for_de
  */
 int
 swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,
-		     enum dma_data_direction dir, struct dma_attrs *attrs)
+		     enum dma_data_direction dir, unsigned long attrs)
 {
 	struct scatterlist *sg;
 	int i;
@@ -775,7 +775,7 @@ int
 swiotlb_map_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 	       enum dma_data_direction dir)
 {
-	return swiotlb_map_sg_attrs(hwdev, sgl, nelems, dir, NULL);
+	return swiotlb_map_sg_attrs(hwdev, sgl, nelems, dir, 0);
 }
 EXPORT_SYMBOL(swiotlb_map_sg);
 
@@ -785,7 +785,8 @@ EXPORT_SYMBOL(swiotlb_map_sg);
  */
 void
 swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
-		       int nelems, enum dma_data_direction dir, struct dma_attrs *attrs)
+		       int nelems, enum dma_data_direction dir,
+		       unsigned long attrs)
 {
 	struct scatterlist *sg;
 	int i;
@@ -802,7 +803,7 @@ void
 swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 		 enum dma_data_direction dir)
 {
-	return swiotlb_unmap_sg_attrs(hwdev, sgl, nelems, dir, NULL);
+	return swiotlb_unmap_sg_attrs(hwdev, sgl, nelems, dir, 0);
 }
 EXPORT_SYMBOL(swiotlb_unmap_sg);
 
