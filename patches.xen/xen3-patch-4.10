From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: Linux: 4.10
Patch-mainline: Never, SUSE-Xen specific
References: none

 This patch contains the differences between 4.9 and 4.10.

Automatically created from "patch-4.10" by xen-port-patches.py
Acked-by: jbeulich@suse.com

--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -433,7 +433,7 @@ config GOLDFISH
 config INTEL_RDT_A
 	bool "Intel Resource Director Technology Allocation support"
 	default n
-	depends on X86 && CPU_SUP_INTEL
+	depends on X86 && !XEN && CPU_SUP_INTEL
 	select KERNFS
 	help
 	  Select to enable resource allocation which is a sub-feature of
--- a/arch/x86/entry/calling.h
+++ b/arch/x86/entry/calling.h
@@ -103,8 +103,8 @@ For 32-bit we have the following convent
 
 #define SIZEOF_PTREGS	21*8
 
-	.macro ALLOC_PT_GPREGS_ON_STACK
-	addq	$-(15*8), %rsp
+	.macro ALLOC_PT_GPREGS_ON_STACK addskip=0
+	addq	$-(15*8+\addskip), %rsp
 	.endm
 
 	.macro SAVE_C_REGS_HELPER offset=0 rax=1 rcx=1 r8910=1 r11=1
--- a/arch/x86/entry/entry_32-xen.S
+++ b/arch/x86/entry/entry_32-xen.S
@@ -45,6 +45,7 @@
 #include <asm/asm.h>
 #include <asm/smap.h>
 #include <asm/export.h>
+#include <asm/frame.h>
 #include <xen/interface/xen.h>
 
 	.section .entry.text, "ax"
@@ -179,6 +180,22 @@ NMI_MASK	= 0x80000000
 	SET_KERNEL_GS %edx
 .endm
 
+/*
+ * This is a sneaky trick to help the unwinder find pt_regs on the stack.  The
+ * frame pointer is replaced with an encoded pointer to pt_regs.  The encoding
+ * is just setting the LSB, which makes it an invalid stack address and is also
+ * a signal to the unwinder that it's a pt_regs pointer in disguise.
+ *
+ * NOTE: This macro must be used *after* SAVE_ALL because it corrupts the
+ * original rbp.
+ */
+.macro ENCODE_FRAME_POINTER
+#ifdef CONFIG_FRAME_POINTER
+	mov %esp, %ebp
+	orl $0x1, %ebp
+#endif
+.endm
+
 .macro RESTORE_INT_REGS
 	popl	%ebx
 	popl	%ecx
@@ -249,6 +266,12 @@ END(__switch_to_asm)
  * edi: kernel thread arg
  */
 ENTRY(ret_from_fork)
+	FRAME_BEGIN		/* help unwinder find end of stack */
+
+	/*
+	 * schedule_tail() is asmlinkage so we have to put its 'prev' argument
+	 * on the stack.
+	 */
 	pushl	%eax
 	call	schedule_tail
 	popl	%eax
@@ -258,8 +281,9 @@ ENTRY(ret_from_fork)
 
 2:
 	/* When we fork, we trace the syscall return in the child, too. */
-	movl    %esp, %eax
+	leal	FRAME_OFFSET(%esp), %eax
 	call    syscall_return_slowpath
+	FRAME_END
 	jmp     restore_all
 
 	/* kernel thread */
@@ -311,13 +335,13 @@ END(ret_from_exception)
 #ifdef CONFIG_PREEMPT
 ENTRY(resume_kernel)
 	DISABLE_INTERRUPTS(CLBR_ANY)
-need_resched:
+.Lneed_resched:
 	cmpl	$0, PER_CPU_VAR(__preempt_count)
 	jnz	restore_all
 	testl	$X86_EFLAGS_IF, PT_EFLAGS(%esp)	# interrupts off (exception path) ?
 	jz	restore_all
 	call	preempt_schedule_irq
-	jmp	need_resched
+	jmp	.Lneed_resched
 END(resume_kernel)
 #endif
 
@@ -365,7 +389,7 @@ GLOBAL(__begin_SYSENTER_singlestep_regio
  */
 ENTRY(entry_SYSENTER_32)
 	movl	SYSENTER_stack_sp0(%esp), %esp
-sysenter_past_esp:
+.Lsysenter_past_esp:
 	pushl	$__USER_DS		/* pt_regs->ss */
 	pushl	%ebp			/* pt_regs->sp (stashed in bp) */
 	pushfl				/* pt_regs->flags (except IF = 0) */
@@ -513,9 +537,9 @@ ENTRY(entry_INT80_32)
 
 restore_all:
 	TRACE_IRQS_IRET
-restore_all_notrace:
+.Lrestore_all_notrace:
 #ifdef CONFIG_X86_ESPFIX32
-	ALTERNATIVE	"jmp restore_nocheck", "", X86_BUG_ESPFIX
+	ALTERNATIVE	"jmp .Lrestore_nocheck", "", X86_BUG_ESPFIX
 
 	movl	PT_EFLAGS(%esp), %eax		# mix EFLAGS, SS and CS
 	/*
@@ -527,9 +551,9 @@ restore_all_notrace:
 	movb	PT_CS(%esp), %al
 	andl	$(X86_EFLAGS_VM | (SEGMENT_TI_MASK << 8) | SEGMENT_RPL_MASK), %eax
 	cmpl	$((SEGMENT_LDT << 8) | USER_RPL), %eax
-	je ldt_ss				# returning to user-space with LDT SS
+	je .Lldt_ss				# returning to user-space with LDT SS
 #endif
-restore_nocheck:
+.Lrestore_nocheck:
 #ifdef CONFIG_XEN
 	movl	PT_EFLAGS(%esp), %eax
 	testl	$(X86_EFLAGS_VM|NMI_MASK), %eax
@@ -541,18 +565,19 @@ restore_nocheck:
 	jnz	restore_all_enable_events	#        != 0 => enable event delivery
 #endif
 	RESTORE_REGS 4				# skip orig_eax/error_code
-irq_return:
+.Lirq_return:
 	INTERRUPT_RETURN
+
 .section .fixup, "ax"
 ENTRY(iret_exc	)
 	pushl	$0				# no error code
 	pushl	$do_iret_error
-	jmp	error_code
+	jmp	common_exception
 .previous
-	_ASM_EXTABLE(irq_return, iret_exc)
+	_ASM_EXTABLE(.Lirq_return, iret_exc)
 
 #ifdef CONFIG_X86_ESPFIX32
-ldt_ss:
+.Lldt_ss:
 /*
  * Setup and switch to ESPFIX stack
  *
@@ -581,7 +606,7 @@ ldt_ss:
 	 */
 	DISABLE_INTERRUPTS(CLBR_EAX)
 	lss	(%esp), %esp			/* switch to espfix segment */
-	jmp	restore_nocheck
+	jmp	.Lrestore_nocheck
 #endif
 #ifdef CONFIG_XEN
         ALIGN
@@ -665,6 +690,7 @@ common_interrupt:
 	ASM_CLAC
 	addl	$-0x80, (%esp)			/* Adjust vector into the [-256, -1] range */
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	TRACE_IRQS_OFF
 	movl	%esp, %eax
 	call	do_IRQ
@@ -676,6 +702,7 @@ ENTRY(name)				\
 	ASM_CLAC;			\
 	pushl	$~(nr);			\
 	SAVE_ALL;			\
+	ENCODE_FRAME_POINTER;		\
 	TRACE_IRQS_OFF			\
 	movl	%esp, %eax;		\
 	call	fn;			\
@@ -719,6 +746,7 @@ ENTRY(hypervisor_callback)
 	ASM_CLAC
 	pushl	$-1
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	movl	PT_CS(%esp), %ecx
 	movl	PT_EIP(%esp), %eax
 	andl	$SEGMENT_RPL_MASK, %ecx
@@ -811,6 +839,7 @@ ENTRY(failsafe_callback)
 	jnz	iret_exc		# EAX != 0 => Category 2 (Bad IRET)
 	pushl	$-1			# EAX == 0 => Category 1 (Bad segment)
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	jmp	ret_from_exception
 .section .fixup,"ax"
 6:	xorl	%eax, %eax
@@ -836,7 +865,7 @@ ENTRY(coprocessor_error)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_coprocessor_error
-	jmp	error_code
+	jmp	common_exception
 END(coprocessor_error)
 
 ENTRY(simd_coprocessor_error)
@@ -850,14 +879,14 @@ ENTRY(simd_coprocessor_error)
 #else
 	pushl	$do_simd_coprocessor_error
 #endif
-	jmp	error_code
+	jmp	common_exception
 END(simd_coprocessor_error)
 
 ENTRY(device_not_available)
 	ASM_CLAC
 	pushl	$-1				# mark this as an int
 	pushl	$do_device_not_available
-	jmp	error_code
+	jmp	common_exception
 END(device_not_available)
 
 #ifdef CONFIG_PARAVIRT
@@ -871,59 +900,59 @@ ENTRY(overflow)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_overflow
-	jmp	error_code
+	jmp	common_exception
 END(overflow)
 
 ENTRY(bounds)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_bounds
-	jmp	error_code
+	jmp	common_exception
 END(bounds)
 
 ENTRY(invalid_op)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_invalid_op
-	jmp	error_code
+	jmp	common_exception
 END(invalid_op)
 
 ENTRY(coprocessor_segment_overrun)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_coprocessor_segment_overrun
-	jmp	error_code
+	jmp	common_exception
 END(coprocessor_segment_overrun)
 
 ENTRY(invalid_TSS)
 	ASM_CLAC
 	pushl	$do_invalid_TSS
-	jmp	error_code
+	jmp	common_exception
 END(invalid_TSS)
 
 ENTRY(segment_not_present)
 	ASM_CLAC
 	pushl	$do_segment_not_present
-	jmp	error_code
+	jmp	common_exception
 END(segment_not_present)
 
 ENTRY(stack_segment)
 	ASM_CLAC
 	pushl	$do_stack_segment
-	jmp	error_code
+	jmp	common_exception
 END(stack_segment)
 
 ENTRY(alignment_check)
 	ASM_CLAC
 	pushl	$do_alignment_check
-	jmp	error_code
+	jmp	common_exception
 END(alignment_check)
 
 ENTRY(divide_error)
 	ASM_CLAC
 	pushl	$0				# no error code
 	pushl	$do_divide_error
-	jmp	error_code
+	jmp	common_exception
 END(divide_error)
 
 #ifdef CONFIG_X86_MCE
@@ -931,7 +960,7 @@ ENTRY(machine_check)
 	ASM_CLAC
 	pushl	$0
 	pushl	machine_check_vector
-	jmp	error_code
+	jmp	common_exception
 END(machine_check)
 #endif
 
@@ -940,13 +969,13 @@ ENTRY(spurious_interrupt_bug)
 	ASM_CLAC
 	pushl	$0
 	pushl	$do_spurious_interrupt_bug
-	jmp	error_code
+	jmp	common_exception
 END(spurious_interrupt_bug)
 #endif /* !CONFIG_XEN */
 
 ENTRY(fixup_4gb_segment)
 	pushl	$do_fixup_4gb_segment
-	jmp error_code
+	jmp	common_exception
 END(fixup_4gb_segment)
 
 #ifdef CONFIG_FUNCTION_TRACER
@@ -974,15 +1003,15 @@ ftrace_call:
 	popl	%edx
 	popl	%ecx
 	popl	%eax
-ftrace_ret:
+.Lftrace_ret:
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 .globl ftrace_graph_call
 ftrace_graph_call:
 	jmp	ftrace_stub
 #endif
 
-.globl ftrace_stub
-ftrace_stub:
+/* This is weak to keep gas from relaxing the jumps */
+WEAK(ftrace_stub)
 	ret
 END(ftrace_caller)
 
@@ -1044,7 +1073,7 @@ GLOBAL(ftrace_regs_call)
 	popl	%gs
 	addl	$8, %esp			/* Skip orig_ax and ip */
 	popf					/* Pop flags at end (no addl to corrupt flags) */
-	jmp	ftrace_ret
+	jmp	.Lftrace_ret
 
 	popf
 	jmp	ftrace_stub
@@ -1055,7 +1084,7 @@ ENTRY(mcount)
 	jb	ftrace_stub			/* Paging not enabled yet? */
 
 	cmpl	$ftrace_stub, ftrace_trace_function
-	jnz	trace
+	jnz	.Ltrace
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	cmpl	$ftrace_stub, ftrace_graph_return
 	jnz	ftrace_graph_caller
@@ -1068,7 +1097,7 @@ ftrace_stub:
 	ret
 
 	/* taken from glibc */
-trace:
+.Ltrace:
 	pushl	%eax
 	pushl	%ecx
 	pushl	%edx
@@ -1126,7 +1155,7 @@ ENTRY(entry_SYSCALL_PV32)
 ENTRY(trace_page_fault)
 	ASM_CLAC
 	pushl	$trace_do_page_fault
-	jmp	error_code
+	jmp	common_exception
 END(trace_page_fault)
 #endif
 
@@ -1134,7 +1163,10 @@ ENTRY(page_fault)
 	ASM_CLAC
 	pushl	$do_page_fault
 	ALIGN
-error_code:
+	jmp common_exception
+END(page_fault)
+
+common_exception:
 	/* the function address is in %gs's slot on the stack */
 	pushl	%fs
 	pushl	%es
@@ -1146,6 +1178,7 @@ error_code:
 	pushl	%edx
 	pushl	%ecx
 	pushl	%ebx
+	ENCODE_FRAME_POINTER
 	cld
 	movl	$(__KERNEL_PERCPU), %ecx
 	movl	%ecx, %fs
@@ -1163,7 +1196,7 @@ error_code:
 	movl	%esp, %eax			# pt_regs pointer
 	call	*%edi
 	jmp	ret_from_exception
-END(page_fault)
+END(common_exception)
 
 ENTRY(debug)
 	/*
@@ -1178,6 +1211,7 @@ ENTRY(debug)
 	ASM_CLAC
 	pushl	$-1				# mark this as an int
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	xorl	%edx, %edx			# error code 0
 	movl	%esp, %eax			# pt_regs pointer
 
@@ -1196,11 +1230,11 @@ ENTRY(debug)
 #ifndef CONFIG_XEN
 .Ldebug_from_sysenter_stack:
 	/* We're on the SYSENTER stack.  Switch off. */
-	movl	%esp, %ebp
+	movl	%esp, %ebx
 	movl	PER_CPU_VAR(cpu_current_top_of_stack), %esp
 	TRACE_IRQS_OFF
 	call	do_debug
-	movl	%ebp, %esp
+	movl	%ebx, %esp
 	jmp	ret_from_exception
 #endif
 END(debug)
@@ -1219,12 +1253,13 @@ ENTRY(nmi)
 	movl	%ss, %eax
 	cmpw	$__ESPFIX_SS, %ax
 	popl	%eax
-	je	nmi_espfix_stack
+	je	.Lnmi_espfix_stack
 #endif
 #ifndef CONFIG_XEN
 
 	pushl	%eax				# pt_regs->orig_ax
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	xorl	%edx, %edx			# zero error code
 	movl	%esp, %eax			# pt_regs pointer
 
@@ -1236,21 +1271,21 @@ ENTRY(nmi)
 
 	/* Not on SYSENTER stack. */
 	call	do_nmi
-	jmp	restore_all_notrace
+	jmp	.Lrestore_all_notrace
 
 .Lnmi_from_sysenter_stack:
 	/*
 	 * We're on the SYSENTER stack.  Switch off.  No one (not even debug)
 	 * is using the thread stack right now, so it's safe for us to use it.
 	 */
-	movl	%esp, %ebp
+	movl	%esp, %ebx
 	movl	PER_CPU_VAR(cpu_current_top_of_stack), %esp
 	call	do_nmi
-	movl	%ebp, %esp
-	jmp	restore_all_notrace
+	movl	%ebx, %esp
+	jmp	.Lrestore_all_notrace
 
 #ifdef CONFIG_X86_ESPFIX32
-nmi_espfix_stack:
+.Lnmi_espfix_stack:
 	/*
 	 * create the pointer to lss back
 	 */
@@ -1263,16 +1298,18 @@ nmi_espfix_stack:
 	.endr
 	pushl	%eax
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	FIXUP_ESPFIX_STACK			# %eax == %esp
 	xorl	%edx, %edx			# zero error code
 	call	do_nmi
 	RESTORE_REGS
 	lss	12+4(%esp), %esp		# back to espfix stack
-	jmp	irq_return
+	jmp	.Lirq_return
 #endif
 #else /* CONFIG_XEN */
  	pushl	%eax
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	xorl	%edx, %edx			# zero error code
 	movl	%esp, %eax			# pt_regs pointer
 	call	do_nmi
@@ -1285,6 +1322,7 @@ ENTRY(int3)
 	ASM_CLAC
 	pushl	$-1				# mark this as an int
 	SAVE_ALL
+	ENCODE_FRAME_POINTER
 	TRACE_IRQS_OFF
 	xorl	%edx, %edx			# zero error code
 	movl	%esp, %eax			# pt_regs pointer
@@ -1294,14 +1332,14 @@ END(int3)
 
 ENTRY(general_protection)
 	pushl	$do_general_protection
-	jmp	error_code
+	jmp	common_exception
 END(general_protection)
 
 #ifdef CONFIG_KVM_GUEST
 ENTRY(async_page_fault)
 	ASM_CLAC
 	pushl	$do_async_page_fault
-	jmp	error_code
+	jmp	common_exception
 END(async_page_fault)
 #endif
 
--- a/arch/x86/entry/entry_64-xen.S
+++ b/arch/x86/entry/entry_64-xen.S
@@ -39,16 +39,11 @@
 #include <asm/smap.h>
 #include <asm/pgtable_types.h>
 #include <asm/export.h>
+#include <asm/frame.h>
 #include <linux/err.h>
 #include <xen/interface/xen.h>
 #include <xen/interface/features.h>
 
-/* Avoid __ASSEMBLER__'ifying <linux/audit.h> just for this.  */
-#include <linux/elf-em.h>
-#define AUDIT_ARCH_X86_64			(EM_X86_64|__AUDIT_ARCH_64BIT|__AUDIT_ARCH_LE)
-#define __AUDIT_ARCH_64BIT			0x80000000
-#define __AUDIT_ARCH_LE				0x40000000
-
 .code64
 .section .entry.text, "ax"
 
@@ -343,15 +338,17 @@ END(__switch_to_asm)
  * r12: kernel thread arg
  */
 ENTRY(ret_from_fork)
+	FRAME_BEGIN			/* help unwinder find end of stack */
 	movq	%rax, %rdi
-	call	schedule_tail			/* rdi: 'prev' task parameter */
+	call	schedule_tail		/* rdi: 'prev' task parameter */
 
-	testq	%rbx, %rbx			/* from kernel_thread? */
-	jnz	1f				/* kernel threads are uncommon */
+	testq	%rbx, %rbx		/* from kernel_thread? */
+	jnz	1f			/* kernel threads are uncommon */
 
 2:
-	movq	%rsp, %rdi
+	leaq	FRAME_OFFSET(%rsp),%rdi	/* pt_regs pointer */
 	call	syscall_return_slowpath	/* returns with IRQs disabled */
+	FRAME_END
 	jmp	restore_regs_and_iret
 
 1:
@@ -564,6 +561,7 @@ restore_all_enable_events:
 	__DISABLE_INTERRUPTS
 .Lecrit: /**** END OF CRITICAL REGION ****/
 	SAVE_EXTRA_REGS
+	ENCODE_FRAME_POINTER
         movq	%rsp, %rdi		# set the argument again
 	jmp	11b
 # At this point, unlike on x86-32, we don't do the fixup to simplify the 
@@ -611,6 +609,7 @@ ENTRY(failsafe_callback)
 	ALLOC_PT_GPREGS_ON_STACK
 	SAVE_C_REGS
 	SAVE_EXTRA_REGS
+	ENCODE_FRAME_POINTER
 	jmp	error_exit
 
 idtentry divide_error			do_divide_error			has_error_code=0
@@ -665,6 +664,7 @@ ENTRY(paranoid_entry)
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
+	ENCODE_FRAME_POINTER 8
 	movl	$1, %ebx
 	movl	$MSR_GS_BASE, %ecx
 	rdmsr
@@ -713,6 +713,7 @@ ENTRY(error_entry)
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
+	ENCODE_FRAME_POINTER 8
 #ifndef CONFIG_XEN
 	xorl	%ebx, %ebx
 	testb	$3, CS+8(%rsp)
--- a/arch/x86/include/asm/microcode_intel.h
+++ b/arch/x86/include/asm/microcode_intel.h
@@ -3,6 +3,7 @@
 
 #include <asm/microcode.h>
 
+#ifndef CONFIG_XEN
 struct microcode_header_intel {
 	unsigned int            hdrver;
 	unsigned int            rev;
@@ -66,6 +67,7 @@ static inline u32 intel_get_microcode_re
 
 	return rev;
 }
+#endif /* CONFIG_XEN */
 
 #ifdef CONFIG_MICROCODE_INTEL
 extern void __init load_ucode_intel_bsp(void);
--- a/arch/x86/include/mach-xen/asm/hypervisor.h
+++ b/arch/x86/include/mach-xen/asm/hypervisor.h
@@ -72,7 +72,7 @@ extern start_info_t *xen_start_info;
 
 DECLARE_PER_CPU(struct vcpu_runstate_info, runstate);
 struct vcpu_runstate_info *setup_runstate_area(unsigned int cpu);
-#define vcpu_running(cpu) (per_cpu(runstate.state, cpu) == RUNSTATE_running)
+#define vcpu_is_preempted(cpu) (per_cpu(runstate.state, cpu) != RUNSTATE_running)
 
 /* arch/xen/kernel/evtchn.c */
 /* Force a proper event-channel callback from Xen. */
--- a/arch/x86/include/mach-xen/asm/mmu_context.h
+++ b/arch/x86/include/mach-xen/asm/mmu_context.h
@@ -54,7 +54,7 @@ struct ldt_struct {
 	 * allocations, but it's not worth trying to optimize.
 	 */
 	struct desc_struct *entries;
-	int size;
+	unsigned int size;
 };
 
 /*
--- a/arch/x86/include/mach-xen/asm/pgtable_64.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_64.h
@@ -116,8 +116,7 @@ static inline void xen_pgd_clear(pgd_t *
 
 #define __pte_mfn(_pte) (((_pte).pte & PTE_PFN_MASK) >> PAGE_SHIFT)
 
-extern void sync_global_pgds(unsigned long start, unsigned long end,
-			     int removed);
+extern void sync_global_pgds(unsigned long start, unsigned long end);
 
 /*
  * Conversion functions: convert a page and protection to a page entry,
--- a/arch/x86/include/mach-xen/asm/processor.h
+++ b/arch/x86/include/mach-xen/asm/processor.h
@@ -109,6 +109,7 @@ struct cpuinfo_x86 {
 #ifndef CONFIG_XEN
 	/* CPUID returned core id bits: */
 	__u8			x86_coreid_bits;
+	__u8			cu_id;
 #endif
 	/* Max extended CPUID function supported: */
 	__u32			extended_cpuid_level;
@@ -149,6 +150,17 @@ struct cpuinfo_x86 {
 #endif
 };
 
+struct cpuid_regs {
+	u32 eax, ebx, ecx, edx;
+};
+
+enum cpuid_regs_idx {
+	CPUID_EAX = 0,
+	CPUID_EBX,
+	CPUID_ECX,
+	CPUID_EDX,
+};
+
 #define X86_VENDOR_INTEL	0
 #define X86_VENDOR_CYRIX	1
 #define X86_VENDOR_AMD		2
@@ -189,6 +201,9 @@ extern void identify_secondary_cpu(struc
 extern void print_cpu_info(struct cpuinfo_x86 *);
 void print_cpu_msr(struct cpuinfo_x86 *);
 extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
+extern u32 get_scattered_cpuid_leaf(unsigned int level,
+				    unsigned int sub_leaf,
+				    enum cpuid_regs_idx reg);
 extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
 extern void init_amd_cacheinfo(struct cpuinfo_x86 *c);
 
@@ -216,6 +231,24 @@ static inline void xen_cpuid(unsigned in
 	    : "memory");
 }
 
+#define xen_cpuid_reg(reg)					\
+static inline unsigned int xen_cpuid_##reg(unsigned int op)	\
+{								\
+	unsigned int eax = op, ebx, ecx = 0, edx;		\
+								\
+	xen_cpuid(&eax, &ebx, &ecx, &edx);			\
+								\
+	return reg;						\
+}
+
+/*
+ * Native CPUID functions returning a single datum.
+ */
+xen_cpuid_reg(eax)
+xen_cpuid_reg(ebx)
+xen_cpuid_reg(ecx)
+xen_cpuid_reg(edx)
+
 static inline void load_cr3(pgd_t *pgdir)
 {
 	write_cr3(__pa(pgdir));
@@ -593,43 +626,76 @@ static __always_inline void cpu_relax(vo
 	rep_nop();
 }
 
-#define cpu_relax_lowlatency() cpu_relax()
-
-/* Stop speculative execution and prefetching of modified code. */
+/*
+ * This function forces the icache and prefetched instruction stream to
+ * catch up with reality in two very specific cases:
+ *
+ *  a) Text was modified using one virtual address and is about to be executed
+ *     from the same physical page at a different virtual address.
+ *
+ *  b) Text was modified on a different CPU, may subsequently be
+ *     executed on this CPU, and you want to make sure the new version
+ *     gets executed.  This generally means you're calling this in a IPI.
+ *
+ * If you're calling this for a different reason, you're probably doing
+ * it wrong.
+ */
 static inline void sync_core(void)
 {
-	int tmp;
-
-#ifdef CONFIG_M486
 	/*
-	 * Do a CPUID if available, otherwise do a jump.  The jump
-	 * can conveniently enough be the jump around CPUID.
+	 * There are quite a few ways to do this.  IRET-to-self is nice
+	 * because it works on every CPU, at any CPL (so it's compatible
+	 * with paravirtualization), and it never exits to a hypervisor.
+	 * The only down sides are that it's a bit slow (it seems to be
+	 * a bit more than 2x slower than the fastest options) and that
+	 * it unmasks NMIs.  The "push %cs" is needed because, in
+	 * paravirtual environments, __KERNEL_CS may not be a valid CS
+	 * value when we do IRET directly.
+	 *
+	 * In case NMI unmasking or performance ever becomes a problem,
+	 * the next best option appears to be MOV-to-CR2 and an
+	 * unconditional jump.  That sequence also works on all CPUs,
+	 * but it will fault at CPL3 (i.e. Xen PV and lguest).
+	 *
+	 * CPUID is the conventional way, but it's nasty: it doesn't
+	 * exist on some 486-like CPUs, and it usually exits to a
+	 * hypervisor.
+	 *
+	 * Like all of Linux's memory ordering operations, this is a
+	 * compiler barrier as well.
 	 */
-	asm volatile("cmpl %2,%1\n\t"
-		     "jl 1f\n\t"
-		     "cpuid\n"
-		     "1:"
-		     : "=a" (tmp)
-		     : "rm" (boot_cpu_data.cpuid_level), "ri" (0), "0" (1)
-		     : "ebx", "ecx", "edx", "memory");
+	register void *__sp asm(_ASM_SP);
+
+#ifdef CONFIG_X86_32
+	asm volatile (
+		"pushfl\n\t"
+		"pushl %%cs\n\t"
+		"pushl $1f\n\t"
+		"iret\n\t"
+		"1:"
+		: "+r" (__sp) : : "memory");
 #else
-	/*
-	 * CPUID is a barrier to speculative execution.
-	 * Prefetched instructions are automatically
-	 * invalidated when modified.
-	 */
-	asm volatile("cpuid"
-		     : "=a" (tmp)
-		     : "0" (1)
-		     : "ebx", "ecx", "edx", "memory");
+	unsigned int tmp;
+
+	asm volatile (
+		"mov %%ss, %0\n\t"
+		"pushq %q0\n\t"
+		"pushq %%rsp\n\t"
+		"addq $8, (%%rsp)\n\t"
+		"pushfq\n\t"
+		"mov %%cs, %0\n\t"
+		"pushq %q0\n\t"
+		"pushq $1f\n\t"
+		"iretq\n\t"
+		"1:"
+		: "=&r" (tmp), "+r" (__sp) : : "cc", "memory");
 #endif
 }
 
 extern void select_idle_routine(const struct cpuinfo_x86 *c);
-extern void init_amd_e400_c1e_mask(void);
+extern void amd_e400_c1e_apic_setup(void);
 
 extern unsigned long		boot_option_idle_override;
-extern bool			amd_e400_c1e_detected;
 
 enum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,
 			 IDLE_POLL};
--- a/arch/x86/include/mach-xen/asm/special_insns.h
+++ b/arch/x86/include/mach-xen/asm/special_insns.h
@@ -25,30 +25,6 @@ static inline void xen_clear_cr0_upd(voi
 	raw_cpu_write_l(xen_x86_cr0_upd, 0);
 }
 
-static inline void xen_clts(void)
-{
-	if (unlikely(xen_read_cr0_upd()))
-		HYPERVISOR_fpu_taskswitch(0);
-	else if (raw_cpu_read_4(xen_x86_cr0) & X86_CR0_TS) {
-		raw_cpu_write_4(xen_x86_cr0_upd, X86_CR0_TS);
-		HYPERVISOR_fpu_taskswitch(0);
-		raw_cpu_and_4(xen_x86_cr0, ~X86_CR0_TS);
-		xen_clear_cr0_upd();
-	}
-}
-
-static inline void xen_stts(void)
-{
-	if (unlikely(xen_read_cr0_upd()))
-		HYPERVISOR_fpu_taskswitch(1);
-	else if (!(raw_cpu_read_4(xen_x86_cr0) & X86_CR0_TS)) {
-		raw_cpu_write_4(xen_x86_cr0_upd, X86_CR0_TS);
-		HYPERVISOR_fpu_taskswitch(1);
-		raw_cpu_or_4(xen_x86_cr0, X86_CR0_TS);
-		xen_clear_cr0_upd();
-	}
-}
-
 /*
  * Volatile isn't enough to prevent the compiler from reordering the
  * read/write functions for the control registers and messing everything up.
@@ -84,16 +60,11 @@ static inline void xen_write_cr0(unsigne
 		native_write_cr0(val);
 		return;
 	}
-	switch (upd) {
-	case 0:
+
+	if (!upd)
 		return;
-	case X86_CR0_TS:
-		HYPERVISOR_fpu_taskswitch(!!(val & X86_CR0_TS));
-		break;
-	default:
-		native_write_cr0(val);
-		break;
-	}
+
+	native_write_cr0(val);
 	raw_cpu_write_l(xen_x86_cr0, val);
 	xen_clear_cr0_upd();
 }
@@ -256,17 +227,6 @@ static inline void load_gs_index(unsigne
 
 #endif
 
-/* Clear the 'TS' bit */
-static inline void clts(void)
-{
-	xen_clts();
-}
-
-static inline void stts(void)
-{
-	xen_stts();
-}
-
 static inline void clflush(volatile void *__p)
 {
 	asm volatile("clflush %0" : "+m" (*(volatile char __force *)__p));
--- a/arch/x86/kernel/apic/io_apic-xen.c
+++ b/arch/x86/kernel/apic/io_apic-xen.c
@@ -48,7 +48,6 @@
 #include <linux/bootmem.h>
 
 #include <asm/irqdomain.h>
-#include <asm/idle.h>
 #include <asm/io.h>
 #include <asm/smp.h>
 #include <asm/cpu.h>
@@ -2197,6 +2196,7 @@ static inline void __init check_timer(vo
 			if (idx != -1 && irq_trigger(idx))
 				unmask_ioapic_irq(irq_get_chip_data(0));
 		}
+		irq_domain_deactivate_irq(irq_data);
 		irq_domain_activate_irq(irq_data);
 		if (timer_irq_works()) {
 			if (disable_timer_pin_1 > 0)
@@ -2218,6 +2218,7 @@ static inline void __init check_timer(vo
 		 * legacy devices should be connected to IO APIC #0
 		 */
 		replace_pin_at_irq_node(data, node, apic1, pin1, apic2, pin2);
+		irq_domain_deactivate_irq(irq_data);
 		irq_domain_activate_irq(irq_data);
 		legacy_pic->unmask(0);
 		if (timer_irq_works()) {
--- a/arch/x86/kernel/cpu/common-xen.c
+++ b/arch/x86/kernel/cpu/common-xen.c
@@ -711,13 +711,14 @@ void get_cpu_cap(struct cpuinfo_x86 *c)
 		c->x86_capability[CPUID_1_EDX] = edx;
 	}
 
+	/* Thermal and Power Management Leaf: level 0x00000006 (eax) */
+	if (c->cpuid_level >= 0x00000006)
+		c->x86_capability[CPUID_6_EAX] = cpuid_eax(0x00000006);
+
 	/* Additional Intel-defined flags: level 0x00000007 */
 	if (c->cpuid_level >= 0x00000007) {
 		cpuid_count(0x00000007, 0, &eax, &ebx, &ecx, &edx);
-
 		c->x86_capability[CPUID_7_0_EBX] = ebx;
-
-		c->x86_capability[CPUID_6_EAX] = cpuid_eax(0x00000006);
 		c->x86_capability[CPUID_7_ECX] = ecx;
 	}
 
@@ -1030,30 +1031,22 @@ static void x86_init_cache_qos(struct cp
 }
 
 /*
- * The physical to logical package id mapping is initialized from the
- * acpi/mptables information. Make sure that CPUID actually agrees with
- * that.
+ * Validate that ACPI/mptables have the same information about the
+ * effective APIC id and update the package map.
  */
-static void sanitize_package_id(struct cpuinfo_x86 *c)
+static void validate_apic_and_package_id(struct cpuinfo_x86 *c)
 {
 #ifndef CONFIG_XEN
 #ifdef CONFIG_SMP
-	unsigned int pkg, apicid, cpu = smp_processor_id();
+	unsigned int apicid, cpu = smp_processor_id();
 
 	apicid = apic->cpu_present_to_apicid(cpu);
-	pkg = apicid >> boot_cpu_data.x86_coreid_bits;
 
-	if (apicid != c->initial_apicid) {
-		pr_err(FW_BUG "CPU%u: APIC id mismatch. Firmware: %x CPUID: %x\n",
+	if (apicid != c->apicid) {
+		pr_err(FW_BUG "CPU%u: APIC id mismatch. Firmware: %x APIC: %x\n",
 		       cpu, apicid, c->initial_apicid);
-		c->initial_apicid = apicid;
-	}
-	if (pkg != c->phys_proc_id) {
-		pr_err(FW_BUG "CPU%u: Using firmware package id %u instead of %u\n",
-		       cpu, pkg, c->phys_proc_id);
-		c->phys_proc_id = pkg;
 	}
-	c->logical_proc_id = topology_phys_to_logical_pkg(pkg);
+	BUG_ON(topology_update_package_map(c->phys_proc_id, cpu));
 #else
 	c->logical_proc_id = 0;
 #endif
@@ -1076,6 +1069,7 @@ static void identify_cpu(struct cpuinfo_
 #ifndef CONFIG_XEN
 	c->x86_max_cores = 1;
 	c->x86_coreid_bits = 0;
+	c->cu_id = 0xff;
 #endif
 #ifdef CONFIG_X86_64
 	c->x86_clflush_size = 64;
@@ -1189,7 +1183,6 @@ static void identify_cpu(struct cpuinfo_
 #ifdef CONFIG_NUMA
 	numa_add_cpu(smp_processor_id());
 #endif
-	sanitize_package_id(c);
 }
 
 /*
@@ -1245,7 +1238,6 @@ void enable_sep_cpu(void)
 void __init identify_boot_cpu(void)
 {
 	identify_cpu(&boot_cpu_data);
-	init_amd_e400_c1e_mask();
 #ifdef CONFIG_X86_32
 	sysenter_setup();
 	enable_sep_cpu();
@@ -1265,53 +1257,9 @@ void identify_secondary_cpu(struct cpuin
 	enable_sep_cpu();
 #endif
 	mtrr_ap_init();
+	validate_apic_and_package_id(c);
 }
 
-struct msr_range {
-	unsigned	min;
-	unsigned	max;
-};
-
-static const struct msr_range msr_range_array[] = {
-	{ 0x00000000, 0x00000418},
-	{ 0xc0000000, 0xc000040b},
-	{ 0xc0010000, 0xc0010142},
-	{ 0xc0011000, 0xc001103b},
-};
-
-static void __print_cpu_msr(void)
-{
-	unsigned index_min, index_max;
-	unsigned index;
-	u64 val;
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(msr_range_array); i++) {
-		index_min = msr_range_array[i].min;
-		index_max = msr_range_array[i].max;
-
-		for (index = index_min; index < index_max; index++) {
-			if (rdmsrl_safe(index, &val))
-				continue;
-			pr_info(" MSR%08x: %016llx\n", index, val);
-		}
-	}
-}
-
-static int show_msr;
-
-static __init int setup_show_msr(char *arg)
-{
-	int num;
-
-	get_option(&arg, &num);
-
-	if (num > 0)
-		show_msr = num;
-	return 1;
-}
-__setup("show_msr=", setup_show_msr);
-
 static __init int setup_noclflush(char *arg)
 {
 	setup_clear_cpu_cap(X86_FEATURE_CLFLUSH);
@@ -1345,21 +1293,13 @@ void print_cpu_info(struct cpuinfo_x86 *
 		pr_cont(", stepping: 0x%x)\n", c->x86_mask);
 	else
 		pr_cont(")\n");
-
-	print_cpu_msr(c);
-}
-
-void print_cpu_msr(struct cpuinfo_x86 *c)
-{
-	if (c->cpu_index < show_msr)
-		__print_cpu_msr();
 }
 
 static __init int setup_disablecpuid(char *arg)
 {
 	int bit;
 
-	if (get_option(&arg, &bit) && bit < NCAPINTS*32)
+	if (get_option(&arg, &bit) && bit >= 0 && bit < NCAPINTS * 32)
 		setup_clear_cpu_cap(bit);
 	else
 		return 0;
@@ -1615,11 +1555,8 @@ void cpu_init(void)
 	 */
 	cr4_init_shadow();
 
-	/*
-	 * Load microcode on this cpu if a valid microcode is available.
-	 * This is early microcode loading procedure.
-	 */
-	load_ucode_ap();
+	if (cpu)
+		load_ucode_ap();
 
 	/* CPU 0 is initialised in head64.c */
 	if (cpu != 0)
--- a/arch/x86/kernel/cpu/microcode/core-xen.c
+++ b/arch/x86/kernel/cpu/microcode/core-xen.c
@@ -3,7 +3,7 @@
  *
  * Copyright (C) 2000-2006 Tigran Aivazian <tigran@aivazian.fsnet.co.uk>
  *	      2006	Shaohua Li <shaohua.li@intel.com>
- *	      2013-2015	Borislav Petkov <bp@alien8.de>
+ *	      2013-2016	Borislav Petkov <bp@alien8.de>
  *
  * X86 CPU microcode early update for Linux:
  *
@@ -38,7 +38,7 @@
 #include <asm/processor.h>
 #include <asm/cpu_device_id.h>
 
-#define MICROCODE_VERSION	"2.01-xen"
+#define DRIVER_VERSION	"2.2-xen"
 
 static bool dis_ucode_ldr;
 module_param(dis_ucode_ldr, bool, 0);
@@ -180,7 +180,7 @@ static int request_microcode(const char
 	return error;
 }
 
-static int __init microcode_init(void)
+static int __init _microcode_init(void)
 {
 	const struct cpuinfo_x86 *c = &boot_cpu_data;
 	char buf[36];
@@ -218,9 +218,8 @@ static int __init microcode_init(void)
 		return error;
 	}
 
-	pr_info("Microcode Update Driver: v" MICROCODE_VERSION
-		" <tigran@aivazian.fsnet.co.uk>, Peter Oruba\n");
+	pr_info("Microcode Update Driver: v" DRIVER_VERSION ".\n");
 
 	return 0;
 }
-late_initcall(microcode_init);
+late_initcall(_microcode_init);
--- a/arch/x86/kernel/head_32-xen.S
+++ b/arch/x86/kernel/head_32-xen.S
@@ -32,13 +32,21 @@
 #define X86_CAPABILITY	new_cpu_data+CPUINFO_x86_capability
 #define X86_VENDOR_ID	new_cpu_data+CPUINFO_x86_vendor_id
 
+#define SIZEOF_PTREGS 17*4
+
 __HEAD
 ENTRY(startup_32)
 	movl %esi,xen_start_info
 	cld
 
-	/* Set up the stack pointer */
-	movl $(init_thread_union+THREAD_SIZE),%esp
+	/*
+	 * Set up the stack pointer.
+	 *
+	 * The SIZEOF_PTREGS gap is a convention which helps the in-kernel
+	 * unwinder reliably detect the end of the stack.
+	 */
+	movl $init_thread_union + THREAD_SIZE - SIZEOF_PTREGS - \
+	      TOP_OF_KERNEL_STACK_PADDING,%esp
 
 	/* get vendor info */
 	xorl %eax,%eax			# call CPUID with 0 -> return vendor ID
--- a/arch/x86/kernel/head_64-xen.S
+++ b/arch/x86/kernel/head_64-xen.S
@@ -20,6 +20,7 @@
 #include <asm/msr.h>
 #include <asm/cache.h>
 #include <asm/percpu.h>
+#include "../entry/calling.h"
 #include <asm/export.h>
 #include <xen/interface/xen.h>
 #include <xen/interface/arch-x86/xen-mca.h>
@@ -30,7 +31,11 @@
 	.code64
 	.globl startup_64
 startup_64:
-	movq $(init_thread_union+THREAD_SIZE-8),%rsp
+	/*
+	 * The SIZEOF_PTREGS gap is a convention which helps the in-kernel
+	 * unwinder reliably detect the end of the stack.
+	 */
+	movq $init_thread_union + THREAD_SIZE - SIZEOF_PTREGS,%rsp
 
 	/* rsi is pointer to startup info structure.
 	   pass it to C */
--- a/arch/x86/kernel/irq-xen.c
+++ b/arch/x86/kernel/irq-xen.c
@@ -14,7 +14,6 @@
 #include <asm/apic.h>
 #include <asm/io_apic.h>
 #include <asm/irq.h>
-#include <asm/idle.h>
 #include <asm/mce.h>
 #include <asm/hw_irq.h>
 #include <asm/desc.h>
--- a/arch/x86/kernel/process-xen.c
+++ b/arch/x86/kernel/process-xen.c
@@ -23,8 +23,7 @@
 #include <asm/cpu.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
-#include <asm/idle.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/mwait.h>
 #include <asm/fpu/internal.h>
 #include <asm/debugreg.h>
@@ -71,23 +70,6 @@ __visible DEFINE_PER_CPU(unsigned long,
 EXPORT_PER_CPU_SYMBOL(cpu_sp0);
 #endif
 
-#ifdef CONFIG_X86_64
-static DEFINE_PER_CPU(unsigned char, is_idle);
-static ATOMIC_NOTIFIER_HEAD(idle_notifier);
-
-void idle_notifier_register(struct notifier_block *n)
-{
-	atomic_notifier_chain_register(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_register);
-
-void idle_notifier_unregister(struct notifier_block *n)
-{
-	atomic_notifier_chain_unregister(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_unregister);
-#endif
-
 /*
  * this gets called so that we can store lazy state into memory and copy the
  * current task into the new thread.
@@ -245,39 +227,10 @@ static inline void play_dead(void)
 }
 #endif
 
-#ifdef CONFIG_X86_64
-void enter_idle(void)
-{
-	this_cpu_write(is_idle, 1);
-	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
-}
-
-static void __exit_idle(void)
-{
-	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
-		return;
-	atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
-}
-
-/* Called from interrupts to signify idle end */
-void exit_idle(void)
-{
-	/* idle loop has pid 0 */
-	if (current->pid)
-		return;
-	__exit_idle();
-}
-#endif
-
 void arch_cpu_idle_enter(void)
 {
+	tsc_verify_tsc_adjust(false);
 	local_touch_nmi();
-	enter_idle();
-}
-
-void arch_cpu_idle_exit(void)
-{
-	__exit_idle();
 }
 
 void arch_cpu_idle_dead(void)
@@ -331,59 +284,33 @@ void stop_this_cpu(void *dummy)
 }
 
 #ifndef CONFIG_XEN
-bool amd_e400_c1e_detected;
-EXPORT_SYMBOL(amd_e400_c1e_detected);
-
-static cpumask_var_t amd_e400_c1e_mask;
-
-void amd_e400_remove_cpu(int cpu)
-{
-	if (amd_e400_c1e_mask != NULL)
-		cpumask_clear_cpu(cpu, amd_e400_c1e_mask);
-}
-
 /*
- * AMD Erratum 400 aware idle routine. We check for C1E active in the interrupt
- * pending message MSR. If we detect C1E, then we handle it the same
- * way as C3 power states (local apic timer and TSC stop)
+ * AMD Erratum 400 aware idle routine. We handle it the same way as C3 power
+ * states (local apic timer and TSC stop).
  */
 static void amd_e400_idle(void)
 {
-	if (!amd_e400_c1e_detected) {
-		u32 lo, hi;
-
-		rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
-
-		if (lo & K8_INTP_C1E_ACTIVE_MASK) {
-			amd_e400_c1e_detected = true;
-			if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
-				mark_tsc_unstable("TSC halt in AMD C1E");
-			pr_info("System has AMD C1E enabled\n");
-		}
+	/*
+	 * We cannot use static_cpu_has_bug() here because X86_BUG_AMD_APIC_C1E
+	 * gets set after static_cpu_has() places have been converted via
+	 * alternatives.
+	 */
+	if (!boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {
+		default_idle();
+		return;
 	}
 
-	if (amd_e400_c1e_detected) {
-		int cpu = smp_processor_id();
-
-		if (!cpumask_test_cpu(cpu, amd_e400_c1e_mask)) {
-			cpumask_set_cpu(cpu, amd_e400_c1e_mask);
-			/* Force broadcast so ACPI can not interfere. */
-			tick_broadcast_force();
-			pr_info("Switch to broadcast mode on CPU%d\n", cpu);
-		}
-		tick_broadcast_enter();
+	tick_broadcast_enter();
 
-		default_idle();
+	default_idle();
 
-		/*
-		 * The switch back from broadcast mode needs to be
-		 * called with interrupts disabled.
-		 */
-		local_irq_disable();
-		tick_broadcast_exit();
-		local_irq_enable();
-	} else
-		default_idle();
+	/*
+	 * The switch back from broadcast mode needs to be called with
+	 * interrupts disabled.
+	 */
+	local_irq_disable();
+	tick_broadcast_exit();
+	local_irq_enable();
 }
 
 /*
@@ -445,8 +372,7 @@ void select_idle_routine(const struct cp
 	if (x86_idle || boot_option_idle_override == IDLE_POLL)
 		return;
 
-	if (cpu_has_bug(c, X86_BUG_AMD_APIC_C1E)) {
-		/* E400: APIC timer interrupt does not wake up CPU from C1e */
+	if (boot_cpu_has_bug(X86_BUG_AMD_E400)) {
 		pr_info("using AMD E400 aware idle routine\n");
 		x86_idle = amd_e400_idle;
 	} else if (prefer_mwait_c1_over_halt(c)) {
@@ -457,14 +383,40 @@ void select_idle_routine(const struct cp
 #endif
 }
 
-void __init init_amd_e400_c1e_mask(void)
-{
 #ifndef CONFIG_XEN
-	/* If we're using amd_e400_idle, we need to allocate amd_e400_c1e_mask. */
-	if (x86_idle == amd_e400_idle)
-		zalloc_cpumask_var(&amd_e400_c1e_mask, GFP_KERNEL);
-#endif
+void amd_e400_c1e_apic_setup(void)
+{
+	if (boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {
+		pr_info("Switch to broadcast mode on CPU%d\n", smp_processor_id());
+		local_irq_disable();
+		tick_broadcast_force();
+		local_irq_enable();
+	}
+}
+
+void __init arch_post_acpi_subsys_init(void)
+{
+	u32 lo, hi;
+
+	if (!boot_cpu_has_bug(X86_BUG_AMD_E400))
+		return;
+
+	/*
+	 * AMD E400 detection needs to happen after ACPI has been enabled. If
+	 * the machine is affected K8_INTP_C1E_ACTIVE_MASK bits are set in
+	 * MSR_K8_INT_PENDING_MSG.
+	 */
+	rdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);
+	if (!(lo & K8_INTP_C1E_ACTIVE_MASK))
+		return;
+
+	boot_cpu_set_bug(X86_BUG_AMD_APIC_C1E);
+
+	if (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))
+		mark_tsc_unstable("TSC halt in AMD C1E");
+	pr_info("System has AMD C1E enabled\n");
 }
+#endif
 
 static int __init idle_setup(char *str)
 {
--- a/arch/x86/kernel/process_32-xen.c
+++ b/arch/x86/kernel/process_32-xen.c
@@ -51,11 +51,11 @@
 
 #include <asm/tlbflush.h>
 #include <asm/cpu.h>
-#include <asm/idle.h>
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
 #include <asm/switch_to.h>
 #include <asm/vm86.h>
+#include <asm/intel_rdt.h>
 
 void __show_regs(struct pt_regs *regs, int all)
 {
@@ -74,10 +74,9 @@ void __show_regs(struct pt_regs *regs, i
 		savesegment(gs, gs);
 	}
 
-	printk(KERN_DEFAULT "EIP: %04x:[<%08lx>] EFLAGS: %08lx CPU: %d\n",
-			(u16)regs->cs, regs->ip, regs->flags,
-			smp_processor_id());
-	print_symbol("EIP is at %s\n", regs->ip);
+	printk(KERN_DEFAULT "EIP: %pS\n", (void *)regs->ip);
+	printk(KERN_DEFAULT "EFLAGS: %08lx CPU: %d\n", regs->flags,
+		smp_processor_id());
 
 	printk(KERN_DEFAULT "EAX: %08lx EBX: %08lx ECX: %08lx EDX: %08lx\n",
 		regs->ax, regs->bx, regs->cx, regs->dx);
@@ -233,11 +232,10 @@ __switch_to(struct task_struct *prev_p,
 			     *next = &next_p->thread;
 	struct fpu *prev_fpu = &prev->fpu;
 	struct fpu *next_fpu = &next->fpu;
-	int cpu = smp_processor_id(), cr0_ts;
+	int cpu = smp_processor_id();
 #ifndef CONFIG_X86_NO_TSS
 	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
 #endif
-	fpu_switch_t fpu_switch;
 #if CONFIG_XEN_COMPAT > 0x030002
 	struct physdev_set_iopl iopl_op;
 	struct physdev_set_iobitmap iobmp_op;
@@ -250,7 +248,7 @@ __switch_to(struct task_struct *prev_p,
 
 	/* XEN NOTE: FS/GS saved in switch_mm(), not here. */
 
-	fpu_switch = xen_switch_fpu_prepare(prev_fpu, next_fpu, cpu, &mcl);
+	switch_fpu_prepare(prev_fpu, cpu);
 
 	/*
 	 * Reload sp0.
@@ -312,20 +310,8 @@ __switch_to(struct task_struct *prev_p,
 	BUG_ON(pdo > _pdo + ARRAY_SIZE(_pdo));
 #endif
 	BUG_ON(mcl > _mcl + ARRAY_SIZE(_mcl));
-	if (_mcl->op == __HYPERVISOR_fpu_taskswitch) {
-		__this_cpu_write(xen_x86_cr0_upd, X86_CR0_TS);
-		cr0_ts = _mcl->args[0] ? 1 : -1;
-	} else
-		cr0_ts = 0;
 	if (unlikely(HYPERVISOR_multicall_check(_mcl, mcl - _mcl, NULL)))
 		BUG();
-	if (cr0_ts) {
-		if (cr0_ts > 0)
-			__this_cpu_or(xen_x86_cr0, X86_CR0_TS);
-		else
-			__this_cpu_and(xen_x86_cr0, ~X86_CR0_TS);
-		xen_clear_cr0_upd();
-	}
 
 	/*
 	 * Now maybe handle debug registers
@@ -357,9 +343,12 @@ __switch_to(struct task_struct *prev_p,
 	if (prev->gs | next->gs)
 		lazy_load_gs(next->gs);
 
-	switch_fpu_finish(next_fpu, fpu_switch);
+	switch_fpu_finish(next_fpu, cpu);
 
 	this_cpu_write(current_task, next_p);
 
+	/* Load the Intel cache allocation PQR MSR. */
+	intel_rdt_sched_in();
+
 	return prev_p;
 }
--- a/arch/x86/kernel/process_64-xen.c
+++ b/arch/x86/kernel/process_64-xen.c
@@ -49,12 +49,12 @@
 #include <asm/proto.h>
 #include <asm/hardirq.h>
 #include <asm/ia32.h>
-#include <asm/idle.h>
 #include <asm/syscalls.h>
 #include <asm/debugreg.h>
 #include <asm/switch_to.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
+#include <asm/intel_rdt.h>
 
 /* Prints also some state that isn't saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)
@@ -64,10 +64,15 @@ void __show_regs(struct pt_regs *regs, i
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
-	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
-	printk_address(regs->ip);
-	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
-			regs->sp, regs->flags);
+	printk(KERN_DEFAULT "RIP: %04lx:%pS\n", regs->cs & 0xffff,
+		(void *)regs->ip);
+	printk(KERN_DEFAULT "RSP: %04lx:%016lx EFLAGS: %08lx", regs->ss,
+		regs->sp, regs->flags);
+	if (regs->orig_ax != -1)
+		pr_cont(" ORIG_RAX: %016lx\n", regs->orig_ax);
+	else
+		pr_cont("\n");
+
 	printk(KERN_DEFAULT "RAX: %016lx RBX: %016lx RCX: %016lx\n",
 	       regs->ax, regs->bx, regs->cx);
 	printk(KERN_DEFAULT "RDX: %016lx RSI: %016lx RDI: %016lx\n",
@@ -273,11 +278,10 @@ __switch_to(struct task_struct *prev_p,
 	struct thread_struct *next = &next_p->thread;
 	struct fpu *prev_fpu = &prev->fpu;
 	struct fpu *next_fpu = &next->fpu;
-	int cpu = smp_processor_id(), cr0_ts;
+	int cpu = smp_processor_id();
 #ifndef CONFIG_X86_NO_TSS
 	struct tss_struct *tss = &per_cpu(cpu_tss, cpu);
 #endif
-	fpu_switch_t fpu_switch;
 #if CONFIG_XEN_COMPAT > 0x030002
 	struct physdev_set_iopl iopl_op;
 	struct physdev_set_iobitmap iobmp_op;
@@ -288,7 +292,7 @@ __switch_to(struct task_struct *prev_p,
 #endif
 	multicall_entry_t _mcl[8], *mcl = _mcl;
 
-	fpu_switch = xen_switch_fpu_prepare(prev_fpu, next_fpu, cpu, &mcl);
+	switch_fpu_prepare(prev_fpu, cpu);
 
 	/* Reload sp0. This is load_sp0(tss, next) with a multicall. */
 	mcl->op      = __HYPERVISOR_stack_switch;
@@ -348,20 +352,8 @@ __switch_to(struct task_struct *prev_p,
 	BUG_ON(pdo > _pdo + ARRAY_SIZE(_pdo));
 #endif
 	BUG_ON(mcl > _mcl + ARRAY_SIZE(_mcl));
-	if (_mcl->op == __HYPERVISOR_fpu_taskswitch) {
-		__this_cpu_write(xen_x86_cr0_upd, X86_CR0_TS);
-		cr0_ts = _mcl->args[0] ? 1 : -1;
-	} else
-		cr0_ts = 0;
 	if (unlikely(HYPERVISOR_multicall_check(_mcl, mcl - _mcl, NULL)))
 		BUG();
-	if (cr0_ts) {
-		if (cr0_ts > 0)
-			__this_cpu_or(xen_x86_cr0, X86_CR0_TS);
-		else
-			__this_cpu_and(xen_x86_cr0, ~X86_CR0_TS);
-		xen_clear_cr0_upd();
-	}
 
 	/* Switch DS and ES.
 	 *
@@ -469,7 +461,7 @@ __switch_to(struct task_struct *prev_p,
 		}
 	}
 
-	switch_fpu_finish(next_fpu, fpu_switch);
+	switch_fpu_finish(next_fpu, cpu);
 
 	/*
 	 * Switch the PDA context.
@@ -517,6 +509,9 @@ __switch_to(struct task_struct *prev_p,
 	}
 #endif
 
+	/* Load the Intel cache allocation PQR MSR. */
+	intel_rdt_sched_in();
+
 	return prev_p;
 }
 
--- a/arch/x86/kernel/setup-xen.c
+++ b/arch/x86/kernel/setup-xen.c
@@ -1106,6 +1106,30 @@ void __init setup_arch(char **cmdline_p)
 
 	parse_early_param();
 
+#ifdef CONFIG_MEMORY_HOTPLUG
+	/*
+	 * Memory used by the kernel cannot be hot-removed because Linux
+	 * cannot migrate the kernel pages. When memory hotplug is
+	 * enabled, we should prevent memblock from allocating memory
+	 * for the kernel.
+	 *
+	 * ACPI SRAT records all hotpluggable memory ranges. But before
+	 * SRAT is parsed, we don't know about it.
+	 *
+	 * The kernel image is loaded into memory at very early time. We
+	 * cannot prevent this anyway. So on NUMA system, we set any
+	 * node the kernel resides in as un-hotpluggable.
+	 *
+	 * Since on modern servers, one node could have double-digit
+	 * gigabytes memory, we can assume the memory around the kernel
+	 * image is also un-hotpluggable. So before SRAT is parsed, just
+	 * allocate memory near the kernel image to try the best to keep
+	 * the kernel away from hotpluggable memory.
+	 */
+	if (movable_node_is_enabled())
+		memblock_set_bottom_up(true);
+#endif
+
 	x86_report_nx();
 
 	/* after early param, so could get panic from serial */
--- a/arch/x86/kernel/time-xen.c
+++ b/arch/x86/kernel/time-xen.c
@@ -391,8 +391,7 @@ static void get_runstate_snapshot(struct
 unsigned long long sched_clock(void)
 {
 	struct vcpu_runstate_info runstate;
-	cycle_t now;
-	u64 ret;
+	u64 now, ret;
 	s64 offset;
 
 	/*
@@ -506,8 +505,7 @@ static irqreturn_t timer_interrupt(int i
 	write_sequnlock(&xtime_lock);
 
 	if (shadow_tv_version != HYPERVISOR_shared_info->wc_version
-	    && !is_initial_xendomain() && !independent_wallclock
-	    && keventd_up())
+	    && !is_initial_xendomain() && !independent_wallclock)
 		schedule_work(&update_wallclock_work);
 
 	/*
@@ -583,13 +581,13 @@ void mark_tsc_unstable(char *reason)
 }
 EXPORT_SYMBOL_GPL(mark_tsc_unstable);
 
-static cycle_t cs_last;
+static u64 cs_last;
 
-static cycle_t xen_clocksource_read(struct clocksource *cs)
+static u64 xen_clocksource_read(struct clocksource *cs)
 {
 #ifdef CONFIG_SMP
-	cycle_t last = get_64bit(&cs_last);
-	cycle_t ret = local_clock();
+	u64 last = get_64bit(&cs_last);
+	u64 ret = local_clock();
 
 	if (unlikely((s64)(ret - last) < 0)) {
 		if (last - ret > permitted_clock_jitter
@@ -608,7 +606,7 @@ static cycle_t xen_clocksource_read(stru
 	}
 
 	for (;;) {
-		cycle_t cur = cmpxchg64(&cs_last, last, ret);
+		u64 cur = cmpxchg64(&cs_last, last, ret);
 
 		if (cur == last || (s64)(ret - cur) < 0)
 			return ret;
--- a/arch/x86/kernel/tracepoint-xen.c
+++ b/arch/x86/kernel/tracepoint-xen.c
@@ -28,7 +28,7 @@ static void switch_trap_table(void *arg)
 	local_irq_restore(flags);
 }
 
-void trace_irq_vector_regfunc(void)
+int trace_irq_vector_regfunc(void)
 {
 	mutex_lock(&trap_table_mutex);
 	if (!trace_trap_table_refcount) {
@@ -38,6 +38,7 @@ void trace_irq_vector_regfunc(void)
 	}
 	trace_trap_table_refcount++;
 	mutex_unlock(&trap_table_mutex);
+	return 0;
 }
 
 void trace_irq_vector_unregfunc(void)
--- a/arch/x86/kernel/traps-xen.c
+++ b/arch/x86/kernel/traps-xen.c
@@ -861,6 +861,8 @@ do_spurious_interrupt_bug(struct pt_regs
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
+	unsigned long cr0;
+
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
 #ifdef CONFIG_MATH_EMULATION
@@ -876,10 +878,20 @@ do_device_not_available(struct pt_regs *
 #endif
 	/* NB. 'clts' is done for us by Xen during virtual trap. */
 	__this_cpu_and(xen_x86_cr0, ~X86_CR0_TS);
-	fpu__restore(&current->thread.fpu); /* interrupts still off */
-#ifdef CONFIG_X86_32
-	cond_local_irq_enable(regs);
-#endif
+
+	/* This should not happen. */
+	cr0 = read_cr0();
+	if (WARN(cr0 & X86_CR0_TS, "CR0.TS was set")) {
+		/* Try to fix it up and carry on. */
+		write_cr0(cr0 & ~X86_CR0_TS);
+	} else {
+		/*
+		 * Something terrible happened, and we're better off trying
+		 * to kill the task than getting stuck in a never-ending
+		 * loop of #NM faults.
+		 */
+		die("unexpected #NM exception", regs, error_code);
+	}
 }
 NOKPROBE_SYMBOL(do_device_not_available);
 
--- a/arch/x86/kernel/x86_init-xen.c
+++ b/arch/x86/kernel/x86_init-xen.c
@@ -85,14 +85,12 @@ struct x86_init_ops x86_init __initdata
 	},
 };
 
-static int default_i8042_detect(void) { return 1; };
 
 struct x86_platform_ops x86_platform __ro_after_init = {
 	.get_wallclock			= xen_read_wallclock,
 	.set_wallclock			= xen_write_wallclock,
 	.is_untracked_pat_range		= is_ISA_range,
 	.get_nmi_reason			= xen_get_nmi_reason,
-	.i8042_detect			= default_i8042_detect
 };
 
 EXPORT_SYMBOL_GPL(x86_platform);
--- a/arch/x86/mm/dump_pagetables-xen.c
+++ b/arch/x86/mm/dump_pagetables-xen.c
@@ -15,6 +15,7 @@
 #include <linux/debugfs.h>
 #include <linux/mm.h>
 #include <linux/init.h>
+#include <linux/sched.h>
 #include <linux/seq_file.h>
 
 #include <xen/interface/xen.h>
@@ -421,6 +422,7 @@ static void ptdump_walk_pgd_level_core(s
 		} else
 			note_page(m, &st, __pgprot(0), 1);
 
+		cond_resched();
 		start++;
 	}
 
--- a/arch/x86/mm/fault-xen.c
+++ b/arch/x86/mm/fault-xen.c
@@ -421,7 +421,7 @@ out:
 
 void vmalloc_sync_all(void)
 {
-	sync_global_pgds(VMALLOC_START & PGDIR_MASK, VMALLOC_END, 0);
+	sync_global_pgds(VMALLOC_START & PGDIR_MASK, VMALLOC_END);
 }
 
 /*
@@ -688,8 +688,7 @@ show_fault_oops(struct pt_regs *regs, un
 		printk(KERN_CONT "paging request");
 
 	printk(KERN_CONT " at %p\n", (void *) address);
-	printk(KERN_ALERT "IP:");
-	printk_address(regs->ip);
+	printk(KERN_ALERT "IP: %pS\n", (void *)regs->ip);
 
 	dump_pagetable(address);
 }
--- a/arch/x86/mm/init_32-xen.c
+++ b/arch/x86/mm/init_32-xen.c
@@ -36,7 +36,7 @@
 #include <asm/asm.h>
 #include <asm/bios_ebda.h>
 #include <asm/processor.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/dma.h>
 #include <asm/fixmap.h>
--- a/arch/x86/mm/init_64-xen.c
+++ b/arch/x86/mm/init_64-xen.c
@@ -39,7 +39,7 @@
 
 #include <asm/processor.h>
 #include <asm/bios_ebda.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/dma.h>
@@ -153,10 +153,10 @@ static int __init nonx32_setup(char *str
 __setup("noexec32=", nonx32_setup);
 
 /*
- * When memory was added/removed make sure all the processes MM have
+ * When memory was added make sure all the processes MM have
  * suitable PGD entries in the local PGD level page.
  */
-void sync_global_pgds(unsigned long start, unsigned long end, int removed)
+void sync_global_pgds(unsigned long start, unsigned long end)
 {
 	unsigned long address;
 
@@ -164,12 +164,7 @@ void sync_global_pgds(unsigned long star
 		const pgd_t *pgd_ref = pgd_offset_k(address);
 		struct page *page;
 
-		/*
-		 * When it is called after memory hot remove, pgd_none()
-		 * returns true. In this case (removed == 1), we must clear
-		 * the PGD entries in the local PGD level page.
-		 */
-		if (pgd_none(*pgd_ref) && !removed)
+		if (pgd_none(*pgd_ref))
 			continue;
 
 		spin_lock(&pgd_lock);
@@ -186,13 +181,8 @@ void sync_global_pgds(unsigned long star
 				BUG_ON(pgd_page_vaddr(*pgd)
 				       != pgd_page_vaddr(*pgd_ref));
 
-			if (removed) {
-				if (pgd_none(*pgd_ref) && !pgd_none(*pgd))
-					pgd_clear(pgd);
-			} else {
-				if (pgd_none(*pgd))
-					set_pgd(pgd, *pgd_ref);
-			}
+			if (pgd_none(*pgd))
+				set_pgd(pgd, *pgd_ref);
 
 			spin_unlock(pgt_lock);
 		}
@@ -906,7 +896,7 @@ kernel_physical_mapping_init(unsigned lo
 	}
 
 	if (pgd_changed)
-		sync_global_pgds(vaddr_start, vaddr_end - 1, 0);
+		sync_global_pgds(vaddr_start, vaddr_end - 1);
 
 	return paddr_last;
 }
@@ -1588,7 +1578,7 @@ int __meminit vmemmap_populate(unsigned
 	} else
 		err = vmemmap_populate_basepages(start, end, node);
 	if (!err)
-		sync_global_pgds(start, end - 1, 0);
+		sync_global_pgds(start, end - 1);
 	return err;
 }
 
--- a/arch/x86/mm/pageattr-xen.c
+++ b/arch/x86/mm/pageattr-xen.c
@@ -20,7 +20,7 @@
 #include <asm/tlbflush.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/pgalloc.h>
 #include <asm/proto.h>
 #include <asm/pat.h>
--- a/arch/x86/mm/pat-xen.c
+++ b/arch/x86/mm/pat-xen.c
@@ -980,20 +980,17 @@ int track_pfn_remap(struct vm_area_struc
 	return 0;
 }
 
-int track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot,
-		     pfn_t pfn)
+void track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot, pfn_t pfn)
 {
 	enum page_cache_mode pcm;
 
 	if (!pat_enabled())
-		return 0;
+		return;
 
 	/* Set prot based on lookup */
 	pcm = lookup_memtype(pfn_t_to_phys(pfn));
 	*prot = __pgprot((pgprot_val(*prot) & (~_PAGE_CACHE_MASK)) |
 			 cachemode2protval(pcm));
-
-	return 0;
 }
 
 /*
--- a/arch/x86/pci/amd_bus.c
+++ b/arch/x86/pci/amd_bus.c
@@ -385,7 +385,7 @@ static int __init pci_io_ecs_init(void)
 				amd_bus_cpu_online, NULL);
 	WARN_ON(ret < 0);
 #else
-	if (cpu = 1, cpu) {
+	if (ret = 0, !ret) {
 		u64 reg;
 		rdmsrl(MSR_AMD64_NB_CFG, reg);
 		if (!(reg & ENABLE_CF8_EXT_CFG))
--- a/drivers/acpi/processor_perflib.c
+++ b/drivers/acpi/processor_perflib.c
@@ -194,8 +194,8 @@ void acpi_processor_ppc_has_changed(stru
 #ifdef CONFIG_CPU_FREQ
 		cpufreq_update_policy(pr->id);
 #elif defined(CONFIG_PROCESSOR_EXTERNAL_CONTROL)
-		return processor_notify_external(pr,
-				PROCESSOR_PM_CHANGE, PM_TYPE_PERF);
+		processor_notify_external(pr, PROCESSOR_PM_CHANGE,
+					  PM_TYPE_PERF);
 #endif
 }
 
--- a/drivers/firmware/efi/apple-properties.c
+++ b/drivers/firmware/efi/apple-properties.c
@@ -185,16 +185,19 @@ skip_device:
 
 static int __init map_properties(void)
 {
+#ifndef CONFIG_XEN
 	struct properties_header *properties;
 	struct setup_data *data;
 	u32 data_len;
 	u64 pa_data;
 	int ret;
+#endif
 
 	if (!dmi_match(DMI_SYS_VENDOR, "Apple Inc.") &&
 	    !dmi_match(DMI_SYS_VENDOR, "Apple Computer, Inc."))
 		return 0;
 
+#if !defined(CONFIG_XEN)
 	pa_data = boot_params.hdr.setup_data;
 	while (pa_data) {
 		data = ioremap(pa_data, sizeof(*data));
@@ -242,6 +245,10 @@ static int __init map_properties(void)
 
 		return ret;
 	}
+#elif defined(CONFIG_EFI)
+	//todo (new hypercall sub-op needed)
+#endif
+
 	return 0;
 }
 
--- a/drivers/hwmon/coretemp-xen.c
+++ b/drivers/hwmon/coretemp-xen.c
@@ -52,6 +52,7 @@ static int force_tjmax;
 module_param_named(tjmax, force_tjmax, int, 0444);
 MODULE_PARM_DESC(tjmax, "TjMax value in degrees Celsius");
 
+#define PKG_SYSFS_ATTR_NO	1	/* Sysfs attribute for package temp */
 #define BASE_SYSFS_ATTR_NO	2	/* Sysfs Base attr no for coretemp */
 #define NUM_REAL_CORES		128	/* Number of Real cores per cpu */
 #define CORETEMP_NAME_LENGTH	19	/* String Length of attrs */
@@ -98,11 +99,11 @@ struct temp_data {
 
 /* Platform Data per Physical CPU */
 struct platform_data {
-	struct device *hwmon_dev;
-	u16 phys_proc_id;
-	u8 x86_model, x86_mask:4;
-	u8 managed:1;
-	struct temp_data *core_data[MAX_CORE_DATA];
+	struct device		*hwmon_dev;
+	u16			phys_proc_id;
+	u8			x86_model, x86_mask:4;
+	u8			managed:1;
+	struct temp_data	*core_data[MAX_CORE_DATA];
 	struct device_attribute name_attr;
 };
 
@@ -142,8 +143,13 @@ static ssize_t show_crit_alarm(struct de
 	struct sensor_device_attribute *attr = to_sensor_dev_attr(devattr);
 	struct platform_data *pdata = dev_get_drvdata(dev);
 	struct temp_data *tdata = pdata->core_data[attr->index];
+	int rc;
+
+	mutex_lock(&tdata->update_lock);
+	rc = rdmsr_safe_on_pcpu(tdata->cpu, tdata->status_reg, &eax, &edx);
+	mutex_unlock(&tdata->update_lock);
 
-	if (rdmsr_safe_on_pcpu(tdata->cpu, tdata->status_reg, &eax, &edx) < 0)
+	if (rc < 0)
 		return sprintf(buf, "\n");
 
 	return sprintf(buf, "%d\n", (eax >> 5) & 1);
@@ -491,7 +497,7 @@ static int create_core_data(struct platf
 	 * The attr number is always core id + 2
 	 * The Pkgtemp will always show up as temp1_*, if available
 	 */
-	attr_no = pkg_flag ? 1 : CORE_ATTR_NO(c->cpu_core_id);
+	attr_no = pkg_flag ? PKG_SYSFS_ATTR_NO : CORE_ATTR_NO(c->cpu_core_id);
 
 	if (attr_no > MAX_CORE_DATA - 1)
 		return -ERANGE;
@@ -682,7 +688,7 @@ static void coretemp_device_remove(unsig
 	mutex_unlock(&pdev_list_mutex);
 }
 
-static bool is_any_core_online(struct platform_data *pdata)
+static int get_online_core_in_package(struct platform_data *pdata)
 {
 	int i;
 
@@ -690,10 +696,10 @@ static bool is_any_core_online(struct pl
 	for (i = MAX_CORE_DATA - 1; i >= 0; --i) {
 		if (pdata->core_data[i] &&
 			!pdata->core_data[i]->is_pkg_data) {
-			return true;
+			return pdata->core_data[i]->cpu;
 		}
 	}
-	return false;
+	return nr_cpu_ids;
 }
 
 static void get_cpuid_info(void *arg)
@@ -782,9 +788,10 @@ static void get_core_online(unsigned int
 
 static void put_core_offline(unsigned int cpu)
 {
-	int i, indx;
-	struct platform_data *pdata;
 	struct platform_device *pdev = coretemp_get_pdev(cpu);
+	struct platform_data *pdata;
+	struct temp_data *tdata;
+	int i, indx, target;
 	u32 cpu_core_id, phys_proc_id;
 
 	/* If the physical CPU device does not exist, just return */
@@ -837,8 +844,21 @@ static void put_core_offline(unsigned in
 	 * which in turn calls coretemp_remove. This removes the
 	 * pkgtemp entry and does other clean ups.
 	 */
-	if (!is_any_core_online(pdata))
+	target = get_online_core_in_package(pdata);
+	if (target >= nr_cpu_ids) {
 		coretemp_device_remove(cpu);
+		return;
+	}
+	/*
+	 * Check whether this core is the target for the package
+	 * interface. We need to assign it to some other cpu.
+	 */
+	tdata = pdata->core_data[PKG_SYSFS_ATTR_NO];
+	if (tdata && tdata->cpu == cpu) {
+		mutex_lock(&tdata->update_lock);
+		tdata->cpu = target;
+		mutex_unlock(&tdata->update_lock);
+	}
 }
 
 static int coretemp_cpu_callback(struct notifier_block *nfb,
--- a/drivers/pci/msi-xen.c
+++ b/drivers/pci/msi-xen.c
@@ -543,14 +543,15 @@ static void cleanup_msi_sysfs(struct pci
  * an error, and a positive return value indicates the number of interrupts
  * which could have been allocated.
  */
-static int msi_capability_init(struct pci_dev *dev, int nvec, bool affinity)
+static int msi_capability_init(struct pci_dev *dev, int nvec,
+			       const struct irq_affinity *affd)
 {
 	struct msi_dev_list *dev_entry = get_msi_dev_pirq_list(dev);
 	int pirq;
 	struct cpumask *masks = NULL;
 
-	if (affinity) {
-		masks = irq_create_affinity_masks(dev->irq_affinity, nvec);
+	if (affd) {
+		masks = irq_create_affinity_masks(nvec, affd);
 		if (!masks)
 			pr_err("Unable to allocate affinity masks, ignoring\n");
 	}
@@ -579,14 +580,14 @@ static int msi_capability_init(struct pc
  * @dev: pointer to the pci_dev data structure of MSI-X device function
  * @entries: pointer to an array of struct msix_entry entries
  * @nvec: number of @entries
- * @affinity: flag to indicate cpu irq affinity mask should be set
+ * @affd: Optional pointer to enable automatic affinity assignement
  *
  * Setup the MSI-X capability structure of device function with a
  * single MSI-X irq. A return of zero indicates the successful setup of
  * requested MSI-X entries with allocated irqs or non-zero for otherwise.
  **/
 static int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,
-				int nvec, bool affinity)
+				int nvec, const struct irq_affinity *affd)
 {
 	u64 table_base;
 	int pirq, i, j, mapped;
@@ -604,8 +605,8 @@ static int msix_capability_init(struct p
 	if (!table_base)
 		return -ENODEV;
 
-	if (affinity) {
-		masks = irq_create_affinity_masks(dev->irq_affinity, nvec);
+	if (affd) {
+		masks = irq_create_affinity_masks(nvec, affd);
 		if (!masks)
 			pr_err("Unable to allocate affinity masks, ignoring\n");
 	}
@@ -809,7 +810,7 @@ int pci_msix_vec_count(struct pci_dev *d
 EXPORT_SYMBOL(pci_msix_vec_count);
 
 static int __pci_enable_msix(struct pci_dev *dev, struct msix_entry *entries,
-			     int nvec, bool affinity)
+			     int nvec, const struct irq_affinity *affd)
 {
 	int status, nr_entries;
 	int i, j, temp;
@@ -833,9 +834,8 @@ static int __pci_enable_msix(struct pci_
 				entries[i].entry = i;
 		}
 
-		if (affinity) {
-			masks = irq_create_affinity_masks(dev->irq_affinity,
-							  nvec);
+		if (affd) {
+			masks = irq_create_affinity_masks(nvec, affd);
 			if (!masks)
 				pr_err("Unable to allocate affinity masks, ignoring\n");
 		}
@@ -907,7 +907,7 @@ static int __pci_enable_msix(struct pci_
 		return -EINVAL;
 	}
 
-	status = msix_capability_init(dev, entries, nvec, affinity);
+	status = msix_capability_init(dev, entries, nvec, affd);
 
 	if ( !status )
 		msi_dev_entry->default_irq = temp;
@@ -932,7 +932,7 @@ static int __pci_enable_msix(struct pci_
  **/
 int pci_enable_msix(struct pci_dev *dev, struct msix_entry *entries, int nvec)
 {
-	return __pci_enable_msix(dev, entries, nvec, false);
+	return __pci_enable_msix(dev, entries, nvec, NULL);
 }
 EXPORT_SYMBOL(pci_enable_msix);
 
@@ -1008,9 +1008,8 @@ int pci_msi_enabled(void)
 EXPORT_SYMBOL(pci_msi_enabled);
 
 static int __pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec,
-		unsigned int flags)
+				  const struct irq_affinity *affd)
 {
-	bool affinity = flags & PCI_IRQ_AFFINITY;
 	int nvec, temp;
 	int rc;
 	struct msi_dev_list *msi_dev_entry = get_msi_dev_pirq_list(dev);
@@ -1042,15 +1041,14 @@ static int __pci_enable_msi_range(struct
 	temp = dev->irq;
 
 	for (;;) {
-		if (affinity) {
-			nvec = irq_calc_affinity_vectors(dev->irq_affinity,
-					nvec);
+		if (affd) {
+			nvec = irq_calc_affinity_vectors(nvec, affd);
 			if (nvec < minvec)
 				return -ENOSPC;
 		}
 
 		if (is_initial_xendomain())
-			rc = msi_capability_init(dev, nvec, affinity);
+			rc = msi_capability_init(dev, nvec, affd);
 		else
 #ifdef CONFIG_XEN_PCIDEV_FRONTEND
 			rc = pci_frontend_enable_msi(dev, nvec);
@@ -1096,29 +1094,27 @@ static int __pci_enable_msi_range(struct
  **/
 int pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec)
 {
-	return __pci_enable_msi_range(dev, minvec, maxvec, 0);
+	return __pci_enable_msi_range(dev, minvec, maxvec, NULL);
 }
 EXPORT_SYMBOL(pci_enable_msi_range);
 
 static int __pci_enable_msix_range(struct pci_dev *dev,
-		struct msix_entry *entries, int minvec, int maxvec,
-		unsigned int flags)
+				   struct msix_entry *entries, int minvec,
+				   int maxvec, const struct irq_affinity *affd)
 {
-	bool affinity = flags & PCI_IRQ_AFFINITY;
 	int rc, nvec = maxvec;
 
 	if (maxvec < minvec)
 		return -ERANGE;
 
 	for (;;) {
-		if (affinity) {
-			nvec = irq_calc_affinity_vectors(dev->irq_affinity,
-					nvec);
+		if (affd) {
+			nvec = irq_calc_affinity_vectors(nvec, affd);
 			if (nvec < minvec)
 				return -ENOSPC;
 		}
 
-		rc = __pci_enable_msix(dev, entries, nvec, affinity);
+		rc = __pci_enable_msix(dev, entries, nvec, affd);
 		if (rc == 0)
 			return nvec;
 
@@ -1149,16 +1145,17 @@ static int __pci_enable_msix_range(struc
 int pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries,
 		int minvec, int maxvec)
 {
-	return __pci_enable_msix_range(dev, entries, minvec, maxvec, 0);
+	return __pci_enable_msix_range(dev, entries, minvec, maxvec, NULL);
 }
 EXPORT_SYMBOL(pci_enable_msix_range);
 
 /**
- * pci_alloc_irq_vectors - allocate multiple IRQs for a device
+ * pci_alloc_irq_vectors_affinity - allocate multiple IRQs for a device
  * @dev:		PCI device to operate on
  * @min_vecs:		minimum number of vectors required (must be >= 1)
  * @max_vecs:		maximum (desired) number of vectors
  * @flags:		flags or quirks for the allocation
+ * @affd:		optional description of the affinity requirements
  *
  * Allocate up to @max_vecs interrupt vectors for @dev, using MSI-X or MSI
  * vectors if available, and fall back to a single legacy vector
@@ -1170,20 +1167,40 @@ EXPORT_SYMBOL(pci_enable_msix_range);
  * To get the Linux IRQ number used for a vector that can be passed to
  * request_irq() use the pci_irq_vector() helper.
  */
-int pci_alloc_irq_vectors(struct pci_dev *dev, unsigned int min_vecs,
-		unsigned int max_vecs, unsigned int flags)
+int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
+				   unsigned int max_vecs, unsigned int flags,
+				   const struct irq_affinity *affd)
 {
+	static const struct irq_affinity msi_default_affd;
 	int vecs = -ENOSPC;
 
+	if (flags & PCI_IRQ_AFFINITY) {
+		if (!affd)
+			affd = &msi_default_affd;
+
+		if (affd->pre_vectors + affd->post_vectors > min_vecs)
+			return -EINVAL;
+
+		/*
+		 * If there aren't any vectors left after applying the pre/post
+		 * vectors don't bother with assigning affinity.
+		 */
+		if (affd->pre_vectors + affd->post_vectors == min_vecs)
+			affd = NULL;
+	} else {
+		if (WARN_ON(affd))
+			affd = NULL;
+	}
+
 	if (flags & PCI_IRQ_MSIX) {
 		vecs = __pci_enable_msix_range(dev, NULL, min_vecs, max_vecs,
-				flags);
+				affd);
 		if (vecs > 0)
 			return vecs;
 	}
 
 	if (flags & PCI_IRQ_MSI) {
-		vecs = __pci_enable_msi_range(dev, min_vecs, max_vecs, flags);
+		vecs = __pci_enable_msi_range(dev, min_vecs, max_vecs, affd);
 		if (vecs > 0)
 			return vecs;
 	}
@@ -1196,7 +1213,7 @@ int pci_alloc_irq_vectors(struct pci_dev
 
 	return vecs;
 }
-EXPORT_SYMBOL(pci_alloc_irq_vectors);
+EXPORT_SYMBOL(pci_alloc_irq_vectors_affinity);
 
 /**
  * pci_free_irq_vectors - free previously allocated IRQs for a device
--- a/drivers/xen/balloon/balloon.c
+++ b/drivers/xen/balloon/balloon.c
@@ -44,6 +44,7 @@
 #include <linux/slab.h>
 #include <linux/mutex.h>
 #include <linux/seq_file.h>
+#include <linux/uaccess.h>
 #include <xen/xen_proc.h>
 #include <asm/hypervisor.h>
 #include <xen/balloon.h>
@@ -52,7 +53,6 @@
 #include <asm/page.h>
 #include <asm/pgalloc.h>
 #include <asm/pgtable.h>
-#include <asm/uaccess.h>
 #include <asm/tlb.h>
 #include <xen/xenbus.h>
 #include "common.h"
--- a/drivers/xen/blkback/blkback.c
+++ b/drivers/xen/blkback/blkback.c
@@ -498,12 +498,12 @@ static void dispatch_rw_block_io(blkif_t
 	case BLKIF_OP_WRITE_BARRIER:
 		blkif->st_br_req++;
 		operation = REQ_OP_WRITE;
-		operation_flags = WRITE_FLUSH_FUA;
+		operation_flags = REQ_PREFLUSH | REQ_FUA;
 		break;
 	case BLKIF_OP_FLUSH_DISKCACHE:
 		blkif->st_fl_req++;
 		operation = REQ_OP_WRITE;
-		operation_flags = WRITE_FLUSH;
+		operation_flags = REQ_PREFLUSH;
 		break;
 	default:
 		operation = 0; /* make gcc happy */
@@ -583,7 +583,7 @@ static void dispatch_rw_block_io(blkif_t
 	}
 
 	/* Wait on all outstanding I/O's and once that has been completed
-	 * issue the WRITE_FLUSH.
+	 * issue the flush.
 	 */
 	if (req->operation == BLKIF_OP_WRITE_BARRIER)
 		drain_io(blkif);
--- a/drivers/xen/blkback/xenbus.c
+++ b/drivers/xen/blkback/xenbus.c
@@ -224,11 +224,9 @@ static void blkback_discard(struct xenbu
 	struct xenbus_device *dev = be->dev;
 	struct vbd *vbd = &be->blkif->vbd;
 	struct request_queue *q = bdev_get_queue(vbd->bdev);
-	int err, state = 0, discard_enable;
+	int err, state = 0;
 
-	err = xenbus_scanf(XBT_NIL, dev->nodename, "discard-enable", "%d",
-			   &discard_enable);
-	if (err == 1 && !discard_enable)
+	if (!xenbus_read_unsigned(dev->nodename, "discard-enable", 1))
 		return;
 
 	if (blk_queue_discard(q)) {
--- a/drivers/xen/blkfront/blkfront.c
+++ b/drivers/xen/blkfront/blkfront.c
@@ -332,7 +332,6 @@ static void blkfront_setup_discard(struc
 {
 	unsigned int discard_granularity;
 	unsigned int discard_alignment;
-	int discard_secure;
 
 	info->feature_discard = 1;
 	if (!xenbus_gather(XBT_NIL, info->xbdev->otherend,
@@ -342,10 +341,8 @@ static void blkfront_setup_discard(struc
 		info->discard_granularity = discard_granularity;
 		info->discard_alignment = discard_alignment;
 	}
-	if (xenbus_scanf(XBT_NIL, info->xbdev->otherend,
-			 "discard-secure", "%d", &discard_secure) != 1)
-		discard_secure = 0;
-	info->feature_secdiscard = !!discard_secure;
+	info->feature_secdiscard = !!xenbus_read_unsigned(info->xbdev->otherend,
+							  "discard-secure", 0);
 }
 
 /*
@@ -356,7 +353,7 @@ static void connect(struct blkfront_info
 {
 	unsigned long long sectors;
 	unsigned int binfo, sector_size, physical_sector_size;
-	int err, barrier, flush, discard;
+	int err, barrier;
 
 	switch (info->connected) {
 	case BLKIF_STATE_CONNECTED:
@@ -396,10 +393,9 @@ static void connect(struct blkfront_info
 	 * provide this. Assume physical sector size to be the same as
 	 * sector_size in that case.
 	 */
-	err = xenbus_scanf(XBT_NIL, info->xbdev->otherend,
-			   "physical-sector-size", "%u", &physical_sector_size);
-	if (err <= 0)
-		physical_sector_size = sector_size;
+	physical_sector_size = xenbus_read_unsigned(info->xbdev->otherend,
+						    "physical-sector-size",
+						    sector_size);
 
 	err = xenbus_scanf(XBT_NIL, info->xbdev->otherend,
 			   "feature-barrier", "%d", &barrier);
@@ -418,9 +414,8 @@ static void connect(struct blkfront_info
 	 * And if there is "feature-flush-cache" use that above
 	 * barriers.
 	 */
-	err = xenbus_scanf(XBT_NIL, info->xbdev->otherend,
-			   "feature-flush-cache", "%d", &flush);
-	if (err > 0 && flush)
+	if (xenbus_read_unsigned(info->xbdev->otherend,
+				 "feature-flush-cache", 0))
 		info->feature_flush = REQ_PREFLUSH;
 #else
 	if (err <= 0)
@@ -431,10 +426,7 @@ static void connect(struct blkfront_info
 		info->feature_flush = QUEUE_ORDERED_NONE;
 #endif
 
-	err = xenbus_scanf(XBT_NIL, info->xbdev->otherend,
-			   "feature-discard", "%d", &discard);
-
-	if (err > 0 && discard)
+	if (xenbus_read_unsigned(info->xbdev->otherend, "feature-discard", 0))
 		blkfront_setup_discard(info);
 
 	err = xlvbd_add(sectors, info->vdevice, binfo, sector_size,
--- a/drivers/xen/blkfront/block.h
+++ b/drivers/xen/blkfront/block.h
@@ -55,7 +55,6 @@
 #include <xen/interface/io/blkif.h>
 #include <xen/interface/io/ring.h>
 #include <asm/io.h>
-#include <asm/uaccess.h>
 
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4,8,0)
 # define REQ_PREFLUSH REQ_FLUSH
--- a/drivers/xen/blktap2-new/control.c
+++ b/drivers/xen/blktap2-new/control.c
@@ -2,7 +2,7 @@
 #include <linux/sched.h>
 #include <linux/miscdevice.h>
 #include <linux/device.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 
 #include "blktap.h"
 
--- a/drivers/xen/core/cpu_hotplug.c
+++ b/drivers/xen/core/cpu_hotplug.c
@@ -67,20 +67,17 @@ static void handle_vcpu_hotplug_event(
 	}
 }
 
-static int smpboot_cpu_notify(struct notifier_block *notifier,
-			      unsigned long action, void *hcpu)
+static int smpboot_cpu_offline(unsigned int cpu)
 {
-	unsigned int cpu = (long)hcpu;
-
 	/*
 	 * We do this in a callback notifier rather than __cpu_disable()
 	 * because local_cpu_hotplug_request() does not work in the latter
 	 * as it's always executed from within a stopmachine kthread.
 	 */
-	if ((action == CPU_DOWN_PREPARE) && local_cpu_hotplug_request())
+	if (local_cpu_hotplug_request())
 		cpumask_clear_cpu(cpu, local_allowed_cpumask);
 
-	return NOTIFY_OK;
+	return 0;
 }
 
 static int setup_cpu_watcher(struct notifier_block *notifier,
@@ -105,15 +102,15 @@ static int setup_cpu_watcher(struct noti
 
 static int __init setup_vcpu_hotplug_event(void)
 {
-	static struct notifier_block hotplug_cpu = {
-		.notifier_call = smpboot_cpu_notify };
 	static struct notifier_block xsn_cpu = {
 		.notifier_call = setup_cpu_watcher };
 
 	if (!is_running_on_xen())
 		return -ENODEV;
 
-	register_cpu_notifier(&hotplug_cpu);
+	cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN, "xen/guest:online",
+				  NULL, smpboot_cpu_offline);
+
 	register_xenstore_notifier(&xsn_cpu);
 
 	return 0;
--- a/drivers/xen/core/evtchn.c
+++ b/drivers/xen/core/evtchn.c
@@ -286,7 +286,6 @@ void __init init_IRQ(void)
 	irq_ctx_init(0);
 	xen_init_IRQ();
 }
-#include <asm/idle.h>
 #endif
 
 /* Xen will never allocate port zero for any purpose. */
@@ -333,7 +332,6 @@ asmlinkage __visible void __irq_entry ev
 
 	old_regs = set_irq_regs(regs);
 	irq_enter();
-	exit_idle();
 
 	do {
 		vcpu_info->evtchn_upcall_pending = 0;
--- a/drivers/xen/core/gnttab.c
+++ b/drivers/xen/core/gnttab.c
@@ -41,7 +41,6 @@
 #include <xen/interface/xen.h>
 #include <xen/gnttab.h>
 #include <asm/pgtable.h>
-#include <asm/uaccess.h>
 #include <asm/cmpxchg.h>
 #include <asm/io.h>
 #include <xen/interface/memory.h>
--- a/drivers/xen/core/reboot.c
+++ b/drivers/xen/core/reboot.c
@@ -295,9 +295,9 @@ static int setup_shutdown_watcher(void)
 	if (is_initial_xendomain())
 		return 0;
 
-	xenbus_scanf(XBT_NIL, "control",
-		     "platform-feature-multiprocessor-suspend",
-		     "%d", &fast_suspend);
+	fast_suspend = xenbus_read_unsigned("control",
+					    "platform-feature-multiprocessor-suspend",
+					    0);
 
 	err = register_xenbus_watch(&shutdown_watch);
 	if (err) {
--- a/drivers/xen/fbfront/xenfb.c
+++ b/drivers/xen/fbfront/xenfb.c
@@ -393,7 +393,7 @@ static int xenfb_vm_fault(struct vm_area
 {
 	struct xenfb_mapping *map = vma->vm_private_data;
 	struct xenfb_info *info = map->info;
-	int pgnr = ((long)vmf->virtual_address - vma->vm_start) >> PAGE_SHIFT;
+	int pgnr = PFN_DOWN(vmf->address - vma->vm_start);
 	unsigned long flags;
 	struct page *page;
 	int y1, y2;
@@ -572,7 +572,6 @@ static int xenfb_probe(struct xenbus_dev
 	struct xenfb_info *info;
 	struct fb_info *fb_info;
 	int fb_size;
-	int val;
 	int ret;
 
 	info = kzalloc(sizeof(*info), GFP_KERNEL);
@@ -582,10 +581,8 @@ static int xenfb_probe(struct xenbus_dev
 	}
 
 	/* Limit kernel param videoram amount to what is in xenstore */
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "videoram", "%d", &val) == 1) {
-		if (val < video[KPARAM_MEM])
-			video[KPARAM_MEM] = val;
-	}
+	video[KPARAM_MEM] = xenbus_read_unsigned(dev->otherend, "videoram",
+						 video[KPARAM_MEM]);
 
 	/* If requested res does not fit in available memory, use default */
 	fb_size = video[KPARAM_MEM] * MB_;
@@ -817,7 +814,6 @@ static void xenfb_backend_changed(struct
 				  enum xenbus_state backend_state)
 {
 	struct xenfb_info *info = dev_get_drvdata(&dev->dev);
-	int val;
 
 	switch (backend_state) {
 	case XenbusStateInitialising:
@@ -841,17 +837,13 @@ static void xenfb_backend_changed(struct
 		if (dev->state != XenbusStateConnected)
 			goto InitWait; /* no InitWait seen yet, fudge it */
 
-
-		if (xenbus_scanf(XBT_NIL, dev->otherend,
-					"feature-resize", "%d", &val) < 0)
-			val = 0;
-		info->feature_resize = val;
-
-		if (xenbus_scanf(XBT_NIL, info->xbdev->otherend,
-				 "request-update", "%d", &val) < 0)
-			val = 0;
-
-		if (val && !info->kthread) {
+		info->feature_resize = !!xenbus_read_unsigned(dev->otherend,
+							      "feature-resize",
+							      0);
+
+		if (xenbus_read_unsigned(dev->otherend,
+					 "request-update", 0) &&
+		    !info->kthread) {
 			info->kthread = kthread_run(xenfb_thread, info,
 						    "xenfb thread");
 			if (IS_ERR(info->kthread)) {
--- a/drivers/xen/fbfront/xenkbd.c
+++ b/drivers/xen/fbfront/xenkbd.c
@@ -123,8 +123,7 @@ int xenkbd_probe(struct xenbus_device *d
 	info->page->in_cons = info->page->in_prod = 0;
 	info->page->out_cons = info->page->out_prod = 0;
 
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-abs-pointer", "%d", &abs) < 0)
-		abs = 0;
+	abs = xenbus_read_unsigned(dev->otherend, "feature-abs-pointer", 0);
 	if (abs)
 		xenbus_write(XBT_NIL, dev->nodename, "request-abs-pointer", "1");
 
@@ -288,11 +287,8 @@ static void xenkbd_backend_changed(struc
 
 	case XenbusStateInitWait:
 	InitWait:
-		ret = xenbus_scanf(XBT_NIL, info->xbdev->otherend,
-				   "feature-abs-pointer", "%d", &val);
-		if (ret < 0)
-			val = 0;
-		if (val) {
+		if (xenbus_read_unsigned(info->xbdev->otherend,
+					 "feature-abs-pointer", 0)) {
 			ret = xenbus_write(XBT_NIL, info->xbdev->nodename,
 					   "request-abs-pointer", "1");
 			if (ret)
--- a/drivers/xen/gntdev/gntdev.c
+++ b/drivers/xen/gntdev/gntdev.c
@@ -24,7 +24,7 @@
 #include <linux/miscdevice.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/io.h>
 #include <xen/gnttab.h>
 #include <asm/hypervisor.h>
--- a/drivers/xen/netback/interface.c
+++ b/drivers/xen/netback/interface.c
@@ -91,7 +91,7 @@ static int net_close(struct net_device *
 
 static int netbk_change_mtu(struct net_device *dev, int mtu)
 {
-	int max = netbk_can_sg(dev) ? 65535 - ETH_HLEN : ETH_DATA_LEN;
+	int max = netbk_can_sg(dev) ? ETH_MAX_MTU - ETH_HLEN : ETH_DATA_LEN;
 
 	if (mtu > max)
 		return -EINVAL;
@@ -233,6 +233,9 @@ netif_t *netif_alloc(struct device *pare
 
 	dev->tx_queue_len = netbk_queue_length;
 
+	dev->min_mtu = 0;
+	dev->max_mtu = ETH_MAX_MTU - ETH_HLEN;
+
 	/*
 	 * Initialise a dummy MAC address. We choose the numerically
 	 * largest non-broadcast address to prevent the address getting
--- a/drivers/xen/netback/xenbus.c
+++ b/drivers/xen/netback/xenbus.c
@@ -425,7 +425,6 @@ static int connect_rings(struct backend_
 	unsigned int tx_ring_ref, rx_ring_ref;
 	unsigned int evtchn, rx_copy;
 	int err;
-	int val;
 
 	DPRINTK("");
 
@@ -454,44 +453,24 @@ static int connect_rings(struct backend_
 	netif->copying_receiver = !!rx_copy;
 
 	if (netif->dev->tx_queue_len != 0) {
-		if (xenbus_scanf(XBT_NIL, dev->otherend,
-				 "feature-rx-notify", "%d", &val) < 0)
-			val = 0;
-		if (val)
+		if (xenbus_read_unsigned(dev->otherend, "feature-rx-notify", 0))
 			netif->can_queue = 1;
 		else
 			/* Must be non-zero for pfifo_fast to work. */
 			netif->dev->tx_queue_len = 1;
 	}
 
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-sg", "%d", &val) < 0)
-		val = 0;
-	netif->can_sg = !!val;
-
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-gso-tcpv4", "%d",
-			 &val) < 0)
-		val = 0;
-	netif->gso = !!val;
-
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-gso-tcpv6", "%d",
-			 &val) < 0)
-		val = 0;
-	netif->gso6 = !!val;
-
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-no-csum-offload",
-			 "%d", &val) < 0)
-		val = 0;
-	netif->csum = !val;
-
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "feature-ipv6-csum-offload",
-			 "%d", &val) < 0)
-		val = 0;
-	netif->csum6 = !!val;
-
-	if (xenbus_scanf(XBT_NIL, dev->otherend, "request-multicast-control",
-			 "%d", &val) < 0)
-		val = 0;
-	netif->mcast = !!val;
+	netif->can_sg = !!xenbus_read_unsigned(dev->otherend, "feature-sg", 0);
+	netif->gso    = !!xenbus_read_unsigned(dev->otherend,
+					       "feature-gso-tcpv4", 0);
+	netif->gso6   = !!xenbus_read_unsigned(dev->otherend,
+					       "feature-gso-tcpv6", 0);
+	netif->csum   = !xenbus_read_unsigned(dev->otherend,
+					       "feature-no-csum-offload", 0);
+	netif->csum6  = !!xenbus_read_unsigned(dev->otherend,
+					       "feature-ipv6-csum-offload", 0);
+	netif->mcast  = !!xenbus_read_unsigned(dev->otherend,
+					       "request-multicast-control", 0);
 
 	/* Map the shared frame, irq etc. */
 	err = netif_map(be, tx_ring_ref, rx_ring_ref, evtchn);
--- a/drivers/xen/netfront/netfront.c
+++ b/drivers/xen/netfront/netfront.c
@@ -54,7 +54,6 @@
 #include <net/pkt_sched.h>
 #include <net/route.h>
 #include <net/tcp.h>
-#include <asm/uaccess.h>
 #include <xen/evtchn.h>
 #include <xen/xenbus.h>
 #include <xen/interface/io/netif.h>
@@ -1849,16 +1848,12 @@ static int network_connect(struct net_de
 	struct sk_buff *skb;
 	grant_ref_t ref;
 	netif_rx_request_t *req;
-	unsigned int feature_rx_copy, feature_rx_flip;
+	bool feature_rx_copy, feature_rx_flip;
 
-	err = xenbus_scanf(XBT_NIL, np->xbdev->otherend,
-			   "feature-rx-copy", "%u", &feature_rx_copy);
-	if (err != 1)
-		feature_rx_copy = 0;
-	err = xenbus_scanf(XBT_NIL, np->xbdev->otherend,
-			   "feature-rx-flip", "%u", &feature_rx_flip);
-	if (err != 1)
-		feature_rx_flip = 1;
+	feature_rx_copy = xenbus_read_unsigned(np->xbdev->otherend,
+					       "feature-rx-copy", 0);
+	feature_rx_flip = xenbus_read_unsigned(np->xbdev->otherend,
+					       "feature-rx-flip", 1);
 
 	/*
 	 * Copy packets on receive path if:
@@ -2081,43 +2076,24 @@ static netdev_features_t xennet_fix_feat
 					     netdev_features_t features)
 {
 	struct netfront_info *np = netdev_priv(dev);
-	int val;
-
-	if (features & NETIF_F_SG) {
-		if (xenbus_scanf(XBT_NIL, np->xbdev->otherend, "feature-sg",
-				 "%d", &val) < 0)
-			val = 0;
-
-		if (!val)
-			features &= ~NETIF_F_SG;
-	}
-
-	if (features & NETIF_F_TSO) {
-		if (xenbus_scanf(XBT_NIL, np->xbdev->otherend,
-				 "feature-gso-tcpv4", "%d", &val) < 0)
-			val = 0;
-
-		if (!val)
-			features &= ~NETIF_F_TSO;
-	}
-
-	if (features & NETIF_F_IPV6_CSUM) {
-		if (xenbus_scanf(XBT_NIL, np->xbdev->otherend,
-				 "feature-ipv6-csum-offload", "%d", &val) < 0)
-			val = 0;
-
-		if (!val)
-			features &= ~NETIF_F_IPV6_CSUM;
-	}
-
-	if (features & NETIF_F_TSO6) {
-		if (xenbus_scanf(XBT_NIL, np->xbdev->otherend,
-				 "feature-gso-tcpv6", "%d", &val) < 0)
-			val = 0;
 
-		if (!val)
-			features &= ~NETIF_F_TSO6;
-	}
+	if ((features & NETIF_F_SG) &&
+	    !xenbus_read_unsigned(np->xbdev->otherend, "feature-sg", 0))
+		features &= ~NETIF_F_SG;
+
+	if ((features & NETIF_F_TSO) &&
+	    !xenbus_read_unsigned(np->xbdev->otherend,
+				  "feature-gso-tcpv4", 0))
+		features &= ~NETIF_F_TSO;
+
+	if ((features & NETIF_F_IPV6_CSUM) &&
+	    !xenbus_read_unsigned(np->xbdev->otherend,
+				  "feature-ipv6-csum-offload", 0))
+		features &= ~NETIF_F_IPV6_CSUM;
+
+	if ((features & NETIF_F_TSO6) &&
+	    !xenbus_read_unsigned(np->xbdev->otherend, "feature-gso-tcpv6", 0))
+		features &= ~NETIF_F_TSO6;
 
 	return features;
 }
@@ -2233,6 +2209,8 @@ static struct net_device *create_netdev(
 	netdev->features |= netdev->hw_features;
 
 	netdev->ethtool_ops = &network_ethtool_ops;
+	netdev->min_mtu = 0;
+	netdev->max_mtu = XEN_NETIF_MAX_TX_SIZE;
 	SET_NETDEV_DEV(netdev, &dev->dev);
 
 	np->netdev = netdev;
--- a/drivers/xen/pcifront/pci_op.c
+++ b/drivers/xen/pcifront/pci_op.c
@@ -35,10 +35,7 @@ static void pcifront_init_sd(struct pcif
 	sd->platform_data = pdev;
 
 	/* Look for resources for this controller in xenbus. */
-	err = xenbus_scanf(XBT_NIL, pdev->xdev->otherend, "root_num",
-			   "%d", &root_num);
-	if (err != 1)
-		return;
+	root_num = xenbus_read_unsigned(pdev->xdev->otherend, "root_num", 0);
 
 	for (i = 0; i < root_num; i++) {
 		len = snprintf(str, sizeof(str), "root-%d", i);
@@ -75,10 +72,8 @@ static void pcifront_init_sd(struct pcif
 	if (unlikely(len >= (sizeof(str) - 1)))
 		return;
 
-	err = xenbus_scanf(XBT_NIL, pdev->xdev->otherend,
-			   str, "%d", &res_count);
-
-	if (err != 1)
+	res_count = xenbus_read_unsigned(pdev->xdev->otherend, str, 0);
+	if (!res_count)
 		return; /* No resources, nothing to do */
 
 	sd->window = kzalloc(sizeof(*sd->window) * res_count, GFP_KERNEL);
--- a/drivers/xen/pcifront/xenbus.c
+++ b/drivers/xen/pcifront/xenbus.c
@@ -339,10 +339,8 @@ static int pcifront_detach_devices(struc
 			err = -ENOMEM;
 			goto out;
 		}
-		err = xenbus_scanf(XBT_NIL, pdev->xdev->otherend, str, "%d",
-				   &state);
-		if (err != 1)
-			state = XenbusStateUnknown;
+		state = xenbus_read_unsigned(pdev->xdev->otherend, str,
+					     XenbusStateUnknown);
 
 		if (state != XenbusStateClosing)
 			continue;
--- a/drivers/xen/privcmd/compat_privcmd.c
+++ b/drivers/xen/privcmd/compat_privcmd.c
@@ -22,8 +22,8 @@
 #include <linux/fs.h>
 #include <linux/ioctl.h>
 #include <linux/syscalls.h>
+#include <linux/uaccess.h>
 #include <asm/hypervisor.h>
-#include <asm/uaccess.h>
 #include <xen/public/privcmd.h>
 #include <xen/compat_ioctl.h>
 
--- a/drivers/xen/privcmd/privcmd.c
+++ b/drivers/xen/privcmd/privcmd.c
@@ -13,11 +13,9 @@
 #include <linux/string.h>
 #include <linux/errno.h>
 #include <linux/mm.h>
-#include <asm/hypervisor.h>
-
+#include <linux/uaccess.h>
 #include <asm/pgalloc.h>
 #include <asm/pgtable.h>
-#include <asm/uaccess.h>
 #include <asm/tlb.h>
 #include <asm/hypervisor.h>
 #include <xen/public/privcmd.h>
--- a/drivers/xen/scsiback/xenbus.c
+++ b/drivers/xen/scsiback/xenbus.c
@@ -306,7 +306,6 @@ static int scsiback_probe(struct xenbus_
 			   const struct xenbus_device_id *id)
 {
 	int err;
-	unsigned val = 0;
 
 	struct backend_info *be = kzalloc(sizeof(struct backend_info),
 					  GFP_KERNEL);
@@ -335,11 +334,7 @@ static int scsiback_probe(struct xenbus_
 
 	scsiback_init_translation_table(be->info);
 
-	err = xenbus_scanf(XBT_NIL, dev->nodename, "feature-host", "%u", &val);
-	if (err <= 0)
-		val = 0;
-
-	if (val)
+	if (xenbus_read_unsigned(dev->nodename, "feature-host", 0))
 		be->info->feature = VSCSI_TYPE_HOST;
 
 	err = xenbus_switch_state(dev, XenbusStateInitWait);
--- a/drivers/xen/tpmback/tpmback.c
+++ b/drivers/xen/tpmback/tpmback.c
@@ -19,7 +19,6 @@
 #include <linux/miscdevice.h>
 #include <linux/poll.h>
 #include <linux/delay.h>
-#include <asm/uaccess.h>
 #include <xen/xenbus.h>
 #include <xen/interface/grant_table.h>
 #include <xen/gnttab.h>
--- a/drivers/xen/xen-pciback/controller.c
+++ b/drivers/xen/xen-pciback/controller.c
@@ -299,8 +299,8 @@ static int _xen_pcibk_publish_pci_roots(
 {
 	struct controller_dev_data *dev_data = pdev->pci_dev_data;
 	struct controller_list_entry *cntrl_entry;
-	int i, root_num, len, err = 0;
-	unsigned int domain, bus;
+	int len, err = 0;
+	unsigned int i, root_num, domain, bus;
 	char str[64];
 	struct walk_info info;
 
@@ -317,11 +317,8 @@ static int _xen_pcibk_publish_pci_roots(
  		 * Now figure out which root-%d this belongs to
 		 * so we can associate resources with it.
 		 */
-		err = xenbus_scanf(XBT_NIL, pdev->xdev->nodename,
-				   "root_num", "%d", &root_num);
-
-		if (err != 1)
-			goto out;
+		root_num = xenbus_read_unsigned(pdev->xdev->nodename,
+						"root_num", 0);
 
 		for (i = 0; i < root_num; i++) {
 			len = snprintf(str, sizeof(str), "root-%d", i);
--- a/drivers/xen/xenbus/xenbus_backend_client.c
+++ b/drivers/xen/xenbus/xenbus_backend_client.c
@@ -93,13 +93,7 @@ EXPORT_SYMBOL_GPL(xenbus_unmap_ring_vfre
 
 int xenbus_dev_is_online(struct xenbus_device *dev)
 {
-	int rc, val;
-
-	rc = xenbus_scanf(XBT_NIL, dev->nodename, "online", "%d", &val);
-	if (rc != 1)
-		val = 0; /* no online node present */
-
-	return val;
+	return !!xenbus_read_unsigned(dev->nodename, "online", 0);
 }
 EXPORT_SYMBOL_GPL(xenbus_dev_is_online);
 
--- a/drivers/xen/xenbus/xenbus_comms.h
+++ b/drivers/xen/xenbus/xenbus_comms.h
@@ -42,7 +42,6 @@ int xb_write(const void *data, unsigned
 int xb_read(void *data, unsigned len);
 int xb_data_to_read(void);
 int xb_wait_for_data_to_read(void);
-int xs_input_avail(void);
 extern struct xenstore_domain_interface *xen_store_interface;
 extern int xen_store_evtchn;
 extern enum xenstore_init xen_store_domain_type;
--- a/drivers/xen/xenbus/xenbus_dev.c
+++ b/drivers/xen/xenbus/xenbus_dev.c
@@ -42,10 +42,10 @@
 #include <linux/fs.h>
 #include <linux/poll.h>
 #include <linux/mutex.h>
+#include <linux/uaccess.h>
 
 #include "xenbus_comms.h"
 
-#include <asm/uaccess.h>
 #include <asm/hypervisor.h>
 #include <xen/xenbus.h>
 #include <xen/xen_proc.h>
--- a/drivers/xen/xenbus/xenbus_probe.c
+++ b/drivers/xen/xenbus/xenbus_probe.c
@@ -1486,7 +1486,7 @@ xenbus_init(void)
 	 * Create xenfs mountpoint in /proc for compatibility with
 	 * utilities that expect to find "xenbus" under "/proc/xen".
 	 */
-	proc_mkdir("xen", NULL);
+	proc_create_mount_point("xen");
 #endif
 
 	return 0;
--- a/drivers/xen/xenbus/xenbus_probe_backend.c
+++ b/drivers/xen/xenbus/xenbus_probe_backend.c
@@ -248,13 +248,7 @@ static int read_frontend_details(struct
 #ifndef CONFIG_XEN
 int xenbus_dev_is_online(struct xenbus_device *dev)
 {
-	int rc, val;
-
-	rc = xenbus_scanf(XBT_NIL, dev->nodename, "online", "%d", &val);
-	if (rc != 1)
-		val = 0; /* no online node present */
-
-	return val;
+	return !!xenbus_read_unsigned(dev->nodename, "online", 0);
 }
 EXPORT_SYMBOL_GPL(xenbus_dev_is_online);
 #endif
--- a/drivers/xen/xenbus/xenbus_xs.c
+++ b/drivers/xen/xenbus/xenbus_xs.c
@@ -580,6 +580,21 @@ int xenbus_scanf(struct xenbus_transacti
 }
 EXPORT_SYMBOL_GPL(xenbus_scanf);
 
+/* Read an (optional) unsigned value. */
+unsigned int xenbus_read_unsigned(const char *dir, const char *node,
+				  unsigned int default_val)
+{
+	unsigned int val;
+	int ret;
+
+	ret = xenbus_scanf(XBT_NIL, dir, node, "%u", &val);
+	if (ret <= 0)
+		val = default_val;
+
+	return val;
+}
+EXPORT_SYMBOL_GPL(xenbus_read_unsigned);
+
 /* Single printf and write: returns -errno or 0. */
 int xenbus_printf(struct xenbus_transaction t,
 		  const char *dir, const char *node, const char *fmt, ...)
@@ -696,7 +711,7 @@ static inline bool xen_strict_xenbus_qui
 static void xs_reset_watches(void)
 {
 #if defined(CONFIG_PARAVIRT_XEN) || defined(MODULE)
-	int err, supported = 0;
+	int err;
 
 #ifdef CONFIG_PARAVIRT_XEN
 	if (!xen_hvm_domain() || xen_initial_domain())
@@ -706,10 +721,8 @@ static void xs_reset_watches(void)
 	if (xen_strict_xenbus_quirk())
 		return;
 
-	err = xenbus_scanf(XBT_NIL, "control",
-			   "platform-feature-xs_reset_watches", "%d",
-			   &supported);
-	if (err != 1 || !supported)
+	if (!xenbus_read_unsigned("control",
+				  "platform-feature-xs_reset_watches", 0))
 		return;
 
 	err = xs_error(xs_single(XBT_NIL, XS_RESET_WATCHES, "", NULL));
--- a/include/acpi/processor.h
+++ b/include/acpi/processor.h
@@ -327,7 +327,7 @@ static inline void acpi_processor_ppc_ex
 	return;
 }
 #ifdef CONFIG_PROCESSOR_EXTERNAL_CONTROL
-int acpi_processor_ppc_has_changed(struct acpi_processor *, int event_flag);
+void acpi_processor_ppc_has_changed(struct acpi_processor *, int event_flag);
 #else
 static inline int acpi_processor_ppc_has_changed(struct acpi_processor *pr,
 								int event_flag)
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -10,9 +10,14 @@ struct page;
 struct scatterlist;
 
 enum swiotlb_force {
+#ifdef CONFIG_XEN
+	SWIOTLB_FORCE_OFF = -1,	/* swiotlb=off */
+#endif
 	SWIOTLB_NORMAL,		/* Default - depending on HW DMA mask etc. */
 	SWIOTLB_FORCE,		/* swiotlb=force */
+#ifndef CONFIG_XEN
 	SWIOTLB_NO_FORCE,	/* swiotlb=noforce */
+#endif
 };
 
 extern enum swiotlb_force swiotlb_force;
--- a/include/trace/events/swiotlb.h
+++ b/include/trace/events/swiotlb.h
@@ -40,7 +40,11 @@ TRACE_EVENT(swiotlb_bounced,
 		__print_symbolic(__entry->swiotlb_force,
 			{ SWIOTLB_NORMAL,	"NORMAL" },
 			{ SWIOTLB_FORCE,	"FORCE" },
+#ifndef CONFIG_XEN
 			{ SWIOTLB_NO_FORCE,	"NO_FORCE" }))
+#else
+			{ SWIOTLB_FORCE_OFF,	"OFF" }))
+#endif
 );
 
 #endif /*  _TRACE_SWIOTLB_H */
--- a/include/xen/xenbus.h
+++ b/include/xen/xenbus.h
@@ -191,6 +191,10 @@ __scanf(4, 5)
 int xenbus_scanf(struct xenbus_transaction t,
 		 const char *dir, const char *node, const char *fmt, ...);
 
+/* Read an (optional) unsigned value. */
+unsigned int xenbus_read_unsigned(const char *dir, const char *node,
+				  unsigned int default_val);
+
 /* Single printf and write: returns -errno or 0. */
 __printf(4, 5)
 int xenbus_printf(struct xenbus_transaction t,
--- a/lib/swiotlb-xen.c
+++ b/lib/swiotlb-xen.c
@@ -42,7 +42,7 @@
 #define OFFSET(val,align) ((unsigned long)((val) & ( (align) - 1)))
 
 int swiotlb;
-int __initdata swiotlb_force;
+enum swiotlb_force __initdata swiotlb_force;
 
 /*
  * Used to do a quick range check in swiotlb_tbl_unmap_single and
@@ -72,6 +72,12 @@ static unsigned int *io_tlb_list;
 static unsigned int io_tlb_index;
 
 /*
+ * Max segment that we can provide which (if pages are contingous) will
+ * not be bounced (unless SWIOTLB_FORCE is set).
+ */
+static unsigned int max_segment = PAGE_SIZE;
+
+/*
  * We need to save away the original address corresponding to a mapped entry
  * for the sync operations.
  */
@@ -109,10 +115,18 @@ setup_io_tlb_npages(char *str)
          * every DMA like it does on native Linux. 'off' forcibly disables
          * use of the swiotlb.
          */
-	if (!strcmp(str, "force"))
-		swiotlb_force = 1;
-	else if (!strcmp(str, "off"))
-		swiotlb_force = -1;
+	if (!strcmp(str, "force")) {
+		swiotlb_force = SWIOTLB_FORCE;
+#ifdef CONFIG_XEN
+	} else if (!strcmp(str, "off")) {
+		swiotlb_force = SWIOTLB_FORCE_OFF;
+		max_segment = 0;
+#else
+	} else if (!strcmp(str, "noforce")) {
+		swiotlb_force = SWIOTLB_NO_FORCE;
+		io_tlb_nslabs = 1;
+#endif
+	}
 
 	return 0;
 }
@@ -125,6 +139,22 @@ unsigned long swiotlb_nr_tbl(void)
 }
 EXPORT_SYMBOL_GPL(swiotlb_nr_tbl);
 
+unsigned int swiotlb_max_segment(void)
+{
+	return max_segment;
+}
+EXPORT_SYMBOL_GPL(swiotlb_max_segment);
+
+#ifndef CONFIG_XEN
+void swiotlb_set_max_segment(unsigned int val)
+{
+	if (swiotlb_force == SWIOTLB_FORCE)
+		max_segment = 1;
+	else
+		max_segment = rounddown(val, PAGE_SIZE);
+}
+#endif
+
 /* default to 64MB */
 #define IO_TLB_DEFAULT_SIZE (64UL<<20)
 
@@ -372,7 +402,8 @@ static void swiotlb_bounce(phys_addr_t o
 phys_addr_t swiotlb_tbl_map_single(struct device *hwdev,
 				   dma_addr_t tbl_dma_addr,
 				   phys_addr_t orig_addr, size_t size,
-				   enum dma_data_direction dir)
+				   enum dma_data_direction dir,
+				   unsigned long attrs)
 {
 	unsigned long flags;
 	phys_addr_t tlb_addr;
@@ -396,11 +427,11 @@ phys_addr_t swiotlb_tbl_map_single(struc
 		    : 1UL << (BITS_PER_LONG - IO_TLB_SHIFT);
 
 	/*
-	 * For mappings greater than a page, we limit the stride (and
-	 * hence alignment) to a page size.
+	 * For mappings greater than or equal to a page, we limit the stride
+	 * (and hence alignment) to a page size.
 	 */
 	nslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;
-	if (size > PAGE_SIZE)
+	if (size >= PAGE_SIZE)
 		stride = (1 << (PAGE_SHIFT - IO_TLB_SHIFT));
 	else
 		stride = 1;
@@ -470,7 +501,8 @@ found:
 	 */
 	for (i = 0; i < nslots; i++)
 		io_tlb_orig_addr[index+i] = orig_addr + (i << IO_TLB_SHIFT);
-	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)
+	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
+	    (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))
 		swiotlb_bounce(orig_addr, tlb_addr, size, DMA_TO_DEVICE);
 
 	return tlb_addr;
@@ -483,18 +515,29 @@ EXPORT_SYMBOL_GPL(swiotlb_tbl_map_single
 
 static phys_addr_t
 map_single(struct device *hwdev, phys_addr_t phys, size_t size,
-	   enum dma_data_direction dir)
+	   enum dma_data_direction dir, unsigned long attrs)
 {
-	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);
+	dma_addr_t start_dma_addr;
+
+#ifndef CONFIG_XEN
+	if (swiotlb_force == SWIOTLB_NO_FORCE) {
+		dev_warn_ratelimited(hwdev, "Cannot do DMA to address %pa\n",
+				     &phys);
+		return SWIOTLB_MAP_ERROR;
+	}
+#endif
 
-	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size, dir);
+	start_dma_addr = phys_to_dma(hwdev, io_tlb_start);
+	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size,
+				      dir, attrs);
 }
 
 /*
  * dma_addr is the kernel virtual address of the bounce buffer to unmap.
  */
 void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
-			      size_t size, enum dma_data_direction dir)
+			      size_t size, enum dma_data_direction dir,
+			      unsigned long attrs)
 {
 	unsigned long flags;
 	int i, count, nslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;
@@ -505,6 +548,7 @@ void swiotlb_tbl_unmap_single(struct dev
 	 * First, sync the memory before unmapping the entry
 	 */
 	if (orig_addr != INVALID_PHYS_ADDR &&
+	    !(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&
 	    ((dir == DMA_FROM_DEVICE) || (dir == DMA_BIDIRECTIONAL)))
 		swiotlb_bounce(orig_addr, tlb_addr, size, DMA_FROM_DEVICE);
 
@@ -576,6 +620,11 @@ static void
 swiotlb_full(struct device *dev, size_t size, enum dma_data_direction dir,
 	     int do_panic)
 {
+#ifndef CONFIG_XEN
+	if (swiotlb_force == SWIOTLB_NO_FORCE)
+		return;
+#endif
+
 	/*
 	 * Ran out of IOMMU space for this operation. This is very bad.
 	 * Unfortunately the drivers cannot handle this operation properly.
@@ -583,8 +632,8 @@ swiotlb_full(struct device *dev, size_t
 	 * When the mapping is small enough return a static buffer to limit
 	 * the damage, or panic when the transfer is too big.
 	 */
-	printk(KERN_ERR "PCI-DMA: Out of SW-IOMMU space for %zu bytes at "
-	       "device %s\n", size, dev ? dev_name(dev) : "?");
+	dev_err_ratelimited(dev, "DMA: Out of SW-IOMMU space for %zu bytes\n",
+			    size);
 
 	if (size <= io_tlb_overflow || !do_panic)
 		return;
@@ -627,7 +676,7 @@ dma_addr_t swiotlb_map_page(struct devic
 
 	/* Oh well, have to allocate and map a bounce buffer. */
 	gnttab_dma_unmap_page(dev_addr);
-	map = map_single(dev, phys, size, dir);
+	map = map_single(dev, phys, size, dir, attrs);
 	if (map == SWIOTLB_MAP_ERROR) {
 		swiotlb_full(dev, size, dir, 1);
 		return phys_to_dma(dev, io_tlb_overflow_buffer);
@@ -636,12 +685,13 @@ dma_addr_t swiotlb_map_page(struct devic
 	dev_addr = phys_to_dma(dev, map);
 
 	/* Ensure that the address returned is DMA'ble */
-	if (!dma_capable(dev, dev_addr, size)) {
-		swiotlb_tbl_unmap_single(dev, map, size, dir);
-		return phys_to_dma(dev, io_tlb_overflow_buffer);
-	}
+	if (dma_capable(dev, dev_addr, size))
+		return dev_addr;
 
-	return dev_addr;
+	attrs |= DMA_ATTR_SKIP_CPU_SYNC;
+	swiotlb_tbl_unmap_single(dev, map, size, dir, attrs);
+
+	return phys_to_dma(dev, io_tlb_overflow_buffer);
 }
 EXPORT_SYMBOL_GPL(swiotlb_map_page);
 
@@ -654,14 +704,15 @@ EXPORT_SYMBOL_GPL(swiotlb_map_page);
  * whatever the device wrote there.
  */
 static void unmap_single(struct device *hwdev, dma_addr_t dev_addr,
-			 size_t size, enum dma_data_direction dir)
+			 size_t size, enum dma_data_direction dir,
+			 unsigned long attrs)
 {
 	phys_addr_t paddr = dma_to_phys(hwdev, dev_addr);
 
 	BUG_ON(dir == DMA_NONE);
 
 	if (_is_swiotlb_buffer(dev_addr)) {
-		swiotlb_tbl_unmap_single(hwdev, paddr, size, dir);
+		swiotlb_tbl_unmap_single(hwdev, paddr, size, dir, attrs);
 		return;
 	}
 
@@ -672,7 +723,7 @@ void swiotlb_unmap_page(struct device *h
 			size_t size, enum dma_data_direction dir,
 			unsigned long attrs)
 {
-	unmap_single(hwdev, dev_addr, size, dir);
+	unmap_single(hwdev, dev_addr, size, dir, attrs);
 }
 EXPORT_SYMBOL_GPL(swiotlb_unmap_page);
 
@@ -752,11 +803,12 @@ swiotlb_map_sg_attrs(struct device *hwde
 
 			gnttab_dma_unmap_page(dev_addr);
 			map = map_single(hwdev, paddr,
-					 sg->length, dir);
+					 sg->length, dir, attrs);
 			if (map == SWIOTLB_MAP_ERROR) {
 				/* Don't panic here, we expect map_sg users
 				   to do proper error handling. */
 				swiotlb_full(hwdev, sg->length, dir, 0);
+				attrs |= DMA_ATTR_SKIP_CPU_SYNC;
 				swiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,
 						       attrs);
 				sg_dma_len(sgl) = 0;
@@ -771,14 +823,6 @@ swiotlb_map_sg_attrs(struct device *hwde
 }
 EXPORT_SYMBOL(swiotlb_map_sg_attrs);
 
-int
-swiotlb_map_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
-	       enum dma_data_direction dir)
-{
-	return swiotlb_map_sg_attrs(hwdev, sgl, nelems, dir, 0);
-}
-EXPORT_SYMBOL(swiotlb_map_sg);
-
 /*
  * Unmap a set of streaming mode DMA translations.  Again, cpu read rules
  * concerning calls here are the same as for swiotlb_unmap_page() above.
@@ -794,19 +838,11 @@ swiotlb_unmap_sg_attrs(struct device *hw
 	BUG_ON(dir == DMA_NONE);
 
 	for_each_sg(sgl, sg, nelems, i)
-		unmap_single(hwdev, sg->dma_address, sg_dma_len(sg), dir);
-
+		unmap_single(hwdev, sg->dma_address, sg_dma_len(sg), dir,
+			     attrs);
 }
 EXPORT_SYMBOL(swiotlb_unmap_sg_attrs);
 
-void
-swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
-		 enum dma_data_direction dir)
-{
-	return swiotlb_unmap_sg_attrs(hwdev, sgl, nelems, dir, 0);
-}
-EXPORT_SYMBOL(swiotlb_unmap_sg);
-
 /*
  * Make physical memory consistent for a set of streaming mode DMA translations
  * after a transfer.
