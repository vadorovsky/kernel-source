From: Tim Chen <tim.c.chen@linux.intel.com>
Date: Sat, 16 Dec 2017 19:35:49 +0100
Subject: x86/kvm: Pad RSB on VM transition
Patch-mainline: submitted on 2018/1/9
References: bsc#1068032

Add code to pad the local CPU's RSB entries to protect
from previous less privilege mode.

Boris:

 - Use asm function instead of duplicating a C function.
 - Add indirection to stuff_rsb() so that EXPORT_SYMBOL_GPL works.
 Otherwise we'd need to backport the asm versions of those from 4.9.

 - Also, that stuff_rsb() dummy for 32-bit should probably be present
 there too, as we want to do that on 32-bit too but we'll address that
 properly once the pile goes upstream.

Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
Signed-off-by: Borislav Petkov <bp@suse.de>
---
 arch/x86/include/asm/processor.h |    1 +
 arch/x86/include/asm/proto.h     |    1 +
 arch/x86/include/asm/spec_ctrl.h |    1 +
 arch/x86/kernel/cpu/spec_ctrl.c  |   11 +++++++++++
 arch/x86/kvm/vmx.c               |    3 +++
 5 files changed, 17 insertions(+)

--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -418,6 +418,7 @@ struct stack_canary {
 };
 DECLARE_PER_CPU_ALIGNED(struct stack_canary, stack_canary);
 #endif
+static inline void stuff_rsb(void) {}
 #endif	/* X86_64 */
 
 extern unsigned int xstate_size;
--- a/arch/x86/include/asm/proto.h
+++ b/arch/x86/include/asm/proto.h
@@ -18,6 +18,7 @@ void syscall32_cpu_init(void);
 
 void x86_configure_nx(void);
 void x86_report_nx(void);
+void stuff_rsb(void);
 
 extern int reboot_force;
 
--- a/arch/x86/include/asm/spec_ctrl.h
+++ b/arch/x86/include/asm/spec_ctrl.h
@@ -57,6 +57,7 @@
 #else /* __ASSEMBLY__ */
 void x86_enable_ibrs(void);
 void x86_disable_ibrs(void);
+void stuff_RSB(void);
 
 static inline void x86_ibp_barrier(void)
 {
--- a/arch/x86/kernel/cpu/spec_ctrl.c
+++ b/arch/x86/kernel/cpu/spec_ctrl.c
@@ -5,6 +5,7 @@
 #include <linux/module.h>
 
 #include <asm/msr.h>
+#include <asm/proto.h>
 #include <asm/processor.h>
 #include <asm/spec_ctrl.h>
 
@@ -21,3 +22,13 @@ void x86_enable_ibrs(void)
 		native_wrmsrl(MSR_IA32_SPEC_CTRL, FEATURE_ENABLE_IBRS);
 }
 EXPORT_SYMBOL_GPL(x86_enable_ibrs);
+
+/*
+ * Do this indirection as otherwise we'd need to backport the
+ * EXPORT_SYMBOL_GPL() for asm stuff.
+ */
+void stuff_RSB(void)
+{
+	stuff_rsb();
+}
+EXPORT_SYMBOL_GPL(stuff_RSB);
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -41,6 +41,7 @@
 #include <asm/xcr.h>
 #include <asm/msr-index.h>
 #include <asm/spec_ctrl.h>
+#include <asm/proto.h>
 
 #include "trace.h"
 
@@ -6842,6 +6843,8 @@ static void __noclone vmx_vcpu_run(struc
 #endif
 	      );
 
+	stuff_RSB();
+
 	vcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)
 				  | (1 << VCPU_EXREG_RFLAGS)
 				  | (1 << VCPU_EXREG_CPL)
