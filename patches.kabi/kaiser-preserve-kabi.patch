From: Jiri Kosina <jkosina@suse.cz>
Subject: [PATCH] kaiser: work around kABI
Patch-mainline: Never, SUSE-specific
References: bsc#1068032

The most potentially dangerous one is the vmstats one.  I can't imagine what
3rd party module would realistically be directly allocating pglist_data,
per_cpu_nodestat, memcg_stat_item, lruvec_stat, etc, but the potential
non-zero risk is there.

Signed-off-by: Jiri Kosina <jkosina@suse.cz>

---
 arch/x86/include/asm/desc.h        |    4 ++++
 arch/x86/include/asm/mmu_context.h |    1 +
 arch/x86/include/asm/processor.h   |    4 ++++
 arch/x86/include/asm/tlbflush.h    |    2 ++
 arch/x86/kernel/cpu/common.c       |    4 ++++
 arch/x86/kernel/process.c          |    4 ++++
 include/linux/mmu_context.h        |    2 ++
 kernel/sched/core.c                |    4 ++++
 8 files changed, 25 insertions(+)

--- a/arch/x86/include/asm/desc.h
+++ b/arch/x86/include/asm/desc.h
@@ -43,7 +43,11 @@ struct gdt_page {
 	struct desc_struct gdt[GDT_ENTRIES];
 } __attribute__((aligned(PAGE_SIZE)));
 
+#ifdef __GENKSYMS__
+DECLARE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page);
+#else
 DECLARE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page);
+#endif
 
 static inline struct desc_struct *get_cpu_gdt_table(unsigned int cpu)
 {
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -305,7 +305,11 @@ struct tss_struct {
 
 } ____cacheline_aligned;
 
+#ifndef __GENKSYMS__
 DECLARE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, cpu_tss);
+#else
+DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);
+#endif
 
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
--- a/arch/x86/include/asm/tlbflush.h
+++ b/arch/x86/include/asm/tlbflush.h
@@ -6,7 +6,9 @@
 
 #include <asm/processor.h>
 #include <asm/special_insns.h>
+#ifndef __GENKSYMS__
 #include <asm/smp.h>
+#endif
 
 static inline void __invpcid(unsigned long pcid, unsigned long addr,
 			     unsigned long type)
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -92,7 +92,11 @@ static const struct cpu_dev default_cpu
 
 static const struct cpu_dev *this_cpu = &default_cpu;
 
+#ifndef __GENKSYMS__
 DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page) = { .gdt = {
+#else
+DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
+#endif
 #ifdef CONFIG_X86_64
 	/*
 	 * We need valid kernel segments for data and code in long mode too
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -39,7 +39,11 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
+#if defined(CONFIG_GENKSYMS)
+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {
+#else
 __visible DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, cpu_tss) = {
+#endif
 	.x86_tss = {
 		.sp0 = TOP_OF_INIT_STACK,
 #ifdef CONFIG_X86_32
--- a/include/linux/mmu_context.h
+++ b/include/linux/mmu_context.h
@@ -1,7 +1,9 @@
 #ifndef _LINUX_MMU_CONTEXT_H
 #define _LINUX_MMU_CONTEXT_H
 
+#ifndef __GENKSYMS__
 #include <asm/mmu_context.h>
+#endif
 
 struct mm_struct;
 
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -32,7 +32,11 @@
 #include <linux/init.h>
 #include <linux/uaccess.h>
 #include <linux/highmem.h>
+#ifndef __GENKSYMS__
 #include <linux/mmu_context.h>
+#else
+#include <asm/mmu_context.h>
+#endif
 #include <linux/interrupt.h>
 #include <linux/capability.h>
 #include <linux/completion.h>
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@ -107,6 +107,7 @@ extern void switch_mm(struct mm_struct *
 
 extern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 			       struct task_struct *tsk);
+#undef switch_mm_irqs_off /* XXX due to compile warning with GENKSYMS */
 #define switch_mm_irqs_off switch_mm_irqs_off
 
 #define activate_mm(prev, next)			\
